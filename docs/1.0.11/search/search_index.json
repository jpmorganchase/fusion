{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#pyfusion","title":"PyFusion","text":"<p>PyFusion is the Python SDK for the Fusion platform API. </p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install pyfusion\n</code></pre> <p>Fusion by J.P. Morgan is a cloud-native data platform for institutional investors, providing end-to-end data management, analytics, and reporting solutions across the investment lifecycle. The platform allows clients to seamlessly integrate and combine data from multiple sources into a single data model that delivers the benefits and scale and reduces costs, along with the ability to more easily unlock timely analysis and insights. Fusion's open data architecture supports flexible distribution, including partnerships with cloud and data providers, all managed by J.P. Morgan data experts. </p> <p>For more information, please visit fusion.jpmorgan.com</p> <p>For the SDK documentation, please visit page</p>"},{"location":"api/","title":"Modules","text":"<p>Main Fusion module.</p> <p>Synchronisation between the local filesystem and Fusion.</p> <p>Parameters:</p> Name Type Description Default <code>fs_fusion</code> <code>fsspec.filesystem</code> <p>Fusion filesystem.</p> required <code>fs_local</code> <code>fsspec.filesystem</code> <p>Local filesystem.</p> required <code>products</code> <code>list</code> <p>List of products.</p> <code>None</code> <code>datasets</code> <code>list</code> <p>List of datasets.</p> <code>None</code> <code>catalog</code> <code>str</code> <p>Fusion catalog.</p> <code>None</code> <code>direction</code> <code>str</code> <p>Direction of synchronisation: upload/download.</p> <code>'upload'</code> <code>flatten</code> <code>bool</code> <p>Flatten the folder structure.</p> <code>False</code> <code>dataset_format</code> <code>str</code> <p>Dataset format for upload/download.</p> <code>None</code> <code>n_par</code> <code>int</code> <p>Specify how many distributions to download in parallel. Defaults to all.</p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>Display a progress bar during data download Defaults to True.</p> <code>True</code> <code>log_level</code> <p>Logging level. Error level by default.</p> <code>logging.ERROR</code> <code>log_path</code> <code>str</code> <p>The folder path where the log is stored.</p> <code>'.'</code> Source code in <code>fusion\\fs_sync.py</code> <pre><code>def fsync(\n    fs_fusion: fsspec.filesystem,\n    fs_local: fsspec.filesystem,\n    products: list = None,\n    datasets: list = None,\n    catalog: str = None,\n    direction: str = \"upload\",\n    flatten=False,\n    dataset_format=None,\n    n_par=None,\n    show_progress=True,\n    log_level=logging.ERROR,\n    log_path: str = \".\",\n):\n\"\"\"Synchronisation between the local filesystem and Fusion.\n\n    Args:\n        fs_fusion (fsspec.filesystem): Fusion filesystem.\n        fs_local (fsspec.filesystem): Local filesystem.\n        products (list): List of products.\n        datasets (list): List of datasets.\n        catalog (str): Fusion catalog.\n        direction (str): Direction of synchronisation: upload/download.\n        flatten (bool): Flatten the folder structure.\n        dataset_format (str): Dataset format for upload/download.\n        n_par (int, optional): Specify how many distributions to download in parallel. Defaults to all.\n        show_progress (bool, optional): Display a progress bar during data download Defaults to True.\n        log_level: Logging level. Error level by default.\n        log_path (str, optional): The folder path where the log is stored.\n\n    Returns:\n\n    \"\"\"\n\n    if logger.hasHandlers():\n        logger.handlers.clear()\n    file_handler = logging.FileHandler(\n        filename=\"{0}/{1}\".format(log_path, \"fusion_fsync.log\")\n    )\n    logging.addLevelName(VERBOSE_LVL, \"VERBOSE\")\n    stdout_handler = logging.StreamHandler(sys.stdout)\n    formatter = logging.Formatter(\n        \"%(asctime)s.%(msecs)03d %(name)s:%(levelname)s %(message)s\",\n        datefmt=\"%Y-%m-%d %H:%M:%S\",\n    )\n    stdout_handler.setFormatter(formatter)\n    logger.addHandler(stdout_handler)\n    logger.addHandler(file_handler)\n    logger.setLevel(log_level)\n\n    catalog = \"common\" if not catalog else catalog\n    datasets = [] if not datasets else datasets\n    products = [] if not products else products\n\n    assert (\n        len(products) &gt; 0 or len(datasets) &gt; 0\n    ), \"At least one list products or datasets should be non-empty.\"\n    assert direction in [\n        \"upload\",\n        \"download\",\n    ], \"The direction must be either upload or download.\"\n\n    for product in products:\n        res = json.loads(fs_fusion.cat(f\"{catalog}/products/{product}\").decode())\n        datasets += [r[\"identifier\"] for r in res[\"resources\"]]\n\n    assert len(datasets) &gt; 0, \"The supplied products did not contain any datasets.\"\n\n    local_state = pd.DataFrame()\n    fusion_state = pd.DataFrame()\n    while True:\n        try:\n            local_state_temp = _get_local_state(\n                fs_local, fs_fusion, datasets, catalog, dataset_format, local_state\n            )\n            fusion_state_temp = _get_fusion_df(\n                fs_fusion, datasets, catalog, flatten, dataset_format\n            )\n            if not local_state_temp.equals(local_state) or not fusion_state_temp.equals(\n                fusion_state\n            ):\n                res = _synchronize(\n                    fs_fusion,\n                    fs_local,\n                    local_state_temp,\n                    fusion_state_temp,\n                    direction,\n                    n_par,\n                    show_progress,\n                )\n                if len(res) == 0 or all((i[0] for i in res)):\n                    local_state = local_state_temp\n                    fusion_state = fusion_state_temp\n\n                if not all(r[0] for r in res):\n                    failed_res = [r for r in res if not r[0]]\n                    msg = f\"Not all {direction}s were successfully completed. The following failed:\\n{failed_res}\"\n                    errs = [r for r in res if not r[2]]\n                    logger.warning(msg)\n                    logger.warning(errs)\n                    warnings.warn(msg)\n\n            else:\n                logger.info(\"All synced, sleeping\")\n                time.sleep(10)\n\n        except KeyboardInterrupt:\n            if input(\"Type exit to exit: \") != \"exit\":\n                continue\n            break\n\n        except Exception as ex:\n            logger.error(\"%s Issue occurred: %s\", type(ex), ex)\n            continue\n</code></pre>"},{"location":"api/#fusion.fusion.Fusion","title":"<code>Fusion</code>","text":"<p>Core Fusion class for API access.</p> Source code in <code>fusion\\fusion.py</code> <pre><code>class Fusion:\n\"\"\"Core Fusion class for API access.\"\"\"\n\n    @staticmethod\n    def _call_for_dataframe(url: str, session: requests.Session) -&gt; pd.DataFrame:\n\"\"\"Private function that calls an API endpoint and returns the data as a pandas dataframe.\n\n        Args:\n            url (Union[FusionCredentials, Union[str, dict]): URL for an API endpoint with valid parameters.\n            session (requests.Session): Specify a proxy if required to access the authentication server. Defaults to {}.\n\n        Returns:\n            pandas.DataFrame: a dataframe containing the requested data.\n        \"\"\"\n        response = session.get(url)\n        response.raise_for_status()\n        table = response.json()[\"resources\"]\n        df = pd.DataFrame(table).reset_index(drop=True)\n        return df\n\n    def __init__(\n        self,\n        credentials: Union[str, dict] = \"config/client_credentials.json\",\n        root_url: str = \"https://fusion-api.jpmorgan.com/fusion/v1/\",\n        download_folder: str = \"downloads\",\n        log_level: int = logging.ERROR,\n        fs=None,\n        log_path: str = \".\",\n    ) -&gt; None:\n\"\"\"Constructor to instantiate a new Fusion object.\n\n        Args:\n            credentials (Union[str, dict], optional): A path to a credentials file or\n                a dictionary containing the required keys.\n                Defaults to 'config/client_credentials.json'.\n            root_url (_type_, optional): The API root URL.\n                Defaults to \"https://fusion-api.jpmorgan.com/fusion/v1/\".\n            download_folder (str, optional): The folder path where downloaded data files\n                are saved. Defaults to \"downloads\".\n            log_level (int, optional): Set the logging level. Defaults to logging.ERROR.\n            fs (fsspec.filesystem): filesystem.\n            log_path (str, optional): The folder path where the log is stored.\n        \"\"\"\n        self._default_catalog = \"common\"\n\n        self.root_url = root_url\n        self.download_folder = download_folder\n        Path(download_folder).mkdir(parents=True, exist_ok=True)\n\n        if logger.hasHandlers():\n            logger.handlers.clear()\n        file_handler = logging.FileHandler(\n            filename=\"{0}/{1}\".format(log_path, \"fusion_sdk.log\")\n        )\n        logging.addLevelName(VERBOSE_LVL, \"VERBOSE\")\n        stdout_handler = logging.StreamHandler(sys.stdout)\n        formatter = logging.Formatter(\n            \"%(asctime)s.%(msecs)03d %(name)s:%(levelname)s %(message)s\",\n            datefmt=\"%Y-%m-%d %H:%M:%S\",\n        )\n        stdout_handler.setFormatter(formatter)\n        logger.addHandler(stdout_handler)\n        logger.addHandler(file_handler)\n        logger.setLevel(log_level)\n\n        if isinstance(credentials, FusionCredentials):\n            self.credentials = credentials\n        else:\n            self.credentials = FusionCredentials.from_object(credentials)\n\n        self.session = get_session(self.credentials, self.root_url)\n        self.fs = fs if fs else get_default_fs()\n\n    def __repr__(self):\n\"\"\"Object representation to list all available methods.\"\"\"\n        return \"Fusion object \\nAvailable methods:\\n\" + tabulate(\n            pd.DataFrame(\n                [\n                    [\n                        method_name\n                        for method_name in dir(Fusion)\n                        if callable(getattr(Fusion, method_name))\n                        and not method_name.startswith(\"_\")\n                    ]\n                    + [\n                        p\n                        for p in dir(Fusion)\n                        if isinstance(getattr(Fusion, p), property)\n                    ],\n                    [\n                        getattr(Fusion, method_name).__doc__.split(\"\\n\")[0]\n                        for method_name in dir(Fusion)\n                        if callable(getattr(Fusion, method_name))\n                        and not method_name.startswith(\"_\")\n                    ]\n                    + [\n                        getattr(Fusion, p).__doc__.split(\"\\n\")[0]\n                        for p in dir(Fusion)\n                        if isinstance(getattr(Fusion, p), property)\n                    ],\n                ]\n            ).T.set_index(0),\n            tablefmt=\"psql\",\n        )\n\n    @property\n    def default_catalog(self) -&gt; str:\n\"\"\"Returns the default catalog.\n\n        Returns:\n            None\n        \"\"\"\n        return self._default_catalog\n\n    @default_catalog.setter\n    def default_catalog(self, catalog) -&gt; None:\n\"\"\"Allow the default catalog, which is \"common\" to be overridden.\n\n        Args:\n            catalog (str): The catalog to use as the default\n\n        Returns:\n            None\n        \"\"\"\n        self._default_catalog = catalog\n\n    def __use_catalog(self, catalog):\n\"\"\"Determine which catalog to use in an API call.\n\n        Args:\n            catalog (str): The catalog value passed as an argument to an API function wrapper.\n\n        Returns:\n            str: The catalog to use\n        \"\"\"\n        if catalog is None:\n            return self.default_catalog\n        else:\n            return catalog\n\n    def get_fusion_filesystem(self):\n\"\"\"Creates Fusion Filesystem.\n\n        Returns: Fusion Filesystem\n\n        \"\"\"\n        return FusionHTTPFileSystem(\n            client_kwargs={\"root_url\": self.root_url, \"credentials\": self.credentials}\n        )\n\n    def list_catalogs(self, output: bool = False) -&gt; pd.DataFrame:\n\"\"\"Lists the catalogs available to the API account.\n\n        Args:\n            output (bool, optional): If True then print the dataframe. Defaults to False.\n\n        Returns:\n            class:`pandas.DataFrame`: A dataframe with a row for each catalog\n        \"\"\"\n        url = f\"{self.root_url}catalogs/\"\n        df = Fusion._call_for_dataframe(url, self.session)\n\n        if output:\n            print(tabulate(df, headers=\"keys\", tablefmt=\"psql\"))\n\n        return df\n\n    def catalog_resources(\n        self, catalog: str = None, output: bool = False\n    ) -&gt; pd.DataFrame:\n\"\"\"List the resources contained within the catalog, for example products and datasets.\n\n        Args:\n            catalog (str, optional): A catalog identifier. Defaults to 'common'.\n            output (bool, optional): If True then print the dataframe. Defaults to False.\n\n        Returns:\n           class:`pandas.DataFrame`: A dataframe with a row for each resource within the catalog\n        \"\"\"\n        catalog = self.__use_catalog(catalog)\n\n        url = f\"{self.root_url}catalogs/{catalog}\"\n        df = Fusion._call_for_dataframe(url, self.session)\n\n        if output:\n            print(tabulate(df, headers=\"keys\", tablefmt=\"psql\", maxcolwidths=30))\n\n        return df\n\n    def list_products(\n        self,\n        contains: Union[str, list] = None,\n        id_contains: bool = False,\n        catalog: str = None,\n        output: bool = False,\n        max_results: int = -1,\n        display_all_columns: bool = False,\n    ) -&gt; pd.DataFrame:\n\"\"\"Get the products contained in a catalog. A product is a grouping of datasets.\n\n        Args:\n            contains (Union[str, list], optional): A string or a list of strings that are product\n                identifiers to filter the products list. If a list is provided then it will return\n                products whose identifier matches any of the strings. Defaults to None.\n            id_contains (bool): Filter datasets only where the string(s) are contained in the identifier,\n                ignoring description.\n            catalog (str, optional): A catalog identifier. Defaults to 'common'.\n            output (bool, optional): If True then print the dataframe. Defaults to False.\n            max_results (int, optional): Limit the number of rows returned in the dataframe.\n                Defaults to -1 which returns all results.\n            display_all_columns (bool, optional): If True displays all columns returned by the API,\n                otherwise only the key columns are displayed\n\n        Returns:\n            class:`pandas.DataFrame`: a dataframe with a row for each product\n        \"\"\"\n        catalog = self.__use_catalog(catalog)\n\n        url = f\"{self.root_url}catalogs/{catalog}/products\"\n        df = Fusion._call_for_dataframe(url, self.session)\n\n        if contains:\n            if isinstance(contains, list):\n                contains = \"|\".join(f\"{s}\" for s in contains)\n            if id_contains:\n                df = df[df[\"identifier\"].str.contains(contains, case=False)]\n            else:\n                df = df[\n                    df[\"identifier\"].str.contains(contains, case=False)\n                    | df[\"description\"].str.contains(contains, case=False)\n                ]\n\n        df[\"category\"] = df.category.str.join(\", \")\n        df[\"region\"] = df.region.str.join(\", \")\n        if not display_all_columns:\n            df = df[\n                [\"identifier\", \"title\", \"region\", \"category\", \"status\", \"description\"]\n            ]\n\n        if max_results &gt; -1:\n            df = df[0:max_results]\n\n        if output:\n            print(tabulate(df, headers=\"keys\", tablefmt=\"psql\", maxcolwidths=30))\n\n        return df\n\n    def list_datasets(\n        self,\n        contains: Union[str, list] = None,\n        id_contains: bool = False,\n        catalog: str = None,\n        output: bool = False,\n        max_results: int = -1,\n        display_all_columns: bool = False,\n    ) -&gt; pd.DataFrame:\n\"\"\"Get the datasets contained in a catalog.\n\n        Args:\n            contains (Union[str, list], optional): A string or a list of strings that are dataset\n                identifiers to filter the datasets list. If a list is provided then it will return\n                datasets whose identifier matches any of the strings. Defaults to None.\n            id_contains (bool): Filter datasets only where the string(s) are contained in the identifier,\n                ignoring description.\n            catalog (str, optional): A catalog identifier. Defaults to 'common'.\n            output (bool, optional): If True then print the dataframe. Defaults to False.\n            max_results (int, optional): Limit the number of rows returned in the dataframe.\n                Defaults to -1 which returns all results.\n            display_all_columns (bool, optional): If True displays all columns returned by the API,\n                otherwise only the key columns are displayed\n\n        Returns:\n            class:`pandas.DataFrame`: a dataframe with a row for each dataset.\n        \"\"\"\n        catalog = self.__use_catalog(catalog)\n\n        url = f\"{self.root_url}catalogs/{catalog}/datasets\"\n        df = Fusion._call_for_dataframe(url, self.session)\n\n        if contains:\n            if isinstance(contains, list):\n                contains = \"|\".join(f\"{s}\" for s in contains)\n            if id_contains:\n                df = df[df[\"identifier\"].str.contains(contains, case=False)]\n            else:\n                df = df[\n                    df[\"identifier\"].str.contains(contains, case=False)\n                    | df[\"description\"].str.contains(contains, case=False)\n                ]\n\n        if max_results &gt; -1:\n            df = df[0:max_results]\n\n        df[\"category\"] = df.category.str.join(\", \")\n        df[\"region\"] = df.region.str.join(\", \")\n        if not display_all_columns:\n            df = df[\n                [\n                    \"identifier\",\n                    \"title\",\n                    \"region\",\n                    \"category\",\n                    \"coverageStartDate\",\n                    \"coverageEndDate\",\n                    \"description\",\n                ]\n            ]\n\n        if output:\n            print(tabulate(df, headers=\"keys\", tablefmt=\"psql\", maxcolwidths=30))\n\n        return df\n\n    def dataset_resources(\n        self, dataset: str, catalog: str = None, output: bool = False\n    ) -&gt; pd.DataFrame:\n\"\"\"List the resources available for a dataset, currently this will always be a datasetseries.\n\n        Args:\n            dataset (str): A dataset identifier\n            catalog (str, optional): A catalog identifier. Defaults to 'common'.\n            output (bool, optional): If True then print the dataframe. Defaults to False.\n\n        Returns:\n            class:`pandas.DataFrame`: A dataframe with a row for each resource\n        \"\"\"\n        catalog = self.__use_catalog(catalog)\n\n        url = f\"{self.root_url}catalogs/{catalog}/datasets/{dataset}\"\n        df = Fusion._call_for_dataframe(url, self.session)\n\n        if output:\n            print(tabulate(df, headers=\"keys\", tablefmt=\"psql\"))\n\n        return df\n\n    def list_dataset_attributes(\n        self,\n        dataset: str,\n        catalog: str = None,\n        output: bool = False,\n        display_all_columns: bool = False,\n    ) -&gt; pd.DataFrame:\n\"\"\"Returns the list of attributes that are in the dataset.\n\n        Args:\n            dataset (str): A dataset identifier\n            catalog (str, optional): A catalog identifier. Defaults to 'common'.\n            output (bool, optional): If True then print the dataframe. Defaults to False.\n            display_all_columns (bool, optional): If True displays all columns returned by the API,\n                otherwise only the key columns are displayed\n\n        Returns:\n            class:`pandas.DataFrame`: A dataframe with a row for each attribute\n        \"\"\"\n        catalog = self.__use_catalog(catalog)\n\n        url = f\"{self.root_url}catalogs/{catalog}/datasets/{dataset}/attributes\"\n        df = (\n            Fusion._call_for_dataframe(url, self.session)\n            .sort_values(by=\"index\")\n            .reset_index(drop=True)\n        )\n\n        if not display_all_columns:\n            df = df[[\"identifier\", \"dataType\", \"isDatasetKey\", \"description\"]]\n\n        if output:\n            print(tabulate(df, headers=\"keys\", tablefmt=\"psql\", maxcolwidths=30))\n\n        return df\n\n    def list_datasetmembers(\n        self,\n        dataset: str,\n        catalog: str = None,\n        output: bool = False,\n        max_results: int = -1,\n    ) -&gt; pd.DataFrame:\n\"\"\"List the available members in the dataset series.\n\n        Args:\n            dataset (str): A dataset identifier\n            catalog (str, optional): A catalog identifier. Defaults to 'common'.\n            output (bool, optional): If True then print the dataframe. Defaults to False.\n            max_results (int, optional): Limit the number of rows returned in the dataframe.\n                Defaults to -1 which returns all results.\n\n        Returns:\n            class:`pandas.DataFrame`: a dataframe with a row for each dataset member.\n        \"\"\"\n        catalog = self.__use_catalog(catalog)\n\n        url = f\"{self.root_url}catalogs/{catalog}/datasets/{dataset}/datasetseries\"\n        df = Fusion._call_for_dataframe(url, self.session)\n\n        if max_results &gt; -1:\n            df = df[0:max_results]\n\n        if output:\n            print(tabulate(df, headers=\"keys\", tablefmt=\"psql\"))\n\n        return df\n\n    def datasetmember_resources(\n        self, dataset: str, series: str, catalog: str = None, output: bool = False\n    ) -&gt; pd.DataFrame:\n\"\"\"List the available resources for a datasetseries member.\n\n        Args:\n            dataset (str): A dataset identifier\n            series (str): The datasetseries identifier\n            catalog (str, optional): A catalog identifier. Defaults to 'common'.\n            output (bool, optional): If True then print the dataframe. Defaults to False.\n\n        Returns:\n            class:`pandas.DataFrame`: A dataframe with a row for each datasetseries member resource.\n                Currently, this will always be distributions.\n        \"\"\"\n        catalog = self.__use_catalog(catalog)\n\n        url = f\"{self.root_url}catalogs/{catalog}/datasets/{dataset}/datasetseries/{series}\"\n        df = Fusion._call_for_dataframe(url, self.session)\n\n        if output:\n            print(tabulate(df, headers=\"keys\", tablefmt=\"psql\"))\n\n        return df\n\n    def list_distributions(\n        self, dataset: str, series: str, catalog: str = None, output: bool = False\n    ) -&gt; pd.DataFrame:\n\"\"\"List the available distributions (downloadable instances of the dataset with a format type).\n\n        Args:\n            dataset (str): A dataset identifier\n            series (str): The datasetseries identifier\n            catalog (str, optional): A catalog identifier. Defaults to 'common'.\n            output (bool, optional): If True then print the dataframe. Defaults to False.\n\n        Returns:\n            class:`pandas.DataFrame`: A dataframe with a row for each distribution.\n        \"\"\"\n        catalog = self.__use_catalog(catalog)\n\n        url = f\"{self.root_url}catalogs/{catalog}/datasets/{dataset}/datasetseries/{series}/distributions\"\n        df = Fusion._call_for_dataframe(url, self.session)\n\n        if output:\n            print(tabulate(df, headers=\"keys\", tablefmt=\"psql\"))\n\n        return df\n\n    def _resolve_distro_tuples(\n        self,\n        dataset: str,\n        dt_str: str = \"latest\",\n        dataset_format: str = \"parquet\",\n        catalog: str = None,\n    ):\n\"\"\"Resolve distribution tuples given specification params.\n\n        A private utility function to generate a list of distribution tuples.\n        Each tuple is a distribution, identified by catalog, dataset id,\n        datasetseries member id, and the file format.\n\n        Args:\n            dataset (str): A dataset identifier\n            dt_str (str, optional): Either a single date or a range identified by a start or end date,\n                or both separated with a \":\". Defaults to 'latest' which will return the most recent\n                instance of the dataset.\n            dataset_format (str, optional): The file format, e.g. CSV or Parquet. Defaults to 'parquet'.\n            catalog (str, optional): A catalog identifier. Defaults to 'common'.\n\n        Returns:\n            list: a list of tuples, one for each distribution\n        \"\"\"\n        catalog = self.__use_catalog(catalog)\n\n        datasetseries_list = self.list_datasetmembers(dataset, catalog)\n\n        if datasetseries_list.empty:\n            raise APIResponseError(\n                f\"No data available for dataset {dataset}. \"\n                f\"Check that a valid dataset identifier and date/date range has been set.\"\n            )\n\n        if dt_str == \"latest\":\n            dt_str = datasetseries_list.iloc[\n                datasetseries_list[\"createdDate\"].values.argmax()\n            ][\"identifier\"]\n\n        parsed_dates = normalise_dt_param_str(dt_str)\n        if len(parsed_dates) == 1:\n            parsed_dates = (parsed_dates[0], parsed_dates[0])\n\n        if parsed_dates[0]:\n            datasetseries_list = datasetseries_list[\n                datasetseries_list[\"fromDate\"] &gt;= parsed_dates[0]\n            ]\n\n        if parsed_dates[1]:\n            datasetseries_list = datasetseries_list[\n                datasetseries_list[\"toDate\"] &lt;= parsed_dates[1]\n            ]\n\n        required_series = list(datasetseries_list[\"@id\"])\n        tups = [\n            (catalog, dataset, series, dataset_format) for series in required_series\n        ]\n\n        return tups\n\n    def download(\n        self,\n        dataset: str,\n        dt_str: str = \"latest\",\n        dataset_format: str = \"parquet\",\n        catalog: str = None,\n        n_par: int = None,\n        show_progress: bool = True,\n        force_download: bool = False,\n        download_folder: str = None,\n        return_paths: bool = False,\n        partitioning: str = None,\n    ):\n\"\"\"Downloads the requested distributions of a dataset to disk.\n\n        Args:\n            dataset (str): A dataset identifier\n            dt_str (str, optional): Either a single date or a range identified by a start or end date,\n                or both separated with a \":\". Defaults to 'latest' which will return the most recent\n                instance of the dataset.\n            dataset_format (str, optional): The file format, e.g. CSV or Parquet. Defaults to 'parquet'.\n            catalog (str, optional): A catalog identifier. Defaults to 'common'.\n            n_par (int, optional): Specify how many distributions to download in parallel.\n                Defaults to all cpus available.\n            show_progress (bool, optional): Display a progress bar during data download Defaults to True.\n            force_download (bool, optional): If True then will always download a file even\n                if it is already on disk. Defaults to True.\n            download_folder (str, optional): The path, absolute or relative, where downloaded files are saved.\n                Defaults to download_folder as set in __init__\n            return_paths (bool, optional): Return paths and success statuses of the downloaded files.\n            partitioning (str, optional): Partitioning specification.\n\n        Returns:\n\n        \"\"\"\n        catalog = self.__use_catalog(catalog)\n\n        n_par = cpu_count(n_par)\n        required_series = self._resolve_distro_tuples(\n            dataset, dt_str, dataset_format, catalog\n        )\n\n        if not download_folder or not isinstance(download_folder, list):\n            download_folders = [self.download_folder] * len(required_series)\n\n        if partitioning == \"hive\":\n            members = [series[2].strip(\"/\") for series in required_series]\n            download_folders = [\n                f\"{download_folders[i]}/{series[0]}/{series[1]}/{members[i]}\"\n                for i, series in enumerate(required_series)\n            ]\n\n        for download_folder in download_folders:\n            if not self.fs.exists(download_folder):\n                self.fs.mkdir(download_folder, create_parents=True)\n        download_spec = [\n            {\n                \"credentials\": self.credentials,\n                \"url\": distribution_to_url(\n                    self.root_url, series[1], series[2], series[3], series[0]\n                ),\n                \"output_file\": distribution_to_filename(\n                    download_folders[i],\n                    series[1],\n                    series[2],\n                    series[3],\n                    series[0],\n                    partitioning=partitioning,\n                ),\n                \"overwrite\": force_download,\n                \"fs\": self.fs,\n            }\n            for i, series in enumerate(required_series)\n        ]\n\n        if show_progress:\n            loop = tqdm(download_spec)\n        else:\n            loop = download_spec\n        logger.log(\n            VERBOSE_LVL,\n            f\"Beginning {len(loop)} downloads in batches of {n_par}\",\n        )\n        res = Parallel(n_jobs=n_par)(\n            delayed(stream_single_file_new_session)(**spec) for spec in loop\n        )\n        if (len(res) &gt; 0) and (not all((r[0] for r in res))):\n            for r in res:\n                if not r[0]:\n                    warnings.warn(f\"The download of {r[1]} was not successful\")\n        return res if return_paths else None\n\n    def to_df(\n        self,\n        dataset: str,\n        dt_str: str = \"latest\",\n        dataset_format: str = \"parquet\",\n        catalog: str = None,\n        n_par: int = None,\n        show_progress: bool = True,\n        columns: List = None,\n        filters: List = None,\n        force_download: bool = False,\n        download_folder: str = None,\n        **kwargs,\n    ) -&gt; pd.DataFrame:\n\"\"\"Gets distributions for a specified date or date range and returns the data as a dataframe.\n\n        Args:\n            dataset (str): A dataset identifier\n            dt_str (str, optional): Either a single date or a range identified by a start or end date,\n                or both separated with a \":\". Defaults to 'latest' which will return the most recent\n                instance of the dataset.\n            dataset_format (str, optional): The file format, e.g. CSV or Parquet. Defaults to 'parquet'.\n            catalog (str, optional): A catalog identifier. Defaults to 'common'.\n            n_par (int, optional): Specify how many distributions to download in parallel.\n                Defaults to all cpus available.\n            show_progress (bool, optional): Display a progress bar during data download Defaults to True.\n            columns (List, optional): A list of columns to return from a parquet file. Defaults to None\n            filters (List, optional): List[Tuple] or List[List[Tuple]] or None (default)\n                Rows which do not match the filter predicate will be removed from scanned data.\n                Partition keys embedded in a nested directory structure will be exploited to avoid\n                loading files at all if they contain no matching rows. If use_legacy_dataset is True,\n                filters can only reference partition keys and only a hive-style directory structure\n                is supported. When setting use_legacy_dataset to False, also within-file level filtering\n                and different partitioning schemes are supported.\n                More on https://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetDataset.html\n            force_download (bool, optional): If True then will always download a file even\n                if it is already on disk. Defaults to False.\n            download_folder (str, optional): The path, absolute or relative, where downloaded files are saved.\n                Defaults to download_folder as set in __init__\n        Returns:\n            class:`pandas.DataFrame`: a dataframe containing the requested data.\n                If multiple dataset instances are retrieved then these are concatenated first.\n        \"\"\"\n        catalog = self.__use_catalog(catalog)\n\n        n_par = cpu_count(n_par)\n        if not download_folder:\n            download_folder = self.download_folder\n        download_res = self.download(\n            dataset,\n            dt_str,\n            dataset_format,\n            catalog,\n            n_par,\n            show_progress,\n            force_download,\n            download_folder,\n            return_paths=True,\n        )\n\n        if not all(res[0] for res in download_res):\n            failed_res = [res for res in download_res if not res[0]]\n            raise Exception(\n                f\"Not all downloads were successfully completed. \"\n                f\"Re-run to collect missing files. The following failed:\\n{failed_res}\"\n            )\n\n        files = [res[1] for res in download_res]\n\n        pd_read_fn_map = {\n            \"csv\": read_csv,\n            \"parquet\": read_parquet,\n            \"parq\": read_parquet,\n            \"json\": read_json,\n            \"raw\": read_csv,\n        }\n\n        pd_read_default_kwargs: Dict[str, Dict[str, object]] = {\n            \"csv\": {\"columns\": columns, \"filters\": filters, \"fs\": self.fs},\n            \"parquet\": {\"columns\": columns, \"filters\": filters, \"fs\": self.fs},\n            \"json\": {\"columns\": columns, \"filters\": filters, \"fs\": self.fs},\n            \"raw\": {\"columns\": columns, \"filters\": filters, \"fs\": self.fs},\n        }\n\n        pd_read_default_kwargs[\"parq\"] = pd_read_default_kwargs[\"parquet\"]\n\n        pd_reader = pd_read_fn_map.get(dataset_format)\n        pd_read_kwargs = pd_read_default_kwargs.get(dataset_format, {})\n        if not pd_reader:\n            raise Exception(\n                f\"No pandas function to read file in format {dataset_format}\"\n            )\n\n        pd_read_kwargs.update(kwargs)\n\n        if len(files) == 0:\n            raise APIResponseError(\n                f\"No series members for dataset: {dataset} \"\n                f\"in date or date range: {dt_str} and format: {dataset_format}\"\n            )\n        if dataset_format in [\"parquet\", \"parq\"]:\n            df = pd_reader(files, **pd_read_kwargs)  # type: ignore\n        elif dataset_format == \"raw\":\n            dataframes = (\n                pd.concat(\n                    [pd_reader(ZipFile(f).open(p), **pd_read_kwargs) for p in ZipFile(f).namelist()], ignore_index=True  # type: ignore\n                )  # type: ignore\n                for f in files\n            )  # type: ignore\n            df = pd.concat(dataframes, ignore_index=True)\n        else:\n            dataframes = (pd_reader(f, **pd_read_kwargs) for f in files)  # type: ignore\n            df = pd.concat(dataframes, ignore_index=True)\n\n        return df\n\n    def to_table(\n        self,\n        dataset: str,\n        dt_str: str = \"latest\",\n        dataset_format: str = \"parquet\",\n        catalog: str = None,\n        n_par: int = None,\n        show_progress: bool = True,\n        columns: List = None,\n        filters: List = None,\n        force_download: bool = False,\n        download_folder: str = None,\n        **kwargs,\n    ) -&gt; pd.DataFrame:\n\"\"\"Gets distributions for a specified date or date range and returns the data as an arrow table.\n\n        Args:\n            dataset (str): A dataset identifier\n            dt_str (str, optional): Either a single date or a range identified by a start or end date,\n                or both separated with a \":\". Defaults to 'latest' which will return the most recent\n                instance of the dataset.\n            dataset_format (str, optional): The file format, e.g. CSV or Parquet. Defaults to 'parquet'.\n            catalog (str, optional): A catalog identifier. Defaults to 'common'.\n            n_par (int, optional): Specify how many distributions to download in parallel.\n                Defaults to all cpus available.\n            show_progress (bool, optional): Display a progress bar during data download Defaults to True.\n            columns (List, optional): A list of columns to return from a parquet file. Defaults to None\n            filters (List, optional): List[Tuple] or List[List[Tuple]] or None (default)\n                Rows which do not match the filter predicate will be removed from scanned data.\n                Partition keys embedded in a nested directory structure will be exploited to avoid\n                loading files at all if they contain no matching rows. If use_legacy_dataset is True,\n                filters can only reference partition keys and only a hive-style directory structure\n                is supported. When setting use_legacy_dataset to False, also within-file level filtering\n                and different partitioning schemes are supported.\n                More on https://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetDataset.html\n            force_download (bool, optional): If True then will always download a file even\n                if it is already on disk. Defaults to False.\n            download_folder (str, optional): The path, absolute or relative, where downloaded files are saved.\n                Defaults to download_folder as set in __init__\n        Returns:\n            class:`pandas.DataFrame`: a dataframe containing the requested data.\n                If multiple dataset instances are retrieved then these are concatenated first.\n        \"\"\"\n        catalog = self.__use_catalog(catalog)\n        n_par = cpu_count(n_par)\n        if not download_folder:\n            download_folder = self.download_folder\n        download_res = self.download(\n            dataset,\n            dt_str,\n            dataset_format,\n            catalog,\n            n_par,\n            show_progress,\n            force_download,\n            download_folder,\n            return_paths=True,\n        )\n\n        if not all(res[0] for res in download_res):\n            failed_res = [res for res in download_res if not res[0]]\n            raise Exception(\n                f\"Not all downloads were successfully completed. \"\n                f\"Re-run to collect missing files. The following failed:\\n{failed_res}\"\n            )\n\n        files = [res[1] for res in download_res]\n\n        read_fn_map = {\n            \"csv\": csv_to_table,\n            \"parquet\": parquet_to_table,\n            \"parq\": parquet_to_table,\n            \"json\": json_to_table,\n            \"raw\": csv_to_table,\n        }\n\n        read_default_kwargs: Dict[str, Dict[str, object]] = {\n            \"csv\": {\"columns\": columns, \"filters\": filters, \"fs\": self.fs},\n            \"parquet\": {\"columns\": columns, \"filters\": filters, \"fs\": self.fs},\n            \"json\": {\"columns\": columns, \"filters\": filters, \"fs\": self.fs},\n            \"raw\": {\"columns\": columns, \"filters\": filters, \"fs\": self.fs},\n        }\n\n        read_default_kwargs[\"parq\"] = read_default_kwargs[\"parquet\"]\n\n        reader = read_fn_map.get(dataset_format)\n        read_kwargs = read_default_kwargs.get(dataset_format, {})\n        if not reader:\n            raise Exception(f\"No function to read file in format {dataset_format}\")\n\n        read_kwargs.update(kwargs)\n\n        if len(files) == 0:\n            raise APIResponseError(\n                f\"No series members for dataset: {dataset} \"\n                f\"in date or date range: {dt_str} and format: {dataset_format}\"\n            )\n        if dataset_format in [\"parquet\", \"parq\"]:\n            tbl = reader(files, **read_kwargs)  # type: ignore\n        else:\n            tbl = (reader(f, **read_kwargs) for f in files)  # type: ignore\n            tbl = pa.concat_tables(tbl)\n\n        return tbl\n\n    def upload(\n        self,\n        path: str,\n        dataset: str = None,\n        dt_str: str = \"latest\",\n        catalog: str = None,\n        n_par: int = None,\n        show_progress: bool = True,\n        return_paths: bool = False,\n        multipart=True,\n        chunk_size=5 * 2 ** 20,\n    ):\n\"\"\"Uploads the requested files/files to Fusion.\n\n        Args:\n            path (str): path to a file or a folder with files\n            dataset (str, optional): Dataset name to which the file will be uplaoded (for single file only).\n                                    If not provided the dataset will be implied from file's name.\n            dt_str (str, optional): A single date. Defaults to 'latest' which will return the most recent.\n                                    Relevant for a single file upload only. If not provided the dataset will\n                                    be implied from file's name.\n            catalog (str, optional): A catalog identifier. Defaults to 'common'.\n            n_par (int, optional): Specify how many distributions to download in parallel.\n                Defaults to all cpus available.\n            show_progress (bool, optional): Display a progress bar during data download Defaults to True.\n            return_paths (bool, optional): Return paths and success statuses of the downloaded files.\n            multipart (bool): Is multipart upload.\n            chunk_size (int): Maximum chunk size.\n\n        Returns:\n\n\n        \"\"\"\n        catalog = self.__use_catalog(catalog)\n\n        if not self.fs.exists(path):\n            raise RuntimeError(\"The provided path does not exist\")\n\n        fs_fusion = self.get_fusion_filesystem()\n        if self.fs.info(path)[\"type\"] == \"directory\":\n            file_path_lst = self.fs.find(path)\n            local_file_validation = validate_file_names(file_path_lst, fs_fusion)\n            file_path_lst = [\n                f for flag, f in zip(local_file_validation, file_path_lst) if flag\n            ]\n            is_raw_lst = is_dataset_raw(file_path_lst, fs_fusion)\n            local_url_eqiv = [\n                path_to_url(i, r) for i, r in zip(file_path_lst, is_raw_lst)\n            ]\n        else:\n            file_path_lst = [path]\n            if not catalog or not dataset:\n                local_file_validation = validate_file_names(file_path_lst, fs_fusion)\n                file_path_lst = [\n                    f for flag, f in zip(local_file_validation, file_path_lst) if flag\n                ]\n                is_raw_lst = is_dataset_raw(file_path_lst, fs_fusion)\n                local_url_eqiv = [\n                    path_to_url(i, r) for i, r in zip(file_path_lst, is_raw_lst)\n                ]\n            else:\n                dt_str = (\n                    dt_str\n                    if dt_str != \"latest\"\n                    else pd.Timestamp(\"today\").date().strftime(\"%Y%m%d\")\n                )\n                dt_str = pd.Timestamp(dt_str).date().strftime(\"%Y%m%d\")\n                if catalog not in fs_fusion.ls(\"\") or dataset not in [\n                    i.split(\"/\")[-1] for i in fs_fusion.ls(f\"{catalog}/datasets\")\n                ]:\n                    msg = (\n                        f\"File file has not been uploaded, one of the catalog: {catalog} \"\n                        f\"or dataset: {dataset} does not exit.\"\n                    )\n                    warnings.warn(msg)\n                    return [(False, path, Exception(msg))]\n                is_raw = js.loads(fs_fusion.cat(f\"{catalog}/datasets/{dataset}\"))[\n                    \"isRawData\"\n                ]\n                file_format = path.split(\".\")[-1]\n                local_url_eqiv = [\n                    path_to_url(f\"{dataset}__{catalog}__{dt_str}.{file_format}\", is_raw)\n                ]\n\n        df = pd.DataFrame([file_path_lst, local_url_eqiv]).T\n        df.columns = [\"path\", \"url\"]\n\n        if show_progress:\n            loop = tqdm(df.iterrows(), total=len(df))\n        else:\n            loop = df.iterrows()\n\n        n_par = cpu_count(n_par)\n        parallel = True if len(df) &gt; 1 else False\n        res = upload_files(\n            fs_fusion,\n            self.fs,\n            loop,\n            parallel=parallel,\n            n_par=n_par,\n            multipart=multipart,\n            chunk_size=chunk_size,\n        )\n\n        if not all(r[0] for r in res):\n            failed_res = [r for r in res if not r[0]]\n            msg = f\"Not all uploads were successfully completed. The following failed:\\n{failed_res}\"\n            logger.warning(msg)\n            warnings.warn(msg)\n\n        return res if return_paths else None\n</code></pre>"},{"location":"api/#fusion.fusion.Fusion.default_catalog","title":"<code>default_catalog: str</code>  <code>property</code> <code>writable</code>","text":"<p>Returns the default catalog.</p> <p>Returns:</p> Type Description <code>str</code> <p>None</p>"},{"location":"api/#fusion.fusion.Fusion.__init__","title":"<code>__init__(credentials='config/client_credentials.json', root_url='https://fusion-api.jpmorgan.com/fusion/v1/', download_folder='downloads', log_level=logging.ERROR, fs=None, log_path='.')</code>","text":"<p>Constructor to instantiate a new Fusion object.</p> <p>Parameters:</p> Name Type Description Default <code>credentials</code> <code>Union[str, dict]</code> <p>A path to a credentials file or a dictionary containing the required keys. Defaults to 'config/client_credentials.json'.</p> <code>'config/client_credentials.json'</code> <code>root_url</code> <code>_type_</code> <p>The API root URL. Defaults to \"https://fusion-api.jpmorgan.com/fusion/v1/\".</p> <code>'https://fusion-api.jpmorgan.com/fusion/v1/'</code> <code>download_folder</code> <code>str</code> <p>The folder path where downloaded data files are saved. Defaults to \"downloads\".</p> <code>'downloads'</code> <code>log_level</code> <code>int</code> <p>Set the logging level. Defaults to logging.ERROR.</p> <code>logging.ERROR</code> <code>fs</code> <code>fsspec.filesystem</code> <p>filesystem.</p> <code>None</code> <code>log_path</code> <code>str</code> <p>The folder path where the log is stored.</p> <code>'.'</code> Source code in <code>fusion\\fusion.py</code> <pre><code>def __init__(\n    self,\n    credentials: Union[str, dict] = \"config/client_credentials.json\",\n    root_url: str = \"https://fusion-api.jpmorgan.com/fusion/v1/\",\n    download_folder: str = \"downloads\",\n    log_level: int = logging.ERROR,\n    fs=None,\n    log_path: str = \".\",\n) -&gt; None:\n\"\"\"Constructor to instantiate a new Fusion object.\n\n    Args:\n        credentials (Union[str, dict], optional): A path to a credentials file or\n            a dictionary containing the required keys.\n            Defaults to 'config/client_credentials.json'.\n        root_url (_type_, optional): The API root URL.\n            Defaults to \"https://fusion-api.jpmorgan.com/fusion/v1/\".\n        download_folder (str, optional): The folder path where downloaded data files\n            are saved. Defaults to \"downloads\".\n        log_level (int, optional): Set the logging level. Defaults to logging.ERROR.\n        fs (fsspec.filesystem): filesystem.\n        log_path (str, optional): The folder path where the log is stored.\n    \"\"\"\n    self._default_catalog = \"common\"\n\n    self.root_url = root_url\n    self.download_folder = download_folder\n    Path(download_folder).mkdir(parents=True, exist_ok=True)\n\n    if logger.hasHandlers():\n        logger.handlers.clear()\n    file_handler = logging.FileHandler(\n        filename=\"{0}/{1}\".format(log_path, \"fusion_sdk.log\")\n    )\n    logging.addLevelName(VERBOSE_LVL, \"VERBOSE\")\n    stdout_handler = logging.StreamHandler(sys.stdout)\n    formatter = logging.Formatter(\n        \"%(asctime)s.%(msecs)03d %(name)s:%(levelname)s %(message)s\",\n        datefmt=\"%Y-%m-%d %H:%M:%S\",\n    )\n    stdout_handler.setFormatter(formatter)\n    logger.addHandler(stdout_handler)\n    logger.addHandler(file_handler)\n    logger.setLevel(log_level)\n\n    if isinstance(credentials, FusionCredentials):\n        self.credentials = credentials\n    else:\n        self.credentials = FusionCredentials.from_object(credentials)\n\n    self.session = get_session(self.credentials, self.root_url)\n    self.fs = fs if fs else get_default_fs()\n</code></pre>"},{"location":"api/#fusion.fusion.Fusion.__repr__","title":"<code>__repr__()</code>","text":"<p>Object representation to list all available methods.</p> Source code in <code>fusion\\fusion.py</code> <pre><code>def __repr__(self):\n\"\"\"Object representation to list all available methods.\"\"\"\n    return \"Fusion object \\nAvailable methods:\\n\" + tabulate(\n        pd.DataFrame(\n            [\n                [\n                    method_name\n                    for method_name in dir(Fusion)\n                    if callable(getattr(Fusion, method_name))\n                    and not method_name.startswith(\"_\")\n                ]\n                + [\n                    p\n                    for p in dir(Fusion)\n                    if isinstance(getattr(Fusion, p), property)\n                ],\n                [\n                    getattr(Fusion, method_name).__doc__.split(\"\\n\")[0]\n                    for method_name in dir(Fusion)\n                    if callable(getattr(Fusion, method_name))\n                    and not method_name.startswith(\"_\")\n                ]\n                + [\n                    getattr(Fusion, p).__doc__.split(\"\\n\")[0]\n                    for p in dir(Fusion)\n                    if isinstance(getattr(Fusion, p), property)\n                ],\n            ]\n        ).T.set_index(0),\n        tablefmt=\"psql\",\n    )\n</code></pre>"},{"location":"api/#fusion.fusion.Fusion.__use_catalog","title":"<code>__use_catalog(catalog)</code>","text":"<p>Determine which catalog to use in an API call.</p> <p>Parameters:</p> Name Type Description Default <code>catalog</code> <code>str</code> <p>The catalog value passed as an argument to an API function wrapper.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The catalog to use</p> Source code in <code>fusion\\fusion.py</code> <pre><code>def __use_catalog(self, catalog):\n\"\"\"Determine which catalog to use in an API call.\n\n    Args:\n        catalog (str): The catalog value passed as an argument to an API function wrapper.\n\n    Returns:\n        str: The catalog to use\n    \"\"\"\n    if catalog is None:\n        return self.default_catalog\n    else:\n        return catalog\n</code></pre>"},{"location":"api/#fusion.fusion.Fusion.catalog_resources","title":"<code>catalog_resources(catalog=None, output=False)</code>","text":"<p>List the resources contained within the catalog, for example products and datasets.</p> <p>Parameters:</p> Name Type Description Default <code>catalog</code> <code>str</code> <p>A catalog identifier. Defaults to 'common'.</p> <code>None</code> <code>output</code> <code>bool</code> <p>If True then print the dataframe. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>class</code> <code>pd.DataFrame</code> <p><code>pandas.DataFrame</code>: A dataframe with a row for each resource within the catalog</p> Source code in <code>fusion\\fusion.py</code> <pre><code>def catalog_resources(\n    self, catalog: str = None, output: bool = False\n) -&gt; pd.DataFrame:\n\"\"\"List the resources contained within the catalog, for example products and datasets.\n\n    Args:\n        catalog (str, optional): A catalog identifier. Defaults to 'common'.\n        output (bool, optional): If True then print the dataframe. Defaults to False.\n\n    Returns:\n       class:`pandas.DataFrame`: A dataframe with a row for each resource within the catalog\n    \"\"\"\n    catalog = self.__use_catalog(catalog)\n\n    url = f\"{self.root_url}catalogs/{catalog}\"\n    df = Fusion._call_for_dataframe(url, self.session)\n\n    if output:\n        print(tabulate(df, headers=\"keys\", tablefmt=\"psql\", maxcolwidths=30))\n\n    return df\n</code></pre>"},{"location":"api/#fusion.fusion.Fusion.dataset_resources","title":"<code>dataset_resources(dataset, catalog=None, output=False)</code>","text":"<p>List the resources available for a dataset, currently this will always be a datasetseries.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>str</code> <p>A dataset identifier</p> required <code>catalog</code> <code>str</code> <p>A catalog identifier. Defaults to 'common'.</p> <code>None</code> <code>output</code> <code>bool</code> <p>If True then print the dataframe. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>class</code> <code>pd.DataFrame</code> <p><code>pandas.DataFrame</code>: A dataframe with a row for each resource</p> Source code in <code>fusion\\fusion.py</code> <pre><code>def dataset_resources(\n    self, dataset: str, catalog: str = None, output: bool = False\n) -&gt; pd.DataFrame:\n\"\"\"List the resources available for a dataset, currently this will always be a datasetseries.\n\n    Args:\n        dataset (str): A dataset identifier\n        catalog (str, optional): A catalog identifier. Defaults to 'common'.\n        output (bool, optional): If True then print the dataframe. Defaults to False.\n\n    Returns:\n        class:`pandas.DataFrame`: A dataframe with a row for each resource\n    \"\"\"\n    catalog = self.__use_catalog(catalog)\n\n    url = f\"{self.root_url}catalogs/{catalog}/datasets/{dataset}\"\n    df = Fusion._call_for_dataframe(url, self.session)\n\n    if output:\n        print(tabulate(df, headers=\"keys\", tablefmt=\"psql\"))\n\n    return df\n</code></pre>"},{"location":"api/#fusion.fusion.Fusion.datasetmember_resources","title":"<code>datasetmember_resources(dataset, series, catalog=None, output=False)</code>","text":"<p>List the available resources for a datasetseries member.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>str</code> <p>A dataset identifier</p> required <code>series</code> <code>str</code> <p>The datasetseries identifier</p> required <code>catalog</code> <code>str</code> <p>A catalog identifier. Defaults to 'common'.</p> <code>None</code> <code>output</code> <code>bool</code> <p>If True then print the dataframe. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>class</code> <code>pd.DataFrame</code> <p><code>pandas.DataFrame</code>: A dataframe with a row for each datasetseries member resource. Currently, this will always be distributions.</p> Source code in <code>fusion\\fusion.py</code> <pre><code>def datasetmember_resources(\n    self, dataset: str, series: str, catalog: str = None, output: bool = False\n) -&gt; pd.DataFrame:\n\"\"\"List the available resources for a datasetseries member.\n\n    Args:\n        dataset (str): A dataset identifier\n        series (str): The datasetseries identifier\n        catalog (str, optional): A catalog identifier. Defaults to 'common'.\n        output (bool, optional): If True then print the dataframe. Defaults to False.\n\n    Returns:\n        class:`pandas.DataFrame`: A dataframe with a row for each datasetseries member resource.\n            Currently, this will always be distributions.\n    \"\"\"\n    catalog = self.__use_catalog(catalog)\n\n    url = f\"{self.root_url}catalogs/{catalog}/datasets/{dataset}/datasetseries/{series}\"\n    df = Fusion._call_for_dataframe(url, self.session)\n\n    if output:\n        print(tabulate(df, headers=\"keys\", tablefmt=\"psql\"))\n\n    return df\n</code></pre>"},{"location":"api/#fusion.fusion.Fusion.download","title":"<code>download(dataset, dt_str='latest', dataset_format='parquet', catalog=None, n_par=None, show_progress=True, force_download=False, download_folder=None, return_paths=False, partitioning=None)</code>","text":"<p>Downloads the requested distributions of a dataset to disk.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>str</code> <p>A dataset identifier</p> required <code>dt_str</code> <code>str</code> <p>Either a single date or a range identified by a start or end date, or both separated with a \":\". Defaults to 'latest' which will return the most recent instance of the dataset.</p> <code>'latest'</code> <code>dataset_format</code> <code>str</code> <p>The file format, e.g. CSV or Parquet. Defaults to 'parquet'.</p> <code>'parquet'</code> <code>catalog</code> <code>str</code> <p>A catalog identifier. Defaults to 'common'.</p> <code>None</code> <code>n_par</code> <code>int</code> <p>Specify how many distributions to download in parallel. Defaults to all cpus available.</p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>Display a progress bar during data download Defaults to True.</p> <code>True</code> <code>force_download</code> <code>bool</code> <p>If True then will always download a file even if it is already on disk. Defaults to True.</p> <code>False</code> <code>download_folder</code> <code>str</code> <p>The path, absolute or relative, where downloaded files are saved. Defaults to download_folder as set in init</p> <code>None</code> <code>return_paths</code> <code>bool</code> <p>Return paths and success statuses of the downloaded files.</p> <code>False</code> <code>partitioning</code> <code>str</code> <p>Partitioning specification.</p> <code>None</code> Source code in <code>fusion\\fusion.py</code> <pre><code>def download(\n    self,\n    dataset: str,\n    dt_str: str = \"latest\",\n    dataset_format: str = \"parquet\",\n    catalog: str = None,\n    n_par: int = None,\n    show_progress: bool = True,\n    force_download: bool = False,\n    download_folder: str = None,\n    return_paths: bool = False,\n    partitioning: str = None,\n):\n\"\"\"Downloads the requested distributions of a dataset to disk.\n\n    Args:\n        dataset (str): A dataset identifier\n        dt_str (str, optional): Either a single date or a range identified by a start or end date,\n            or both separated with a \":\". Defaults to 'latest' which will return the most recent\n            instance of the dataset.\n        dataset_format (str, optional): The file format, e.g. CSV or Parquet. Defaults to 'parquet'.\n        catalog (str, optional): A catalog identifier. Defaults to 'common'.\n        n_par (int, optional): Specify how many distributions to download in parallel.\n            Defaults to all cpus available.\n        show_progress (bool, optional): Display a progress bar during data download Defaults to True.\n        force_download (bool, optional): If True then will always download a file even\n            if it is already on disk. Defaults to True.\n        download_folder (str, optional): The path, absolute or relative, where downloaded files are saved.\n            Defaults to download_folder as set in __init__\n        return_paths (bool, optional): Return paths and success statuses of the downloaded files.\n        partitioning (str, optional): Partitioning specification.\n\n    Returns:\n\n    \"\"\"\n    catalog = self.__use_catalog(catalog)\n\n    n_par = cpu_count(n_par)\n    required_series = self._resolve_distro_tuples(\n        dataset, dt_str, dataset_format, catalog\n    )\n\n    if not download_folder or not isinstance(download_folder, list):\n        download_folders = [self.download_folder] * len(required_series)\n\n    if partitioning == \"hive\":\n        members = [series[2].strip(\"/\") for series in required_series]\n        download_folders = [\n            f\"{download_folders[i]}/{series[0]}/{series[1]}/{members[i]}\"\n            for i, series in enumerate(required_series)\n        ]\n\n    for download_folder in download_folders:\n        if not self.fs.exists(download_folder):\n            self.fs.mkdir(download_folder, create_parents=True)\n    download_spec = [\n        {\n            \"credentials\": self.credentials,\n            \"url\": distribution_to_url(\n                self.root_url, series[1], series[2], series[3], series[0]\n            ),\n            \"output_file\": distribution_to_filename(\n                download_folders[i],\n                series[1],\n                series[2],\n                series[3],\n                series[0],\n                partitioning=partitioning,\n            ),\n            \"overwrite\": force_download,\n            \"fs\": self.fs,\n        }\n        for i, series in enumerate(required_series)\n    ]\n\n    if show_progress:\n        loop = tqdm(download_spec)\n    else:\n        loop = download_spec\n    logger.log(\n        VERBOSE_LVL,\n        f\"Beginning {len(loop)} downloads in batches of {n_par}\",\n    )\n    res = Parallel(n_jobs=n_par)(\n        delayed(stream_single_file_new_session)(**spec) for spec in loop\n    )\n    if (len(res) &gt; 0) and (not all((r[0] for r in res))):\n        for r in res:\n            if not r[0]:\n                warnings.warn(f\"The download of {r[1]} was not successful\")\n    return res if return_paths else None\n</code></pre>"},{"location":"api/#fusion.fusion.Fusion.get_fusion_filesystem","title":"<code>get_fusion_filesystem()</code>","text":"<p>Creates Fusion Filesystem.</p> Source code in <code>fusion\\fusion.py</code> <pre><code>def get_fusion_filesystem(self):\n\"\"\"Creates Fusion Filesystem.\n\n    Returns: Fusion Filesystem\n\n    \"\"\"\n    return FusionHTTPFileSystem(\n        client_kwargs={\"root_url\": self.root_url, \"credentials\": self.credentials}\n    )\n</code></pre>"},{"location":"api/#fusion.fusion.Fusion.list_catalogs","title":"<code>list_catalogs(output=False)</code>","text":"<p>Lists the catalogs available to the API account.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>bool</code> <p>If True then print the dataframe. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>class</code> <code>pd.DataFrame</code> <p><code>pandas.DataFrame</code>: A dataframe with a row for each catalog</p> Source code in <code>fusion\\fusion.py</code> <pre><code>def list_catalogs(self, output: bool = False) -&gt; pd.DataFrame:\n\"\"\"Lists the catalogs available to the API account.\n\n    Args:\n        output (bool, optional): If True then print the dataframe. Defaults to False.\n\n    Returns:\n        class:`pandas.DataFrame`: A dataframe with a row for each catalog\n    \"\"\"\n    url = f\"{self.root_url}catalogs/\"\n    df = Fusion._call_for_dataframe(url, self.session)\n\n    if output:\n        print(tabulate(df, headers=\"keys\", tablefmt=\"psql\"))\n\n    return df\n</code></pre>"},{"location":"api/#fusion.fusion.Fusion.list_dataset_attributes","title":"<code>list_dataset_attributes(dataset, catalog=None, output=False, display_all_columns=False)</code>","text":"<p>Returns the list of attributes that are in the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>str</code> <p>A dataset identifier</p> required <code>catalog</code> <code>str</code> <p>A catalog identifier. Defaults to 'common'.</p> <code>None</code> <code>output</code> <code>bool</code> <p>If True then print the dataframe. Defaults to False.</p> <code>False</code> <code>display_all_columns</code> <code>bool</code> <p>If True displays all columns returned by the API, otherwise only the key columns are displayed</p> <code>False</code> <p>Returns:</p> Name Type Description <code>class</code> <code>pd.DataFrame</code> <p><code>pandas.DataFrame</code>: A dataframe with a row for each attribute</p> Source code in <code>fusion\\fusion.py</code> <pre><code>def list_dataset_attributes(\n    self,\n    dataset: str,\n    catalog: str = None,\n    output: bool = False,\n    display_all_columns: bool = False,\n) -&gt; pd.DataFrame:\n\"\"\"Returns the list of attributes that are in the dataset.\n\n    Args:\n        dataset (str): A dataset identifier\n        catalog (str, optional): A catalog identifier. Defaults to 'common'.\n        output (bool, optional): If True then print the dataframe. Defaults to False.\n        display_all_columns (bool, optional): If True displays all columns returned by the API,\n            otherwise only the key columns are displayed\n\n    Returns:\n        class:`pandas.DataFrame`: A dataframe with a row for each attribute\n    \"\"\"\n    catalog = self.__use_catalog(catalog)\n\n    url = f\"{self.root_url}catalogs/{catalog}/datasets/{dataset}/attributes\"\n    df = (\n        Fusion._call_for_dataframe(url, self.session)\n        .sort_values(by=\"index\")\n        .reset_index(drop=True)\n    )\n\n    if not display_all_columns:\n        df = df[[\"identifier\", \"dataType\", \"isDatasetKey\", \"description\"]]\n\n    if output:\n        print(tabulate(df, headers=\"keys\", tablefmt=\"psql\", maxcolwidths=30))\n\n    return df\n</code></pre>"},{"location":"api/#fusion.fusion.Fusion.list_datasetmembers","title":"<code>list_datasetmembers(dataset, catalog=None, output=False, max_results=-1)</code>","text":"<p>List the available members in the dataset series.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>str</code> <p>A dataset identifier</p> required <code>catalog</code> <code>str</code> <p>A catalog identifier. Defaults to 'common'.</p> <code>None</code> <code>output</code> <code>bool</code> <p>If True then print the dataframe. Defaults to False.</p> <code>False</code> <code>max_results</code> <code>int</code> <p>Limit the number of rows returned in the dataframe. Defaults to -1 which returns all results.</p> <code>-1</code> <p>Returns:</p> Name Type Description <code>class</code> <code>pd.DataFrame</code> <p><code>pandas.DataFrame</code>: a dataframe with a row for each dataset member.</p> Source code in <code>fusion\\fusion.py</code> <pre><code>def list_datasetmembers(\n    self,\n    dataset: str,\n    catalog: str = None,\n    output: bool = False,\n    max_results: int = -1,\n) -&gt; pd.DataFrame:\n\"\"\"List the available members in the dataset series.\n\n    Args:\n        dataset (str): A dataset identifier\n        catalog (str, optional): A catalog identifier. Defaults to 'common'.\n        output (bool, optional): If True then print the dataframe. Defaults to False.\n        max_results (int, optional): Limit the number of rows returned in the dataframe.\n            Defaults to -1 which returns all results.\n\n    Returns:\n        class:`pandas.DataFrame`: a dataframe with a row for each dataset member.\n    \"\"\"\n    catalog = self.__use_catalog(catalog)\n\n    url = f\"{self.root_url}catalogs/{catalog}/datasets/{dataset}/datasetseries\"\n    df = Fusion._call_for_dataframe(url, self.session)\n\n    if max_results &gt; -1:\n        df = df[0:max_results]\n\n    if output:\n        print(tabulate(df, headers=\"keys\", tablefmt=\"psql\"))\n\n    return df\n</code></pre>"},{"location":"api/#fusion.fusion.Fusion.list_datasets","title":"<code>list_datasets(contains=None, id_contains=False, catalog=None, output=False, max_results=-1, display_all_columns=False)</code>","text":"<p>Get the datasets contained in a catalog.</p> <p>Parameters:</p> Name Type Description Default <code>contains</code> <code>Union[str, list]</code> <p>A string or a list of strings that are dataset identifiers to filter the datasets list. If a list is provided then it will return datasets whose identifier matches any of the strings. Defaults to None.</p> <code>None</code> <code>id_contains</code> <code>bool</code> <p>Filter datasets only where the string(s) are contained in the identifier, ignoring description.</p> <code>False</code> <code>catalog</code> <code>str</code> <p>A catalog identifier. Defaults to 'common'.</p> <code>None</code> <code>output</code> <code>bool</code> <p>If True then print the dataframe. Defaults to False.</p> <code>False</code> <code>max_results</code> <code>int</code> <p>Limit the number of rows returned in the dataframe. Defaults to -1 which returns all results.</p> <code>-1</code> <code>display_all_columns</code> <code>bool</code> <p>If True displays all columns returned by the API, otherwise only the key columns are displayed</p> <code>False</code> <p>Returns:</p> Name Type Description <code>class</code> <code>pd.DataFrame</code> <p><code>pandas.DataFrame</code>: a dataframe with a row for each dataset.</p> Source code in <code>fusion\\fusion.py</code> <pre><code>def list_datasets(\n    self,\n    contains: Union[str, list] = None,\n    id_contains: bool = False,\n    catalog: str = None,\n    output: bool = False,\n    max_results: int = -1,\n    display_all_columns: bool = False,\n) -&gt; pd.DataFrame:\n\"\"\"Get the datasets contained in a catalog.\n\n    Args:\n        contains (Union[str, list], optional): A string or a list of strings that are dataset\n            identifiers to filter the datasets list. If a list is provided then it will return\n            datasets whose identifier matches any of the strings. Defaults to None.\n        id_contains (bool): Filter datasets only where the string(s) are contained in the identifier,\n            ignoring description.\n        catalog (str, optional): A catalog identifier. Defaults to 'common'.\n        output (bool, optional): If True then print the dataframe. Defaults to False.\n        max_results (int, optional): Limit the number of rows returned in the dataframe.\n            Defaults to -1 which returns all results.\n        display_all_columns (bool, optional): If True displays all columns returned by the API,\n            otherwise only the key columns are displayed\n\n    Returns:\n        class:`pandas.DataFrame`: a dataframe with a row for each dataset.\n    \"\"\"\n    catalog = self.__use_catalog(catalog)\n\n    url = f\"{self.root_url}catalogs/{catalog}/datasets\"\n    df = Fusion._call_for_dataframe(url, self.session)\n\n    if contains:\n        if isinstance(contains, list):\n            contains = \"|\".join(f\"{s}\" for s in contains)\n        if id_contains:\n            df = df[df[\"identifier\"].str.contains(contains, case=False)]\n        else:\n            df = df[\n                df[\"identifier\"].str.contains(contains, case=False)\n                | df[\"description\"].str.contains(contains, case=False)\n            ]\n\n    if max_results &gt; -1:\n        df = df[0:max_results]\n\n    df[\"category\"] = df.category.str.join(\", \")\n    df[\"region\"] = df.region.str.join(\", \")\n    if not display_all_columns:\n        df = df[\n            [\n                \"identifier\",\n                \"title\",\n                \"region\",\n                \"category\",\n                \"coverageStartDate\",\n                \"coverageEndDate\",\n                \"description\",\n            ]\n        ]\n\n    if output:\n        print(tabulate(df, headers=\"keys\", tablefmt=\"psql\", maxcolwidths=30))\n\n    return df\n</code></pre>"},{"location":"api/#fusion.fusion.Fusion.list_distributions","title":"<code>list_distributions(dataset, series, catalog=None, output=False)</code>","text":"<p>List the available distributions (downloadable instances of the dataset with a format type).</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>str</code> <p>A dataset identifier</p> required <code>series</code> <code>str</code> <p>The datasetseries identifier</p> required <code>catalog</code> <code>str</code> <p>A catalog identifier. Defaults to 'common'.</p> <code>None</code> <code>output</code> <code>bool</code> <p>If True then print the dataframe. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>class</code> <code>pd.DataFrame</code> <p><code>pandas.DataFrame</code>: A dataframe with a row for each distribution.</p> Source code in <code>fusion\\fusion.py</code> <pre><code>def list_distributions(\n    self, dataset: str, series: str, catalog: str = None, output: bool = False\n) -&gt; pd.DataFrame:\n\"\"\"List the available distributions (downloadable instances of the dataset with a format type).\n\n    Args:\n        dataset (str): A dataset identifier\n        series (str): The datasetseries identifier\n        catalog (str, optional): A catalog identifier. Defaults to 'common'.\n        output (bool, optional): If True then print the dataframe. Defaults to False.\n\n    Returns:\n        class:`pandas.DataFrame`: A dataframe with a row for each distribution.\n    \"\"\"\n    catalog = self.__use_catalog(catalog)\n\n    url = f\"{self.root_url}catalogs/{catalog}/datasets/{dataset}/datasetseries/{series}/distributions\"\n    df = Fusion._call_for_dataframe(url, self.session)\n\n    if output:\n        print(tabulate(df, headers=\"keys\", tablefmt=\"psql\"))\n\n    return df\n</code></pre>"},{"location":"api/#fusion.fusion.Fusion.list_products","title":"<code>list_products(contains=None, id_contains=False, catalog=None, output=False, max_results=-1, display_all_columns=False)</code>","text":"<p>Get the products contained in a catalog. A product is a grouping of datasets.</p> <p>Parameters:</p> Name Type Description Default <code>contains</code> <code>Union[str, list]</code> <p>A string or a list of strings that are product identifiers to filter the products list. If a list is provided then it will return products whose identifier matches any of the strings. Defaults to None.</p> <code>None</code> <code>id_contains</code> <code>bool</code> <p>Filter datasets only where the string(s) are contained in the identifier, ignoring description.</p> <code>False</code> <code>catalog</code> <code>str</code> <p>A catalog identifier. Defaults to 'common'.</p> <code>None</code> <code>output</code> <code>bool</code> <p>If True then print the dataframe. Defaults to False.</p> <code>False</code> <code>max_results</code> <code>int</code> <p>Limit the number of rows returned in the dataframe. Defaults to -1 which returns all results.</p> <code>-1</code> <code>display_all_columns</code> <code>bool</code> <p>If True displays all columns returned by the API, otherwise only the key columns are displayed</p> <code>False</code> <p>Returns:</p> Name Type Description <code>class</code> <code>pd.DataFrame</code> <p><code>pandas.DataFrame</code>: a dataframe with a row for each product</p> Source code in <code>fusion\\fusion.py</code> <pre><code>def list_products(\n    self,\n    contains: Union[str, list] = None,\n    id_contains: bool = False,\n    catalog: str = None,\n    output: bool = False,\n    max_results: int = -1,\n    display_all_columns: bool = False,\n) -&gt; pd.DataFrame:\n\"\"\"Get the products contained in a catalog. A product is a grouping of datasets.\n\n    Args:\n        contains (Union[str, list], optional): A string or a list of strings that are product\n            identifiers to filter the products list. If a list is provided then it will return\n            products whose identifier matches any of the strings. Defaults to None.\n        id_contains (bool): Filter datasets only where the string(s) are contained in the identifier,\n            ignoring description.\n        catalog (str, optional): A catalog identifier. Defaults to 'common'.\n        output (bool, optional): If True then print the dataframe. Defaults to False.\n        max_results (int, optional): Limit the number of rows returned in the dataframe.\n            Defaults to -1 which returns all results.\n        display_all_columns (bool, optional): If True displays all columns returned by the API,\n            otherwise only the key columns are displayed\n\n    Returns:\n        class:`pandas.DataFrame`: a dataframe with a row for each product\n    \"\"\"\n    catalog = self.__use_catalog(catalog)\n\n    url = f\"{self.root_url}catalogs/{catalog}/products\"\n    df = Fusion._call_for_dataframe(url, self.session)\n\n    if contains:\n        if isinstance(contains, list):\n            contains = \"|\".join(f\"{s}\" for s in contains)\n        if id_contains:\n            df = df[df[\"identifier\"].str.contains(contains, case=False)]\n        else:\n            df = df[\n                df[\"identifier\"].str.contains(contains, case=False)\n                | df[\"description\"].str.contains(contains, case=False)\n            ]\n\n    df[\"category\"] = df.category.str.join(\", \")\n    df[\"region\"] = df.region.str.join(\", \")\n    if not display_all_columns:\n        df = df[\n            [\"identifier\", \"title\", \"region\", \"category\", \"status\", \"description\"]\n        ]\n\n    if max_results &gt; -1:\n        df = df[0:max_results]\n\n    if output:\n        print(tabulate(df, headers=\"keys\", tablefmt=\"psql\", maxcolwidths=30))\n\n    return df\n</code></pre>"},{"location":"api/#fusion.fusion.Fusion.to_df","title":"<code>to_df(dataset, dt_str='latest', dataset_format='parquet', catalog=None, n_par=None, show_progress=True, columns=None, filters=None, force_download=False, download_folder=None, **kwargs)</code>","text":"<p>Gets distributions for a specified date or date range and returns the data as a dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>str</code> <p>A dataset identifier</p> required <code>dt_str</code> <code>str</code> <p>Either a single date or a range identified by a start or end date, or both separated with a \":\". Defaults to 'latest' which will return the most recent instance of the dataset.</p> <code>'latest'</code> <code>dataset_format</code> <code>str</code> <p>The file format, e.g. CSV or Parquet. Defaults to 'parquet'.</p> <code>'parquet'</code> <code>catalog</code> <code>str</code> <p>A catalog identifier. Defaults to 'common'.</p> <code>None</code> <code>n_par</code> <code>int</code> <p>Specify how many distributions to download in parallel. Defaults to all cpus available.</p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>Display a progress bar during data download Defaults to True.</p> <code>True</code> <code>columns</code> <code>List</code> <p>A list of columns to return from a parquet file. Defaults to None</p> <code>None</code> <code>filters</code> <code>List</code> <p>List[Tuple] or List[List[Tuple]] or None (default) Rows which do not match the filter predicate will be removed from scanned data. Partition keys embedded in a nested directory structure will be exploited to avoid loading files at all if they contain no matching rows. If use_legacy_dataset is True, filters can only reference partition keys and only a hive-style directory structure is supported. When setting use_legacy_dataset to False, also within-file level filtering and different partitioning schemes are supported. More on https://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetDataset.html</p> <code>None</code> <code>force_download</code> <code>bool</code> <p>If True then will always download a file even if it is already on disk. Defaults to False.</p> <code>False</code> <code>download_folder</code> <code>str</code> <p>The path, absolute or relative, where downloaded files are saved. Defaults to download_folder as set in init</p> <code>None</code> <p>Returns:</p> Name Type Description <code>class</code> <code>pd.DataFrame</code> <p><code>pandas.DataFrame</code>: a dataframe containing the requested data. If multiple dataset instances are retrieved then these are concatenated first.</p> Source code in <code>fusion\\fusion.py</code> <pre><code>def to_df(\n    self,\n    dataset: str,\n    dt_str: str = \"latest\",\n    dataset_format: str = \"parquet\",\n    catalog: str = None,\n    n_par: int = None,\n    show_progress: bool = True,\n    columns: List = None,\n    filters: List = None,\n    force_download: bool = False,\n    download_folder: str = None,\n    **kwargs,\n) -&gt; pd.DataFrame:\n\"\"\"Gets distributions for a specified date or date range and returns the data as a dataframe.\n\n    Args:\n        dataset (str): A dataset identifier\n        dt_str (str, optional): Either a single date or a range identified by a start or end date,\n            or both separated with a \":\". Defaults to 'latest' which will return the most recent\n            instance of the dataset.\n        dataset_format (str, optional): The file format, e.g. CSV or Parquet. Defaults to 'parquet'.\n        catalog (str, optional): A catalog identifier. Defaults to 'common'.\n        n_par (int, optional): Specify how many distributions to download in parallel.\n            Defaults to all cpus available.\n        show_progress (bool, optional): Display a progress bar during data download Defaults to True.\n        columns (List, optional): A list of columns to return from a parquet file. Defaults to None\n        filters (List, optional): List[Tuple] or List[List[Tuple]] or None (default)\n            Rows which do not match the filter predicate will be removed from scanned data.\n            Partition keys embedded in a nested directory structure will be exploited to avoid\n            loading files at all if they contain no matching rows. If use_legacy_dataset is True,\n            filters can only reference partition keys and only a hive-style directory structure\n            is supported. When setting use_legacy_dataset to False, also within-file level filtering\n            and different partitioning schemes are supported.\n            More on https://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetDataset.html\n        force_download (bool, optional): If True then will always download a file even\n            if it is already on disk. Defaults to False.\n        download_folder (str, optional): The path, absolute or relative, where downloaded files are saved.\n            Defaults to download_folder as set in __init__\n    Returns:\n        class:`pandas.DataFrame`: a dataframe containing the requested data.\n            If multiple dataset instances are retrieved then these are concatenated first.\n    \"\"\"\n    catalog = self.__use_catalog(catalog)\n\n    n_par = cpu_count(n_par)\n    if not download_folder:\n        download_folder = self.download_folder\n    download_res = self.download(\n        dataset,\n        dt_str,\n        dataset_format,\n        catalog,\n        n_par,\n        show_progress,\n        force_download,\n        download_folder,\n        return_paths=True,\n    )\n\n    if not all(res[0] for res in download_res):\n        failed_res = [res for res in download_res if not res[0]]\n        raise Exception(\n            f\"Not all downloads were successfully completed. \"\n            f\"Re-run to collect missing files. The following failed:\\n{failed_res}\"\n        )\n\n    files = [res[1] for res in download_res]\n\n    pd_read_fn_map = {\n        \"csv\": read_csv,\n        \"parquet\": read_parquet,\n        \"parq\": read_parquet,\n        \"json\": read_json,\n        \"raw\": read_csv,\n    }\n\n    pd_read_default_kwargs: Dict[str, Dict[str, object]] = {\n        \"csv\": {\"columns\": columns, \"filters\": filters, \"fs\": self.fs},\n        \"parquet\": {\"columns\": columns, \"filters\": filters, \"fs\": self.fs},\n        \"json\": {\"columns\": columns, \"filters\": filters, \"fs\": self.fs},\n        \"raw\": {\"columns\": columns, \"filters\": filters, \"fs\": self.fs},\n    }\n\n    pd_read_default_kwargs[\"parq\"] = pd_read_default_kwargs[\"parquet\"]\n\n    pd_reader = pd_read_fn_map.get(dataset_format)\n    pd_read_kwargs = pd_read_default_kwargs.get(dataset_format, {})\n    if not pd_reader:\n        raise Exception(\n            f\"No pandas function to read file in format {dataset_format}\"\n        )\n\n    pd_read_kwargs.update(kwargs)\n\n    if len(files) == 0:\n        raise APIResponseError(\n            f\"No series members for dataset: {dataset} \"\n            f\"in date or date range: {dt_str} and format: {dataset_format}\"\n        )\n    if dataset_format in [\"parquet\", \"parq\"]:\n        df = pd_reader(files, **pd_read_kwargs)  # type: ignore\n    elif dataset_format == \"raw\":\n        dataframes = (\n            pd.concat(\n                [pd_reader(ZipFile(f).open(p), **pd_read_kwargs) for p in ZipFile(f).namelist()], ignore_index=True  # type: ignore\n            )  # type: ignore\n            for f in files\n        )  # type: ignore\n        df = pd.concat(dataframes, ignore_index=True)\n    else:\n        dataframes = (pd_reader(f, **pd_read_kwargs) for f in files)  # type: ignore\n        df = pd.concat(dataframes, ignore_index=True)\n\n    return df\n</code></pre>"},{"location":"api/#fusion.fusion.Fusion.to_table","title":"<code>to_table(dataset, dt_str='latest', dataset_format='parquet', catalog=None, n_par=None, show_progress=True, columns=None, filters=None, force_download=False, download_folder=None, **kwargs)</code>","text":"<p>Gets distributions for a specified date or date range and returns the data as an arrow table.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>str</code> <p>A dataset identifier</p> required <code>dt_str</code> <code>str</code> <p>Either a single date or a range identified by a start or end date, or both separated with a \":\". Defaults to 'latest' which will return the most recent instance of the dataset.</p> <code>'latest'</code> <code>dataset_format</code> <code>str</code> <p>The file format, e.g. CSV or Parquet. Defaults to 'parquet'.</p> <code>'parquet'</code> <code>catalog</code> <code>str</code> <p>A catalog identifier. Defaults to 'common'.</p> <code>None</code> <code>n_par</code> <code>int</code> <p>Specify how many distributions to download in parallel. Defaults to all cpus available.</p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>Display a progress bar during data download Defaults to True.</p> <code>True</code> <code>columns</code> <code>List</code> <p>A list of columns to return from a parquet file. Defaults to None</p> <code>None</code> <code>filters</code> <code>List</code> <p>List[Tuple] or List[List[Tuple]] or None (default) Rows which do not match the filter predicate will be removed from scanned data. Partition keys embedded in a nested directory structure will be exploited to avoid loading files at all if they contain no matching rows. If use_legacy_dataset is True, filters can only reference partition keys and only a hive-style directory structure is supported. When setting use_legacy_dataset to False, also within-file level filtering and different partitioning schemes are supported. More on https://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetDataset.html</p> <code>None</code> <code>force_download</code> <code>bool</code> <p>If True then will always download a file even if it is already on disk. Defaults to False.</p> <code>False</code> <code>download_folder</code> <code>str</code> <p>The path, absolute or relative, where downloaded files are saved. Defaults to download_folder as set in init</p> <code>None</code> <p>Returns:</p> Name Type Description <code>class</code> <code>pd.DataFrame</code> <p><code>pandas.DataFrame</code>: a dataframe containing the requested data. If multiple dataset instances are retrieved then these are concatenated first.</p> Source code in <code>fusion\\fusion.py</code> <pre><code>def to_table(\n    self,\n    dataset: str,\n    dt_str: str = \"latest\",\n    dataset_format: str = \"parquet\",\n    catalog: str = None,\n    n_par: int = None,\n    show_progress: bool = True,\n    columns: List = None,\n    filters: List = None,\n    force_download: bool = False,\n    download_folder: str = None,\n    **kwargs,\n) -&gt; pd.DataFrame:\n\"\"\"Gets distributions for a specified date or date range and returns the data as an arrow table.\n\n    Args:\n        dataset (str): A dataset identifier\n        dt_str (str, optional): Either a single date or a range identified by a start or end date,\n            or both separated with a \":\". Defaults to 'latest' which will return the most recent\n            instance of the dataset.\n        dataset_format (str, optional): The file format, e.g. CSV or Parquet. Defaults to 'parquet'.\n        catalog (str, optional): A catalog identifier. Defaults to 'common'.\n        n_par (int, optional): Specify how many distributions to download in parallel.\n            Defaults to all cpus available.\n        show_progress (bool, optional): Display a progress bar during data download Defaults to True.\n        columns (List, optional): A list of columns to return from a parquet file. Defaults to None\n        filters (List, optional): List[Tuple] or List[List[Tuple]] or None (default)\n            Rows which do not match the filter predicate will be removed from scanned data.\n            Partition keys embedded in a nested directory structure will be exploited to avoid\n            loading files at all if they contain no matching rows. If use_legacy_dataset is True,\n            filters can only reference partition keys and only a hive-style directory structure\n            is supported. When setting use_legacy_dataset to False, also within-file level filtering\n            and different partitioning schemes are supported.\n            More on https://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetDataset.html\n        force_download (bool, optional): If True then will always download a file even\n            if it is already on disk. Defaults to False.\n        download_folder (str, optional): The path, absolute or relative, where downloaded files are saved.\n            Defaults to download_folder as set in __init__\n    Returns:\n        class:`pandas.DataFrame`: a dataframe containing the requested data.\n            If multiple dataset instances are retrieved then these are concatenated first.\n    \"\"\"\n    catalog = self.__use_catalog(catalog)\n    n_par = cpu_count(n_par)\n    if not download_folder:\n        download_folder = self.download_folder\n    download_res = self.download(\n        dataset,\n        dt_str,\n        dataset_format,\n        catalog,\n        n_par,\n        show_progress,\n        force_download,\n        download_folder,\n        return_paths=True,\n    )\n\n    if not all(res[0] for res in download_res):\n        failed_res = [res for res in download_res if not res[0]]\n        raise Exception(\n            f\"Not all downloads were successfully completed. \"\n            f\"Re-run to collect missing files. The following failed:\\n{failed_res}\"\n        )\n\n    files = [res[1] for res in download_res]\n\n    read_fn_map = {\n        \"csv\": csv_to_table,\n        \"parquet\": parquet_to_table,\n        \"parq\": parquet_to_table,\n        \"json\": json_to_table,\n        \"raw\": csv_to_table,\n    }\n\n    read_default_kwargs: Dict[str, Dict[str, object]] = {\n        \"csv\": {\"columns\": columns, \"filters\": filters, \"fs\": self.fs},\n        \"parquet\": {\"columns\": columns, \"filters\": filters, \"fs\": self.fs},\n        \"json\": {\"columns\": columns, \"filters\": filters, \"fs\": self.fs},\n        \"raw\": {\"columns\": columns, \"filters\": filters, \"fs\": self.fs},\n    }\n\n    read_default_kwargs[\"parq\"] = read_default_kwargs[\"parquet\"]\n\n    reader = read_fn_map.get(dataset_format)\n    read_kwargs = read_default_kwargs.get(dataset_format, {})\n    if not reader:\n        raise Exception(f\"No function to read file in format {dataset_format}\")\n\n    read_kwargs.update(kwargs)\n\n    if len(files) == 0:\n        raise APIResponseError(\n            f\"No series members for dataset: {dataset} \"\n            f\"in date or date range: {dt_str} and format: {dataset_format}\"\n        )\n    if dataset_format in [\"parquet\", \"parq\"]:\n        tbl = reader(files, **read_kwargs)  # type: ignore\n    else:\n        tbl = (reader(f, **read_kwargs) for f in files)  # type: ignore\n        tbl = pa.concat_tables(tbl)\n\n    return tbl\n</code></pre>"},{"location":"api/#fusion.fusion.Fusion.upload","title":"<code>upload(path, dataset=None, dt_str='latest', catalog=None, n_par=None, show_progress=True, return_paths=False, multipart=True, chunk_size=5 * 2 ** 20)</code>","text":"<p>Uploads the requested files/files to Fusion.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>path to a file or a folder with files</p> required <code>dataset</code> <code>str</code> <p>Dataset name to which the file will be uplaoded (for single file only).                     If not provided the dataset will be implied from file's name.</p> <code>None</code> <code>dt_str</code> <code>str</code> <p>A single date. Defaults to 'latest' which will return the most recent.                     Relevant for a single file upload only. If not provided the dataset will                     be implied from file's name.</p> <code>'latest'</code> <code>catalog</code> <code>str</code> <p>A catalog identifier. Defaults to 'common'.</p> <code>None</code> <code>n_par</code> <code>int</code> <p>Specify how many distributions to download in parallel. Defaults to all cpus available.</p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>Display a progress bar during data download Defaults to True.</p> <code>True</code> <code>return_paths</code> <code>bool</code> <p>Return paths and success statuses of the downloaded files.</p> <code>False</code> <code>multipart</code> <code>bool</code> <p>Is multipart upload.</p> <code>True</code> <code>chunk_size</code> <code>int</code> <p>Maximum chunk size.</p> <code>5 * 2 ** 20</code> Source code in <code>fusion\\fusion.py</code> <pre><code>def upload(\n    self,\n    path: str,\n    dataset: str = None,\n    dt_str: str = \"latest\",\n    catalog: str = None,\n    n_par: int = None,\n    show_progress: bool = True,\n    return_paths: bool = False,\n    multipart=True,\n    chunk_size=5 * 2 ** 20,\n):\n\"\"\"Uploads the requested files/files to Fusion.\n\n    Args:\n        path (str): path to a file or a folder with files\n        dataset (str, optional): Dataset name to which the file will be uplaoded (for single file only).\n                                If not provided the dataset will be implied from file's name.\n        dt_str (str, optional): A single date. Defaults to 'latest' which will return the most recent.\n                                Relevant for a single file upload only. If not provided the dataset will\n                                be implied from file's name.\n        catalog (str, optional): A catalog identifier. Defaults to 'common'.\n        n_par (int, optional): Specify how many distributions to download in parallel.\n            Defaults to all cpus available.\n        show_progress (bool, optional): Display a progress bar during data download Defaults to True.\n        return_paths (bool, optional): Return paths and success statuses of the downloaded files.\n        multipart (bool): Is multipart upload.\n        chunk_size (int): Maximum chunk size.\n\n    Returns:\n\n\n    \"\"\"\n    catalog = self.__use_catalog(catalog)\n\n    if not self.fs.exists(path):\n        raise RuntimeError(\"The provided path does not exist\")\n\n    fs_fusion = self.get_fusion_filesystem()\n    if self.fs.info(path)[\"type\"] == \"directory\":\n        file_path_lst = self.fs.find(path)\n        local_file_validation = validate_file_names(file_path_lst, fs_fusion)\n        file_path_lst = [\n            f for flag, f in zip(local_file_validation, file_path_lst) if flag\n        ]\n        is_raw_lst = is_dataset_raw(file_path_lst, fs_fusion)\n        local_url_eqiv = [\n            path_to_url(i, r) for i, r in zip(file_path_lst, is_raw_lst)\n        ]\n    else:\n        file_path_lst = [path]\n        if not catalog or not dataset:\n            local_file_validation = validate_file_names(file_path_lst, fs_fusion)\n            file_path_lst = [\n                f for flag, f in zip(local_file_validation, file_path_lst) if flag\n            ]\n            is_raw_lst = is_dataset_raw(file_path_lst, fs_fusion)\n            local_url_eqiv = [\n                path_to_url(i, r) for i, r in zip(file_path_lst, is_raw_lst)\n            ]\n        else:\n            dt_str = (\n                dt_str\n                if dt_str != \"latest\"\n                else pd.Timestamp(\"today\").date().strftime(\"%Y%m%d\")\n            )\n            dt_str = pd.Timestamp(dt_str).date().strftime(\"%Y%m%d\")\n            if catalog not in fs_fusion.ls(\"\") or dataset not in [\n                i.split(\"/\")[-1] for i in fs_fusion.ls(f\"{catalog}/datasets\")\n            ]:\n                msg = (\n                    f\"File file has not been uploaded, one of the catalog: {catalog} \"\n                    f\"or dataset: {dataset} does not exit.\"\n                )\n                warnings.warn(msg)\n                return [(False, path, Exception(msg))]\n            is_raw = js.loads(fs_fusion.cat(f\"{catalog}/datasets/{dataset}\"))[\n                \"isRawData\"\n            ]\n            file_format = path.split(\".\")[-1]\n            local_url_eqiv = [\n                path_to_url(f\"{dataset}__{catalog}__{dt_str}.{file_format}\", is_raw)\n            ]\n\n    df = pd.DataFrame([file_path_lst, local_url_eqiv]).T\n    df.columns = [\"path\", \"url\"]\n\n    if show_progress:\n        loop = tqdm(df.iterrows(), total=len(df))\n    else:\n        loop = df.iterrows()\n\n    n_par = cpu_count(n_par)\n    parallel = True if len(df) &gt; 1 else False\n    res = upload_files(\n        fs_fusion,\n        self.fs,\n        loop,\n        parallel=parallel,\n        n_par=n_par,\n        multipart=multipart,\n        chunk_size=chunk_size,\n    )\n\n    if not all(r[0] for r in res):\n        failed_res = [r for r in res if not r[0]]\n        msg = f\"Not all uploads were successfully completed. The following failed:\\n{failed_res}\"\n        logger.warning(msg)\n        warnings.warn(msg)\n\n    return res if return_paths else None\n</code></pre>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#changelog","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#1011-2023-03-23","title":"[1.0.11] - 2023-03-23","text":"<ul> <li>support bearer token authentication</li> <li>fix proxy support to aiohttp</li> <li>fix filtering support for csv and json</li> </ul>"},{"location":"changelog/#1011-2023-05-09","title":"[1.0.11] - 2023-05-09","text":"<ul> <li>bearer token authentication</li> <li>proxy bug fix for aiohttp</li> </ul>"},{"location":"changelog/#1010-2023-03-23","title":"[1.0.10] - 2023-03-23","text":"<ul> <li>md5 to sha256 convention change</li> <li>fsync continuous updates bug fix</li> <li>to_table function addition</li> <li>saving files in a hive friendly folder structure</li> <li>new bearer token add for download/upload operations</li> <li>raw data upload functionality fix</li> </ul>"},{"location":"changelog/#109-2023-01-23","title":"[1.0.9] - 2023-01-23","text":"<ul> <li>operational enhancements</li> </ul>"},{"location":"changelog/#108-2023-01-19","title":"[1.0.8] - 2023-01-19","text":"<ul> <li>cloud storage compatibility</li> </ul>"},{"location":"changelog/#107-2023-01-12","title":"[1.0.7] - 2023-01-12","text":"<ul> <li>Multi-part upload</li> <li>fsync</li> </ul>"},{"location":"changelog/#106-2022-11-21","title":"[1.0.6] - 2022-11-21","text":"<ul> <li>Support setting of the default catalog</li> <li>Fusion filesystem module</li> <li>Upload functionality</li> <li>Folder traversing for credentials</li> <li>Filters for parquet and csv file opening</li> </ul>"},{"location":"changelog/#105-2022-06-22","title":"[1.0.5] - 2022-06-22","text":"<ul> <li>Add support for internal auth methods</li> </ul>"},{"location":"changelog/#104-2022-05-19","title":"[1.0.4] - 2022-05-19","text":"<ul> <li>Support proxy servers in auth post requests</li> <li>Add back support for '2020-01-01' and '20200101' date formats</li> <li>Various bug fixes</li> <li>Streamline credentials creation</li> </ul>"},{"location":"changelog/#103-2022-05-12","title":"[1.0.3] - 2022-05-12","text":"<ul> <li>Add support for 'latest' datasets</li> </ul>"},{"location":"changelog/#102-2022-05-12","title":"[1.0.2] - 2022-05-12","text":"<ul> <li>Integrate build with docs </li> </ul>"},{"location":"changelog/#101-2022-05-12","title":"[1.0.1] - 2022-05-12","text":"<ul> <li>First live release on JPMC gitub</li> </ul>"},{"location":"contributing/","title":"Contributing","text":""},{"location":"contributing/#contributing","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/{{ cookiecutter.github_username }}/{{ cookiecutter.project_slug }}/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>{{ cookiecutter.project_name }} could always use more documentation, whether as part of the official {{ cookiecutter.project_name }} docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/{{ cookiecutter.github_username }}/{{ cookiecutter.project_slug }}/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions   are welcome :)</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up <code>{{ cookiecutter.project_slug }}</code> for local development.</p> <ol> <li>Fork the <code>{{ cookiecutter.project_slug }}</code> repo on GitHub.</li> <li> <p>Clone your fork locally</p> <pre><code>$ git clone git@github.com:your_name_here/{{ cookiecutter.project_slug }}.git\n</code></pre> </li> <li> <p>Ensure poetry is installed.</p> </li> <li> <p>Install dependencies and start your virtualenv:</p> <pre><code>$ poetry install -E test -E doc -E dev\n</code></pre> </li> <li> <p>Create a branch for local development:</p> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> </li> <li> <p>When you're done making changes, check that your changes pass the    tests, including testing other Python versions, with tox:</p> <pre><code>$ poetry run tox\n</code></pre> </li> <li> <p>Commit your changes and push your branch to GitHub:</p> <pre><code>$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> </li> <li> <p>Submit a pull request through the GitHub website.</p> </li> </ol>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated. Put    your new functionality into a function with a docstring, and add the    feature to the list in README.md.</li> <li>The pull request should work for Python 3.7, 3.8 and 3.9. Check    https://github.com/{{ cookiecutter.github_username }}/{{ cookiecutter.project_slug }}/actions    and make sure that the tests pass for all supported Python versions.</li> </ol>"},{"location":"contributing/#tips","title":"Tips","text":"<pre><code>$ poetry run pytest tests/test_{{ cookiecutter.pkg_name }}.py\n</code></pre> <p>To run a subset of tests.</p>"},{"location":"contributing/#deploying","title":"Deploying","text":"<p>A reminder for the maintainers on how to deploy. Make sure all your changes are committed (including an entry in CHANGELOG.md). Then run:</p> <pre><code>$ poetry run bump2version patch # possible: major / minor / patch\n$ git push\n$ git push --tags\n</code></pre> <p>GitHub Actions will then deploy to PyPI if tests pass.</p>"},{"location":"get_started/","title":"Fusion - get started","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom fusion import Fusion\n\nplt.style.use(\"bmh\")\n</pre> import pandas as pd import matplotlib.pyplot as plt from fusion import Fusion  plt.style.use(\"bmh\") In\u00a0[2]: Copied! <pre>fusion = Fusion()\n</pre> fusion = Fusion() In\u00a0[3]: Copied! <pre>fusion\n</pre> fusion Out[3]: <pre>Fusion object \nAvailable methods:\n+-------------------------+\n| catalog_resources       |\n| dataset_resources       |\n| datasetmember_resources |\n| download                |\n| get_fusion_filesystem   |\n| list_catalogs           |\n| list_dataset_attributes |\n| list_datasetmembers     |\n| list_datasets           |\n| list_distributions      |\n| list_products           |\n| to_df                   |\n| upload                  |\n| default_catalog         |\n+-------------------------+</pre> In\u00a0[4]: Copied! <pre>fusion.to_df?\n</pre> fusion.to_df? <pre>Signature:\nfusion.to_df(\n    dataset: str,\n    dt_str: str = 'latest',\n    dataset_format: str = 'parquet',\n    catalog: str = None,\n    n_par: int = None,\n    show_progress: bool = True,\n    columns: List = None,\n    filters: List = None,\n    force_download: bool = False,\n    download_folder: str = None,\n**kwargs,\n) -&gt; pandas.core.frame.DataFrame\nDocstring:\nGets distributions for a specified date or date range and returns the data as a dataframe.\n\nArgs:\n    dataset (str): A dataset identifier\n    dt_str (str, optional): Either a single date or a range identified by a start or end date,\n        or both separated with a \":\". Defaults to 'latest' which will return the most recent\n        instance of the dataset.\n    dataset_format (str, optional): The file format, e.g. CSV or Parquet. Defaults to 'parquet'.\n    catalog (str, optional): A catalog identifier. Defaults to 'common'.\n    n_par (int, optional): Specify how many distributions to download in parallel.\n        Defaults to all cpus available.\n    show_progress (bool, optional): Display a progress bar during data download Defaults to True.\n    columns (List, optional): A list of columns to return from a parquet file. Defaults to None\n    filters (List, optional): List[Tuple] or List[List[Tuple]] or None (default)\n        Rows which do not match the filter predicate will be removed from scanned data.\n        Partition keys embedded in a nested directory structure will be exploited to avoid\n        loading files at all if they contain no matching rows. If use_legacy_dataset is True,\n        filters can only reference partition keys and only a hive-style directory structure\n        is supported. When setting use_legacy_dataset to False, also within-file level filtering\n        and different partitioning schemes are supported.\n        More on https://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetDataset.html\n    force_download (bool, optional): If True then will always download a file even\n        if it is already on disk. Defaults to False.\n    download_folder (str, optional): The path, absolute or relative, where downloaded files are saved.\n        Defaults to download_folder as set in __init__\nReturns:\n    class:`pandas.DataFrame`: a dataframe containing the requested data.\n        If multiple dataset instances are retrieved then these are concatenated first.\nFile:      d:\\dev\\fusion\\fusion\\fusion.py\nType:      method\n</pre> In\u00a0[5]: Copied! <pre>fusion.list_datasets(\"FX\")\n</pre> fusion.list_datasets(\"FX\") Out[5]: identifier title region category coverageStartDate coverageEndDate description 7 FX_EASIDX Economic Activity Surprise Index (EASI) FX EMEA, North America, APAC, Emerging Markets, G... Economics 2019-01-01 2023-01-04 The Economic Activity Surprise Index is publis... 15 FX_MEAN_IMM FX Mean Reversion Strategies IMM EMEA, North America, APAC, Emerging Markets, G... FX 2019-01-01 2023-01-04 The FX Mean Reversion, IMM dataset from J.P. M... 19 FXO_SP FX Cash Rate EMEA, North America, APAC, Emerging Markets, G... FX 2019-01-01 2023-01-04 This dataset includes FX spot rates for major ... 20 FXO_RR FX Option Structure | Risk Reversal EMEA, North America, APAC, Emerging Markets, G... FX 2019-01-01 2023-01-04 Implied volatility for 10 and 25 delta FX opti... 21 FXO_ST FX Option Structure | Strangles EMEA, North America, APAC, Emerging Markets, G... FX 2019-01-01 2023-01-04 Implied volatility for 10 and 25 delta FX opti... 34 FX-ECV-Post-Trade FX ECV Post-Trade Report Global FX NaN NaN FX ECV Post-Trade Report. 46 FX_ECONOMIC FX Specialized | Momentum Strategies (Economics) EMEA, North America, APAC, Emerging Markets, G... FX 2019-01-01 2023-01-04 Momentum signals in a trend following strategy... 63 FXO_IV FX Specialized | Option Implied Volatility EMEA, North America, APAC, Emerging Markets, G... FX 2019-01-01 2023-01-04 Global FX option implied volatility data for a... 74 FX_JPM_TCI FX Passive Index EMEA, North America, APAC, Global FX 2019-01-01 2023-01-04 FX passive index level and currency sub-indices. 75 FX_MEAN_HFFV FX Mean Reversion Strategies Hi Freq Fair Value EMEA, North America, APAC, Global FX 2019-01-01 2023-01-04 The FX High Frequency Fair Value dataset from ... In\u00a0[6]: Copied! <pre>fusion.list_dataset_attributes(\"FXO_SP\")\n</pre> fusion.list_dataset_attributes(\"FXO_SP\") Out[6]: identifier dataType isDatasetKey description 0 instrument_name String True The instrument name 1 currency_pair String False The currency pair 2 term String False The time period of an investment, agreement or... 3 product String False The product identifier 4 date String False The snapshot date 5 fx_rate Double False The spot and forward fx rate In\u00a0[7]: Copied! <pre>df = fusion.to_df(\"FXO_SP\", \"20220101:20221231\", columns=[\"currency_pair\", \"date\", \"fx_rate\"], filters=[(\"currency_pair\", \"=\", \"GBPUSD\")])\n</pre> df = fusion.to_df(\"FXO_SP\", \"20220101:20221231\", columns=[\"currency_pair\", \"date\", \"fx_rate\"], filters=[(\"currency_pair\", \"=\", \"GBPUSD\")]) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 258/258 [00:03&lt;00:00, 68.80it/s]\n</pre> In\u00a0[8]: Copied! <pre>df.head()\n</pre> df.head() Out[8]: currency_pair date fx_rate 0 GBPUSD 20220318 1.31705 1 GBPUSD 20220322 1.32590 2 GBPUSD 20220103 1.34475 3 GBPUSD 20220104 1.35530 4 GBPUSD 20220105 1.35660 In\u00a0[9]: Copied! <pre>df[\"date\"] = pd.to_datetime(df[\"date\"].astype(\"str\"))\ndf.sort_values(\"date\").set_index(\"date\").plot(grid=True);\n</pre> df[\"date\"] = pd.to_datetime(df[\"date\"].astype(\"str\")) df.sort_values(\"date\").set_index(\"date\").plot(grid=True);"},{"location":"get_started/#fusion-get-started","title":"Fusion - get started\u00b6","text":""},{"location":"get_started/#establish-the-connection","title":"Establish the connection\u00b6","text":""},{"location":"get_started/#show-the-available-functionality","title":"Show the available functionality\u00b6","text":""},{"location":"get_started/#access-function-documentation","title":"Access function documentation\u00b6","text":""},{"location":"get_started/#explore-the-datasets","title":"Explore the datasets\u00b6","text":""},{"location":"get_started/#display-the-attributes","title":"Display the attributes\u00b6","text":""},{"location":"get_started/#download-and-load","title":"Download and load\u00b6","text":""},{"location":"get_started/#analyze","title":"Analyze\u00b6","text":""},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#installation","title":"Installation","text":""},{"location":"installation/#stable-release","title":"Stable release","text":"<p>To install PyFusion, run this command in your terminal:</p> <pre><code>$ pip install pyfusion\n</code></pre> <p>This is the preferred method to install PyFusion, as it will always install the most recent stable release.</p> <p>If you don't have pip installed, this Python installation guide can guide you through the process.</p>"},{"location":"installation/#from-source","title":"From source","text":"<p>The source for PyFusion can be downloaded from the Github repo.</p> <p>You can either clone the public repository:</p> <pre><code>$ git clone git://github.com/jpmorganchase/fusion\n</code></pre> <p>Or download the tarball:</p> <pre><code>$ curl -OJL https://github.com/jpmorganchase/fusion/tarball/master\n</code></pre> <p>Once you have a copy of the source, you can install it with:</p> <pre><code>$ pip install .\n</code></pre>"},{"location":"usage/","title":"Usage","text":""},{"location":"usage/#pyfusion","title":"PyFusion","text":"<p>PyFusion is the Python SDK for the Fusion platform API. </p>"},{"location":"usage/#installation","title":"Installation","text":"<pre><code>pip install pyfusion\n</code></pre>"}]}