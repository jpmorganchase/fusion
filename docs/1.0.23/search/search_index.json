{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#pyfusion","title":"PyFusion","text":"<p>PyFusion is the Python SDK for the Fusion platform API. </p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install pyfusion\n</code></pre> <p>Fusion by J.P. Morgan is a cloud-native data platform for institutional investors, providing end-to-end data management, analytics, and reporting solutions across the investment lifecycle. The platform allows clients to seamlessly integrate and combine data from multiple sources into a single data model that delivers the benefits and scale and reduces costs, along with the ability to more easily unlock timely analysis and insights. Fusion's open data architecture supports flexible distribution, including partnerships with cloud and data providers, all managed by J.P. Morgan data experts. </p> <p>For more information, please visit fusion.jpmorgan.com</p> <p>For the SDK documentation, please visit page</p>"},{"location":"api/","title":"Modules","text":"<p>Main Fusion module.</p> <p>Synchronisation between the local filesystem and Fusion.</p> <p>Parameters:</p> Name Type Description Default <code>fs_fusion</code> <code>fsspec.filesystem</code> <p>Fusion filesystem.</p> required <code>fs_local</code> <code>fsspec.filesystem</code> <p>Local filesystem.</p> required <code>products</code> <code>list</code> <p>List of products.</p> <code>None</code> <code>datasets</code> <code>list</code> <p>List of datasets.</p> <code>None</code> <code>catalog</code> <code>str</code> <p>Fusion catalog.</p> <code>None</code> <code>direction</code> <code>str</code> <p>Direction of synchronisation: upload/download.</p> <code>'upload'</code> <code>flatten</code> <code>bool</code> <p>Flatten the folder structure.</p> <code>False</code> <code>dataset_format</code> <code>str</code> <p>Dataset format for upload/download.</p> <code>None</code> <code>n_par</code> <code>int</code> <p>Specify how many distributions to download in parallel. Defaults to all.</p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>Display a progress bar during data download Defaults to True.</p> <code>True</code> <code>local_path</code> <code>str</code> <p>path to files in the local filesystem, e.g., \"s3a://my_bucket/\"</p> <code>''</code> <code>log_level</code> <p>Logging level. Error level by default.</p> <code>logging.ERROR</code> <code>log_path</code> <code>str</code> <p>The folder path where the log is stored.</p> <code>'.'</code> Source code in <code>fusion/fs_sync.py</code> <pre><code>def fsync(\n    fs_fusion: fsspec.filesystem,\n    fs_local: fsspec.filesystem,\n    products: list = None,\n    datasets: list = None,\n    catalog: str = None,\n    direction: str = \"upload\",\n    flatten=False,\n    dataset_format=None,\n    n_par=None,\n    show_progress=True,\n    local_path=\"\",\n    log_level=logging.ERROR,\n    log_path: str = \".\",\n):\n\"\"\"Synchronisation between the local filesystem and Fusion.\n\n    Args:\n        fs_fusion (fsspec.filesystem): Fusion filesystem.\n        fs_local (fsspec.filesystem): Local filesystem.\n        products (list): List of products.\n        datasets (list): List of datasets.\n        catalog (str): Fusion catalog.\n        direction (str): Direction of synchronisation: upload/download.\n        flatten (bool): Flatten the folder structure.\n        dataset_format (str): Dataset format for upload/download.\n        n_par (int, optional): Specify how many distributions to download in parallel. Defaults to all.\n        show_progress (bool, optional): Display a progress bar during data download Defaults to True.\n        local_path (str, optional): path to files in the local filesystem, e.g., \"s3a://my_bucket/\"\n        log_level: Logging level. Error level by default.\n        log_path (str, optional): The folder path where the log is stored.\n\n    Returns:\n\n    \"\"\"\n\n    if logger.hasHandlers():\n        logger.handlers.clear()\n    file_handler = logging.FileHandler(\n        filename=\"{0}/{1}\".format(log_path, \"fusion_fsync.log\")\n    )\n    logging.addLevelName(VERBOSE_LVL, \"VERBOSE\")\n    stdout_handler = logging.StreamHandler(sys.stdout)\n    formatter = logging.Formatter(\n        \"%(asctime)s.%(msecs)03d %(name)s:%(levelname)s %(message)s\",\n        datefmt=\"%Y-%m-%d %H:%M:%S\",\n    )\n    stdout_handler.setFormatter(formatter)\n    logger.addHandler(stdout_handler)\n    logger.addHandler(file_handler)\n    logger.setLevel(log_level)\n\n    catalog = \"common\" if not catalog else catalog\n    datasets = [] if not datasets else datasets\n    products = [] if not products else products\n\n    assert (\n        len(products) &gt; 0 or len(datasets) &gt; 0\n    ), \"At least one list products or datasets should be non-empty.\"\n    assert direction in [\n        \"upload\",\n        \"download\",\n    ], \"The direction must be either upload or download.\"\n\n    if len(local_path) &gt; 0 and local_path[-1] != \"/\":\n        local_path += \"/\"\n\n    for product in products:\n        res = json.loads(fs_fusion.cat(f\"{catalog}/products/{product}\").decode())\n        datasets += [r[\"identifier\"] for r in res[\"resources\"]]\n\n    assert len(datasets) &gt; 0, \"The supplied products did not contain any datasets.\"\n\n    local_state = pd.DataFrame()\n    fusion_state = pd.DataFrame()\n    while True:\n        try:\n            local_state_temp = _get_local_state(\n                fs_local,\n                fs_fusion,\n                datasets,\n                catalog,\n                dataset_format,\n                local_state,\n                local_path,\n            )\n            fusion_state_temp = _get_fusion_df(\n                fs_fusion, datasets, catalog, flatten, dataset_format\n            )\n            if not local_state_temp.equals(local_state) or not fusion_state_temp.equals(\n                fusion_state\n            ):\n                res = _synchronize(\n                    fs_fusion,\n                    fs_local,\n                    local_state_temp,\n                    fusion_state_temp,\n                    direction,\n                    n_par,\n                    show_progress,\n                    local_path,\n                )\n                if len(res) == 0 or all((i[0] for i in res)):\n                    local_state = local_state_temp\n                    fusion_state = fusion_state_temp\n\n                if not all(r[0] for r in res):\n                    failed_res = [r for r in res if not r[0]]\n                    msg = f\"Not all {direction}s were successfully completed. The following failed:\\n{failed_res}\"\n                    errs = [r for r in res if not r[2]]\n                    logger.warning(msg)\n                    logger.warning(errs)\n                    warnings.warn(msg)\n\n            else:\n                logger.info(\"All synced, sleeping\")\n                time.sleep(10)\n\n        except KeyboardInterrupt:\n            if input(\"Type exit to exit: \") != \"exit\":\n                continue\n            break\n\n        except Exception as ex:\n            logger.error(\"%s Issue occurred: %s\", type(ex), ex)\n            continue\n</code></pre>"},{"location":"api/#fusion.fusion.Fusion","title":"<code>Fusion</code>","text":"<p>Core Fusion class for API access.</p> Source code in <code>fusion/fusion.py</code> <pre><code>class Fusion:\n\"\"\"Core Fusion class for API access.\"\"\"\n\n    @staticmethod\n    def _call_for_dataframe(url: str, session: requests.Session) -&gt; pd.DataFrame:\n\"\"\"Private function that calls an API endpoint and returns the data as a pandas dataframe.\n\n        Args:\n            url (Union[FusionCredentials, Union[str, dict]): URL for an API endpoint with valid parameters.\n            session (requests.Session): Specify a proxy if required to access the authentication server. Defaults to {}.\n\n        Returns:\n            pandas.DataFrame: a dataframe containing the requested data.\n        \"\"\"\n        response = session.get(url)\n        response.raise_for_status()\n        table = response.json()[\"resources\"]\n        df = pd.DataFrame(table).reset_index(drop=True)\n        return df\n\n    @staticmethod\n    def _call_for_bytes_object(url: str, session: requests.Session) -&gt; BytesIO:\n\"\"\"Private function that calls an API endpoint and returns the data as a bytes object in memory.\n\n        Args:\n            url (Union[FusionCredentials, Union[str, dict]): URL for an API endpoint with valid parameters.\n            session (requests.Session): Specify a proxy if required to access the authentication server. Defaults to {}.\n\n        Returns:\n            io.BytesIO: in memory file content\n        \"\"\"\n\n        response = session.get(url)\n        response.raise_for_status()\n\n        return BytesIO(response.content)\n\n    def __init__(\n        self,\n        credentials: Union[str, dict] = \"config/client_credentials.json\",\n        root_url: str = \"https://fusion-api.jpmorgan.com/fusion/v1/\",\n        download_folder: str = \"downloads\",\n        log_level: int = logging.ERROR,\n        fs=None,\n        log_path: str = \".\",\n    ) -&gt; None:\n\"\"\"Constructor to instantiate a new Fusion object.\n\n        Args:\n            credentials (Union[str, dict], optional): A path to a credentials file or\n                a dictionary containing the required keys.\n                Defaults to 'config/client_credentials.json'.\n            root_url (_type_, optional): The API root URL.\n                Defaults to \"https://fusion-api.jpmorgan.com/fusion/v1/\".\n            download_folder (str, optional): The folder path where downloaded data files\n                are saved. Defaults to \"downloads\".\n            log_level (int, optional): Set the logging level. Defaults to logging.ERROR.\n            fs (fsspec.filesystem): filesystem.\n            log_path (str, optional): The folder path where the log is stored.\n        \"\"\"\n        self._default_catalog = \"common\"\n\n        self.root_url = root_url\n        self.download_folder = download_folder\n        Path(download_folder).mkdir(parents=True, exist_ok=True)\n\n        if logger.hasHandlers():\n            logger.handlers.clear()\n        file_handler = logging.FileHandler(filename=f\"{log_path}/fusion_sdk.log\")\n        logging.addLevelName(VERBOSE_LVL, \"VERBOSE\")\n        stdout_handler = logging.StreamHandler(sys.stdout)\n        formatter = logging.Formatter(\n            \"%(asctime)s.%(msecs)03d %(name)s:%(levelname)s %(message)s\",\n            datefmt=\"%Y-%m-%d %H:%M:%S\",\n        )\n        stdout_handler.setFormatter(formatter)\n        logger.addHandler(stdout_handler)\n        logger.addHandler(file_handler)\n        logger.setLevel(log_level)\n\n        if isinstance(credentials, FusionCredentials):\n            self.credentials = credentials\n        else:\n            self.credentials = FusionCredentials.from_object(credentials)\n\n        self.session = get_session(self.credentials, self.root_url)\n        self.fs = fs if fs else get_default_fs()\n        self.events = None\n\n    def __repr__(self):\n\"\"\"Object representation to list all available methods.\"\"\"\n        return \"Fusion object \\nAvailable methods:\\n\" + tabulate(\n            pd.DataFrame(\n                [\n                    [\n                        method_name\n                        for method_name in dir(Fusion)\n                        if callable(getattr(Fusion, method_name))\n                        and not method_name.startswith(\"_\")\n                    ]\n                    + [\n                        p\n                        for p in dir(Fusion)\n                        if isinstance(getattr(Fusion, p), property)\n                    ],\n                    [\n                        getattr(Fusion, method_name).__doc__.split(\"\\n\")[0]\n                        for method_name in dir(Fusion)\n                        if callable(getattr(Fusion, method_name))\n                        and not method_name.startswith(\"_\")\n                    ]\n                    + [\n                        getattr(Fusion, p).__doc__.split(\"\\n\")[0]\n                        for p in dir(Fusion)\n                        if isinstance(getattr(Fusion, p), property)\n                    ],\n                ]\n            ).T.set_index(0),\n            tablefmt=\"psql\",\n        )\n\n    @property\n    def default_catalog(self) -&gt; str:\n\"\"\"Returns the default catalog.\n\n        Returns:\n            None\n        \"\"\"\n        return self._default_catalog\n\n    @default_catalog.setter\n    def default_catalog(self, catalog) -&gt; None:\n\"\"\"Allow the default catalog, which is \"common\" to be overridden.\n\n        Args:\n            catalog (str): The catalog to use as the default\n\n        Returns:\n            None\n        \"\"\"\n        self._default_catalog = catalog\n\n    def __use_catalog(self, catalog):\n\"\"\"Determine which catalog to use in an API call.\n\n        Args:\n            catalog (str): The catalog value passed as an argument to an API function wrapper.\n\n        Returns:\n            str: The catalog to use\n        \"\"\"\n        if catalog is None:\n            return self.default_catalog\n\n        return catalog\n\n    def get_fusion_filesystem(self):\n\"\"\"Creates Fusion Filesystem.\n\n        Returns: Fusion Filesystem\n\n        \"\"\"\n        return FusionHTTPFileSystem(\n            client_kwargs={\"root_url\": self.root_url, \"credentials\": self.credentials}\n        )\n\n    def list_catalogs(self, output: bool = False) -&gt; pd.DataFrame:\n\"\"\"Lists the catalogs available to the API account.\n\n        Args:\n            output (bool, optional): If True then print the dataframe. Defaults to False.\n\n        Returns:\n            class:`pandas.DataFrame`: A dataframe with a row for each catalog\n        \"\"\"\n        url = f\"{self.root_url}catalogs/\"\n        df = Fusion._call_for_dataframe(url, self.session)\n\n        if output:\n            print(tabulate(df, headers=\"keys\", tablefmt=\"psql\"))\n\n        return df\n\n    def catalog_resources(\n        self, catalog: str = None, output: bool = False\n    ) -&gt; pd.DataFrame:\n\"\"\"List the resources contained within the catalog, for example products and datasets.\n\n        Args:\n            catalog (str, optional): A catalog identifier. Defaults to 'common'.\n            output (bool, optional): If True then print the dataframe. Defaults to False.\n\n        Returns:\n           class:`pandas.DataFrame`: A dataframe with a row for each resource within the catalog\n        \"\"\"\n        catalog = self.__use_catalog(catalog)\n\n        url = f\"{self.root_url}catalogs/{catalog}\"\n        df = Fusion._call_for_dataframe(url, self.session)\n\n        if output:\n            print(tabulate(df, headers=\"keys\", tablefmt=\"psql\", maxcolwidths=30))\n\n        return df\n\n    def list_products(\n        self,\n        contains: Union[str, list] = None,\n        id_contains: bool = False,\n        catalog: str = None,\n        output: bool = False,\n        max_results: int = -1,\n        display_all_columns: bool = False,\n    ) -&gt; pd.DataFrame:\n\"\"\"Get the products contained in a catalog. A product is a grouping of datasets.\n\n        Args:\n            contains (Union[str, list], optional): A string or a list of strings that are product\n                identifiers to filter the products list. If a list is provided then it will return\n                products whose identifier matches any of the strings. Defaults to None.\n            id_contains (bool): Filter datasets only where the string(s) are contained in the identifier,\n                ignoring description.\n            catalog (str, optional): A catalog identifier. Defaults to 'common'.\n            output (bool, optional): If True then print the dataframe. Defaults to False.\n            max_results (int, optional): Limit the number of rows returned in the dataframe.\n                Defaults to -1 which returns all results.\n            display_all_columns (bool, optional): If True displays all columns returned by the API,\n                otherwise only the key columns are displayed\n\n        Returns:\n            class:`pandas.DataFrame`: a dataframe with a row for each product\n        \"\"\"\n        catalog = self.__use_catalog(catalog)\n\n        url = f\"{self.root_url}catalogs/{catalog}/products\"\n        df: pd.DataFrame = Fusion._call_for_dataframe(url, self.session)\n\n        if contains:\n            if isinstance(contains, list):\n                contains = \"|\".join(f\"{s}\" for s in contains)\n            if id_contains:\n                df = df[df[\"identifier\"].str.contains(contains, case=False)]\n            else:\n                df = df[\n                    df[\"identifier\"].str.contains(contains, case=False)\n                    | df[\"description\"].str.contains(contains, case=False)\n                ]\n\n        df[\"category\"] = df.category.str.join(\", \")\n        df[\"region\"] = df.region.str.join(\", \")\n        if not display_all_columns:\n            df = df[\n                df.columns.intersection(\n                    [\n                        \"identifier\",\n                        \"title\",\n                        \"region\",\n                        \"category\",\n                        \"status\",\n                        \"description\",\n                    ]\n                )\n            ]\n\n        if max_results &gt; -1:\n            df = df[0:max_results]\n\n        if output:\n            print(tabulate(df, headers=\"keys\", tablefmt=\"psql\", maxcolwidths=30))\n\n        return df\n\n    def list_datasets(\n        self,\n        contains: Union[str, list] = None,\n        id_contains: bool = False,\n        product: Union[str, list] = None,\n        catalog: str = None,\n        output: bool = False,\n        max_results: int = -1,\n        display_all_columns: bool = False,\n        status: str = None,\n    ) -&gt; pd.DataFrame:\n\"\"\"Get the datasets contained in a catalog.\n\n        Args:\n            contains (Union[str, list], optional): A string or a list of strings that are dataset\n                identifiers to filter the datasets list. If a list is provided then it will return\n                datasets whose identifier matches any of the strings. Defaults to None.\n            id_contains (bool): Filter datasets only where the string(s) are contained in the identifier,\n                ignoring description.\n            product (Union[str, list], optional): A string or a list of strings that are product\n                identifiers to filter the datasets list. Defaults to None.\n            catalog (str, optional): A catalog identifier. Defaults to 'common'.\n            output (bool, optional): If True then print the dataframe. Defaults to False.\n            max_results (int, optional): Limit the number of rows returned in the dataframe.\n                Defaults to -1 which returns all results.\n            display_all_columns (bool, optional): If True displays all columns returned by the API,\n                otherwise only the key columns are displayed\n            status (str, optional): filter the datasets by status, default is to show all results.\n\n        Returns:\n            class:`pandas.DataFrame`: a dataframe with a row for each dataset.\n        \"\"\"\n        catalog = self.__use_catalog(catalog)\n\n        url = f\"{self.root_url}catalogs/{catalog}/datasets\"\n        df = Fusion._call_for_dataframe(url, self.session)\n\n        if contains:\n            if isinstance(contains, list):\n                contains = \"|\".join(f\"{s}\" for s in contains)\n            if id_contains:\n                df = df[df[\"identifier\"].str.contains(contains, case=False)]\n            else:\n                df = df[\n                    df[\"identifier\"].str.contains(contains, case=False)\n                    | df[\"description\"].str.contains(contains, case=False)\n                ]\n\n        if product:\n            url = f\"{self.root_url}catalogs/{catalog}/productDatasets\"\n            df_pr = Fusion._call_for_dataframe(url, self.session)\n            df_pr = (\n                df_pr[df_pr[\"product\"] == product]\n                if isinstance(product, str)\n                else df_pr[df_pr[\"product\"].isin(product)]\n            )\n            df = df[df[\"identifier\"].isin(df_pr[\"dataset\"])].reset_index(drop=True)\n\n        if max_results &gt; -1:\n            df = df[0:max_results]\n\n        df[\"category\"] = df.category.str.join(\", \")\n        df[\"region\"] = df.region.str.join(\", \")\n        if not display_all_columns:\n            cols = [\n                \"identifier\",\n                \"title\",\n                \"containerType\",\n                \"region\",\n                \"category\",\n                \"coverageStartDate\",\n                \"coverageEndDate\",\n                \"description\",\n                \"status\",\n            ]\n            cols = [c for c in cols if c in df.columns]\n            df = df[cols]\n\n        if status is not None:\n            df = df[df[\"status\"] == status]\n\n        if output:\n            print(tabulate(df, headers=\"keys\", tablefmt=\"psql\", maxcolwidths=30))\n\n        return df\n\n    def dataset_resources(\n        self, dataset: str, catalog: str = None, output: bool = False\n    ) -&gt; pd.DataFrame:\n\"\"\"List the resources available for a dataset, currently this will always be a datasetseries.\n\n        Args:\n            dataset (str): A dataset identifier\n            catalog (str, optional): A catalog identifier. Defaults to 'common'.\n            output (bool, optional): If True then print the dataframe. Defaults to False.\n\n        Returns:\n            class:`pandas.DataFrame`: A dataframe with a row for each resource\n        \"\"\"\n        catalog = self.__use_catalog(catalog)\n\n        url = f\"{self.root_url}catalogs/{catalog}/datasets/{dataset}\"\n        df = Fusion._call_for_dataframe(url, self.session)\n\n        if output:\n            print(tabulate(df, headers=\"keys\", tablefmt=\"psql\"))\n\n        return df\n\n    def list_dataset_attributes(\n        self,\n        dataset: str,\n        catalog: str = None,\n        output: bool = False,\n        display_all_columns: bool = False,\n    ) -&gt; pd.DataFrame:\n\"\"\"Returns the list of attributes that are in the dataset.\n\n        Args:\n            dataset (str): A dataset identifier\n            catalog (str, optional): A catalog identifier. Defaults to 'common'.\n            output (bool, optional): If True then print the dataframe. Defaults to False.\n            display_all_columns (bool, optional): If True displays all columns returned by the API,\n                otherwise only the key columns are displayed\n\n        Returns:\n            class:`pandas.DataFrame`: A dataframe with a row for each attribute\n        \"\"\"\n        catalog = self.__use_catalog(catalog)\n\n        url = f\"{self.root_url}catalogs/{catalog}/datasets/{dataset}/attributes\"\n        df = (\n            Fusion._call_for_dataframe(url, self.session)\n            .sort_values(by=\"index\")\n            .reset_index(drop=True)\n        )\n\n        if not display_all_columns:\n            df = df[\n                df.columns.intersection(\n                    [\n                        \"identifier\",\n                        \"title\",\n                        \"dataType\",\n                        \"isDatasetKey\",\n                        \"description\",\n                        \"source\",\n                    ]\n                )\n            ]\n\n        if output:\n            print(tabulate(df, headers=\"keys\", tablefmt=\"psql\", maxcolwidths=30))\n\n        return df\n\n    def list_datasetmembers(\n        self,\n        dataset: str,\n        catalog: str = None,\n        output: bool = False,\n        max_results: int = -1,\n    ) -&gt; pd.DataFrame:\n\"\"\"List the available members in the dataset series.\n\n        Args:\n            dataset (str): A dataset identifier\n            catalog (str, optional): A catalog identifier. Defaults to 'common'.\n            output (bool, optional): If True then print the dataframe. Defaults to False.\n            max_results (int, optional): Limit the number of rows returned in the dataframe.\n                Defaults to -1 which returns all results.\n\n        Returns:\n            class:`pandas.DataFrame`: a dataframe with a row for each dataset member.\n        \"\"\"\n        catalog = self.__use_catalog(catalog)\n\n        url = f\"{self.root_url}catalogs/{catalog}/datasets/{dataset}/datasetseries\"\n        df = Fusion._call_for_dataframe(url, self.session)\n\n        if max_results &gt; -1:\n            df = df[0:max_results]\n\n        if output:\n            print(tabulate(df, headers=\"keys\", tablefmt=\"psql\"))\n\n        return df\n\n    def datasetmember_resources(\n        self, dataset: str, series: str, catalog: str = None, output: bool = False\n    ) -&gt; pd.DataFrame:\n\"\"\"List the available resources for a datasetseries member.\n\n        Args:\n            dataset (str): A dataset identifier\n            series (str): The datasetseries identifier\n            catalog (str, optional): A catalog identifier. Defaults to 'common'.\n            output (bool, optional): If True then print the dataframe. Defaults to False.\n\n        Returns:\n            class:`pandas.DataFrame`: A dataframe with a row for each datasetseries member resource.\n                Currently, this will always be distributions.\n        \"\"\"\n        catalog = self.__use_catalog(catalog)\n\n        url = f\"{self.root_url}catalogs/{catalog}/datasets/{dataset}/datasetseries/{series}\"\n        df = Fusion._call_for_dataframe(url, self.session)\n\n        if output:\n            print(tabulate(df, headers=\"keys\", tablefmt=\"psql\"))\n\n        return df\n\n    def list_distributions(\n        self, dataset: str, series: str, catalog: str = None, output: bool = False\n    ) -&gt; pd.DataFrame:\n\"\"\"List the available distributions (downloadable instances of the dataset with a format type).\n\n        Args:\n            dataset (str): A dataset identifier\n            series (str): The datasetseries identifier\n            catalog (str, optional): A catalog identifier. Defaults to 'common'.\n            output (bool, optional): If True then print the dataframe. Defaults to False.\n\n        Returns:\n            class:`pandas.DataFrame`: A dataframe with a row for each distribution.\n        \"\"\"\n        catalog = self.__use_catalog(catalog)\n\n        url = f\"{self.root_url}catalogs/{catalog}/datasets/{dataset}/datasetseries/{series}/distributions\"\n        df = Fusion._call_for_dataframe(url, self.session)\n\n        if output:\n            print(tabulate(df, headers=\"keys\", tablefmt=\"psql\"))\n\n        return df\n\n    def _resolve_distro_tuples(\n        self,\n        dataset: str,\n        dt_str: str = \"latest\",\n        dataset_format: str = \"parquet\",\n        catalog: str = None,\n    ):\n\"\"\"Resolve distribution tuples given specification params.\n\n        A private utility function to generate a list of distribution tuples.\n        Each tuple is a distribution, identified by catalog, dataset id,\n        datasetseries member id, and the file format.\n\n        Args:\n            dataset (str): A dataset identifier\n            dt_str (str, optional): Either a single date or a range identified by a start or end date,\n                or both separated with a \":\". Defaults to 'latest' which will return the most recent\n                instance of the dataset.\n            dataset_format (str, optional): The file format, e.g. CSV or Parquet. Defaults to 'parquet'.\n            catalog (str, optional): A catalog identifier. Defaults to 'common'.\n\n        Returns:\n            list: a list of tuples, one for each distribution\n        \"\"\"\n        catalog = self.__use_catalog(catalog)\n\n        datasetseries_list = self.list_datasetmembers(dataset, catalog)\n        if len(datasetseries_list) == 0:\n            raise AssertionError(\n                f\"There are no dataset members for dataset {dataset} in catalog {catalog}\"\n            )\n\n        if datasetseries_list.empty:\n            raise APIResponseError(\n                f\"No data available for dataset {dataset}. \"\n                f\"Check that a valid dataset identifier and date/date range has been set.\"\n            )\n\n        if dt_str == \"latest\":\n            dt_str = datasetseries_list.iloc[\n                datasetseries_list[\"createdDate\"].values.argmax()\n            ][\"identifier\"]\n\n        parsed_dates = normalise_dt_param_str(dt_str)\n        if len(parsed_dates) == 1:\n            parsed_dates = (parsed_dates[0], parsed_dates[0])\n\n        if parsed_dates[0]:\n            datasetseries_list = datasetseries_list[\n                pd.to_datetime(datasetseries_list[\"identifier\"]) &gt;= parsed_dates[0]\n            ]\n\n        if parsed_dates[1]:\n            datasetseries_list = datasetseries_list[\n                pd.to_datetime(datasetseries_list[\"identifier\"]) &lt;= parsed_dates[1]\n            ]\n\n        if len(datasetseries_list) == 0:\n            raise APIResponseError(\n                f\"No data available for dataset {dataset} in catalog {catalog}.\\n\"\n                f\"Check that a valid dataset identifier and date/date range has been set.\"\n            )\n\n        required_series = list(datasetseries_list[\"@id\"])\n        tups = [\n            (catalog, dataset, series, dataset_format) for series in required_series\n        ]\n\n        return tups\n\n    def download(\n        self,\n        dataset: str,\n        dt_str: str = \"latest\",\n        dataset_format: str = \"parquet\",\n        catalog: str = None,\n        n_par: int = None,\n        show_progress: bool = True,\n        force_download: bool = False,\n        download_folder: str = None,\n        return_paths: bool = False,\n        partitioning: str = None,\n    ):\n\"\"\"Downloads the requested distributions of a dataset to disk.\n\n        Args:\n            dataset (str): A dataset identifier\n            dt_str (str, optional): Either a single date or a range identified by a start or end date,\n                or both separated with a \":\". Defaults to 'latest' which will return the most recent\n                instance of the dataset.\n            dataset_format (str, optional): The file format, e.g. CSV or Parquet. Defaults to 'parquet'.\n            catalog (str, optional): A catalog identifier. Defaults to 'common'.\n            n_par (int, optional): Specify how many distributions to download in parallel.\n                Defaults to all cpus available.\n            show_progress (bool, optional): Display a progress bar during data download Defaults to True.\n            force_download (bool, optional): If True then will always download a file even\n                if it is already on disk. Defaults to True.\n            download_folder (str, optional): The path, absolute or relative, where downloaded files are saved.\n                Defaults to download_folder as set in __init__\n            return_paths (bool, optional): Return paths and success statuses of the downloaded files.\n            partitioning (str, optional): Partitioning specification.\n\n        Returns:\n\n        \"\"\"\n        catalog = self.__use_catalog(catalog)\n\n        valid_date_range = re.compile(\n            r\"^(\\d{4}\\d{2}\\d{2})$|^((\\d{4}\\d{2}\\d{2})?([:])(\\d{4}\\d{2}\\d{2})?)$\"\n        )\n\n        if valid_date_range.match(dt_str) or dt_str == \"latest\":\n            required_series = self._resolve_distro_tuples(\n                dataset, dt_str, dataset_format, catalog\n            )\n        else:\n            # sample data is limited to csv\n            if dt_str == \"sample\":\n                dataset_format = \"csv\"\n            required_series = [(catalog, dataset, dt_str, dataset_format)]\n\n        if not download_folder:\n            download_folder = self.download_folder\n\n        download_folders = [download_folder] * len(required_series)\n\n        if partitioning == \"hive\":\n            members = [series[2].strip(\"/\") for series in required_series]\n            download_folders = [\n                f\"{download_folders[i]}/{series[0]}/{series[1]}/{members[i]}\"\n                for i, series in enumerate(required_series)\n            ]\n\n        for d in download_folders:\n            if not self.fs.exists(d):\n                self.fs.mkdir(d, create_parents=True)\n\n        if len(required_series) == 1:\n            n_par = cpu_count(n_par, is_threading=True)\n            with tqdm(total=1) as pbar:\n                output_file = distribution_to_filename(\n                    download_folders[0],\n                    required_series[0][1],\n                    required_series[0][2],\n                    required_series[0][3],\n                    required_series[0][0],\n                    partitioning=partitioning,\n                )\n                res = download_single_file_threading(\n                    self.credentials,\n                    distribution_to_url(\n                        self.root_url,\n                        required_series[0][1],\n                        required_series[0][2],\n                        required_series[0][3],\n                        required_series[0][0],\n                        is_download=True,\n                    ),\n                    output_file,\n                    fs=self.fs,\n                    max_threads=n_par,\n                )\n                if (len(res) &gt; 0) and all((r[0] for r in res)):\n                    pbar.update(1)\n                    res = [(res[0][0], output_file, res[0][2])]\n\n        else:\n            n_par = cpu_count(n_par)\n            download_spec = [\n                {\n                    \"credentials\": self.credentials,\n                    \"url\": distribution_to_url(\n                        self.root_url,\n                        series[1],\n                        series[2],\n                        series[3],\n                        series[0],\n                        is_download=True,\n                    ),\n                    \"output_file\": distribution_to_filename(\n                        download_folders[i],\n                        series[1],\n                        series[2],\n                        series[3],\n                        series[0],\n                        partitioning=partitioning,\n                    ),\n                    \"overwrite\": force_download,\n                    \"fs\": self.fs,\n                }\n                for i, series in enumerate(required_series)\n            ]\n\n            logger.log(\n                VERBOSE_LVL,\n                f\"Beginning {len(download_spec)} downloads in batches of {n_par}\",\n            )\n            if show_progress:\n                with tqdm_joblib(tqdm(total=len(download_spec))) as _:\n                    res = Parallel(n_jobs=n_par)(\n                        delayed(stream_single_file_new_session)(**spec)\n                        for spec in download_spec\n                    )\n            else:\n                res = Parallel(n_jobs=n_par)(\n                    delayed(stream_single_file_new_session)(**spec)\n                    for spec in download_spec\n                )\n\n        if (len(res) &gt; 0) and (not all((r[0] for r in res))):\n            for r in res:\n                if not r[0]:\n                    warnings.warn(f\"The download of {r[1]} was not successful\")\n        return res if return_paths else None\n\n    def to_df(\n        self,\n        dataset: str,\n        dt_str: str = \"latest\",\n        dataset_format: str = \"parquet\",\n        catalog: str = None,\n        n_par: int = None,\n        show_progress: bool = True,\n        columns: List = None,\n        filters: List = None,\n        force_download: bool = False,\n        download_folder: str = None,\n        dataframe_type: str = \"pandas\",\n        **kwargs,\n    ) -&gt; pd.DataFrame:\n\"\"\"Gets distributions for a specified date or date range and returns the data as a dataframe.\n\n        Args:\n            dataset (str): A dataset identifier\n            dt_str (str, optional): Either a single date or a range identified by a start or end date,\n                or both separated with a \":\". Defaults to 'latest' which will return the most recent\n                instance of the dataset.\n            dataset_format (str, optional): The file format, e.g. CSV or Parquet. Defaults to 'parquet'.\n            catalog (str, optional): A catalog identifier. Defaults to 'common'.\n            n_par (int, optional): Specify how many distributions to download in parallel.\n                Defaults to all cpus available.\n            show_progress (bool, optional): Display a progress bar during data download Defaults to True.\n            columns (List, optional): A list of columns to return from a parquet file. Defaults to None\n            filters (List, optional): List[Tuple] or List[List[Tuple]] or None (default)\n                Rows which do not match the filter predicate will be removed from scanned data.\n                Partition keys embedded in a nested directory structure will be exploited to avoid\n                loading files at all if they contain no matching rows. If use_legacy_dataset is True,\n                filters can only reference partition keys and only a hive-style directory structure\n                is supported. When setting use_legacy_dataset to False, also within-file level filtering\n                and different partitioning schemes are supported.\n                More on https://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetDataset.html\n            force_download (bool, optional): If True then will always download a file even\n                if it is already on disk. Defaults to False.\n            download_folder (str, optional): The path, absolute or relative, where downloaded files are saved.\n                Defaults to download_folder as set in __init__\n            dataframe_type (str, optional): Type\n        Returns:\n            class:`pandas.DataFrame`: a dataframe containing the requested data.\n                If multiple dataset instances are retrieved then these are concatenated first.\n        \"\"\"\n        catalog = self.__use_catalog(catalog)\n\n        # sample data is limited to csv\n        if dt_str == \"sample\":\n            dataset_format = \"csv\"\n\n        if not download_folder:\n            download_folder = self.download_folder\n        download_res = self.download(\n            dataset,\n            dt_str,\n            dataset_format,\n            catalog,\n            n_par,\n            show_progress,\n            force_download,\n            download_folder,\n            return_paths=True,\n        )\n\n        if not all(res[0] for res in download_res):\n            failed_res = [res for res in download_res if not res[0]]\n            raise Exception(\n                f\"Not all downloads were successfully completed. \"\n                f\"Re-run to collect missing files. The following failed:\\n{failed_res}\"\n            )\n\n        files = [res[1] for res in download_res]\n\n        pd_read_fn_map = {\n            \"csv\": read_csv,\n            \"parquet\": read_parquet,\n            \"parq\": read_parquet,\n            \"json\": read_json,\n            \"raw\": read_csv,\n        }\n\n        pd_read_default_kwargs: Dict[str, Dict[str, object]] = {\n            \"csv\": {\n                \"columns\": columns,\n                \"filters\": filters,\n                \"fs\": self.fs,\n                \"dataframe_type\": dataframe_type,\n            },\n            \"parquet\": {\n                \"columns\": columns,\n                \"filters\": filters,\n                \"fs\": self.fs,\n                \"dataframe_type\": dataframe_type,\n            },\n            \"json\": {\n                \"columns\": columns,\n                \"filters\": filters,\n                \"fs\": self.fs,\n                \"dataframe_type\": dataframe_type,\n            },\n            \"raw\": {\n                \"columns\": columns,\n                \"filters\": filters,\n                \"fs\": self.fs,\n                \"dataframe_type\": dataframe_type,\n            },\n        }\n\n        pd_read_default_kwargs[\"parq\"] = pd_read_default_kwargs[\"parquet\"]\n\n        pd_reader = pd_read_fn_map.get(dataset_format)\n        pd_read_kwargs = pd_read_default_kwargs.get(dataset_format, {})\n        if not pd_reader:\n            raise Exception(\n                f\"No pandas function to read file in format {dataset_format}\"\n            )\n\n        pd_read_kwargs.update(kwargs)\n\n        if len(files) == 0:\n            raise APIResponseError(\n                f\"No series members for dataset: {dataset} \"\n                f\"in date or date range: {dt_str} and format: {dataset_format}\"\n            )\n        if dataset_format in [\"parquet\", \"parq\"]:\n            df = pd_reader(files, **pd_read_kwargs)  # type: ignore\n        elif dataset_format == \"raw\":\n            dataframes = (\n                pd.concat(\n                    [pd_reader(ZipFile(f).open(p), **pd_read_kwargs) for p in ZipFile(f).namelist()], ignore_index=True  # type: ignore\n                )  # type: ignore\n                for f in files\n            )  # type: ignore\n            df = pd.concat(dataframes, ignore_index=True)\n        else:\n            dataframes = (pd_reader(f, **pd_read_kwargs) for f in files)  # type: ignore\n            if dataframe_type == \"pandas\":\n                df = pd.concat(dataframes, ignore_index=True)\n            if dataframe_type == \"polars\":\n                import polars as pl\n\n                df = pl.concat(dataframes, how=\"diagonal\")\n\n        return df\n\n    def to_bytes(\n        self,\n        dataset: str,\n        series_member: str,\n        dataset_format: str = \"parquet\",\n        catalog: str = None,\n    ) -&gt; BytesIO:\n\"\"\"Returns an instance of dataset (the distribution) as a bytes object.\n\n        Args:\n            dataset (str): A dataset identifier\n            series_member (str,): A dataset series member identifier\n            dataset_format (str, optional): The file format, e.g. CSV or Parquet. Defaults to 'parquet'.\n            catalog (str, optional): A catalog identifier. Defaults to 'common'.\n        \"\"\"\n\n        catalog = self.__use_catalog(catalog)\n\n        url = distribution_to_url(\n            self.root_url, dataset, series_member, dataset_format, catalog  # type: ignore\n        )\n\n        return Fusion._call_for_bytes_object(url, self.session)\n\n    def to_table(\n        self,\n        dataset: str,\n        dt_str: str = \"latest\",\n        dataset_format: str = \"parquet\",\n        catalog: str = None,\n        n_par: int = None,\n        show_progress: bool = True,\n        columns: List = None,\n        filters: List = None,\n        force_download: bool = False,\n        download_folder: str = None,\n        **kwargs,\n    ) -&gt; pd.DataFrame:\n\"\"\"Gets distributions for a specified date or date range and returns the data as an arrow table.\n\n        Args:\n            dataset (str): A dataset identifier\n            dt_str (str, optional): Either a single date or a range identified by a start or end date,\n                or both separated with a \":\". Defaults to 'latest' which will return the most recent\n                instance of the dataset.\n            dataset_format (str, optional): The file format, e.g. CSV or Parquet. Defaults to 'parquet'.\n            catalog (str, optional): A catalog identifier. Defaults to 'common'.\n            n_par (int, optional): Specify how many distributions to download in parallel.\n                Defaults to all cpus available.\n            show_progress (bool, optional): Display a progress bar during data download Defaults to True.\n            columns (List, optional): A list of columns to return from a parquet file. Defaults to None\n            filters (List, optional): List[Tuple] or List[List[Tuple]] or None (default)\n                Rows which do not match the filter predicate will be removed from scanned data.\n                Partition keys embedded in a nested directory structure will be exploited to avoid\n                loading files at all if they contain no matching rows. If use_legacy_dataset is True,\n                filters can only reference partition keys and only a hive-style directory structure\n                is supported. When setting use_legacy_dataset to False, also within-file level filtering\n                and different partitioning schemes are supported.\n                More on https://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetDataset.html\n            force_download (bool, optional): If True then will always download a file even\n                if it is already on disk. Defaults to False.\n            download_folder (str, optional): The path, absolute or relative, where downloaded files are saved.\n                Defaults to download_folder as set in __init__\n        Returns:\n            class:`pandas.DataFrame`: a dataframe containing the requested data.\n                If multiple dataset instances are retrieved then these are concatenated first.\n        \"\"\"\n        catalog = self.__use_catalog(catalog)\n        n_par = cpu_count(n_par)\n        if not download_folder:\n            download_folder = self.download_folder\n        download_res = self.download(\n            dataset,\n            dt_str,\n            dataset_format,\n            catalog,\n            n_par,\n            show_progress,\n            force_download,\n            download_folder,\n            return_paths=True,\n        )\n\n        if not all(res[0] for res in download_res):\n            failed_res = [res for res in download_res if not res[0]]\n            raise RuntimeError(\n                f\"Not all downloads were successfully completed. \"\n                f\"Re-run to collect missing files. The following failed:\\n{failed_res}\"\n            )\n\n        files = [res[1] for res in download_res]\n\n        read_fn_map = {\n            \"csv\": csv_to_table,\n            \"parquet\": parquet_to_table,\n            \"parq\": parquet_to_table,\n            \"json\": json_to_table,\n            \"raw\": csv_to_table,\n        }\n\n        read_default_kwargs: Dict[str, Dict[str, object]] = {\n            \"csv\": {\"columns\": columns, \"filters\": filters, \"fs\": self.fs},\n            \"parquet\": {\"columns\": columns, \"filters\": filters, \"fs\": self.fs},\n            \"json\": {\"columns\": columns, \"filters\": filters, \"fs\": self.fs},\n            \"raw\": {\"columns\": columns, \"filters\": filters, \"fs\": self.fs},\n        }\n\n        read_default_kwargs[\"parq\"] = read_default_kwargs[\"parquet\"]\n\n        reader = read_fn_map.get(dataset_format)\n        read_kwargs = read_default_kwargs.get(dataset_format, {})\n        if not reader:\n            raise AssertionError(f\"No function to read file in format {dataset_format}\")\n\n        read_kwargs.update(kwargs)\n\n        if len(files) == 0:\n            raise APIResponseError(\n                f\"No series members for dataset: {dataset} \"\n                f\"in date or date range: {dt_str} and format: {dataset_format}\"\n            )\n        if dataset_format in [\"parquet\", \"parq\"]:\n            tbl = reader(files, **read_kwargs)  # type: ignore\n        else:\n            tbl = (reader(f, **read_kwargs) for f in files)  # type: ignore\n            tbl = pa.concat_tables(tbl)\n\n        return tbl\n\n    def upload(\n        self,\n        path: str,\n        dataset: str = None,\n        dt_str: str = \"latest\",\n        catalog: str = None,\n        n_par: int = None,\n        show_progress: bool = True,\n        return_paths: bool = False,\n        multipart=True,\n        chunk_size=5 * 2**20,\n        from_date=None,\n        to_date=None,\n    ):\n\"\"\"Uploads the requested files/files to Fusion.\n\n        Args:\n            path (str): path to a file or a folder with files\n            dataset (str, optional): Dataset name to which the file will be uplaoded (for single file only).\n                                    If not provided the dataset will be implied from file's name.\n            dt_str (str, optional): A single date. Defaults to 'latest' which will return the most recent.\n                                    Relevant for a single file upload only. If not provided the dataset will\n                                    be implied from file's name.\n            catalog (str, optional): A catalog identifier. Defaults to 'common'.\n            n_par (int, optional): Specify how many distributions to download in parallel.\n                Defaults to all cpus available.\n            show_progress (bool, optional): Display a progress bar during data download Defaults to True.\n            return_paths (bool, optional): Return paths and success statuses of the downloaded files.\n            multipart (bool, optional): Is multipart upload.\n            chunk_size (int, optional): Maximum chunk size.\n            from_date (str, optional): start of the data date range contained in the distribution, defaults to upoad date\n            to_date (str, optional): end of the data date range contained in the distribution, defaults to upload date.\n\n        Returns:\n\n\n        \"\"\"\n        catalog = self.__use_catalog(catalog)\n\n        if not self.fs.exists(path):\n            raise RuntimeError(\"The provided path does not exist\")\n\n        fs_fusion = self.get_fusion_filesystem()\n        if self.fs.info(path)[\"type\"] == \"directory\":\n            file_path_lst = self.fs.find(path)\n            local_file_validation = validate_file_names(file_path_lst, fs_fusion)\n            file_path_lst = [\n                f for flag, f in zip(local_file_validation, file_path_lst) if flag\n            ]\n            is_raw_lst = is_dataset_raw(file_path_lst, fs_fusion)\n            local_url_eqiv = [\n                path_to_url(i, r) for i, r in zip(file_path_lst, is_raw_lst)\n            ]\n        else:\n            file_path_lst = [path]\n            if not catalog or not dataset:\n                local_file_validation = validate_file_names(file_path_lst, fs_fusion)\n                file_path_lst = [\n                    f for flag, f in zip(local_file_validation, file_path_lst) if flag\n                ]\n                is_raw_lst = is_dataset_raw(file_path_lst, fs_fusion)\n                local_url_eqiv = [\n                    path_to_url(i, r) for i, r in zip(file_path_lst, is_raw_lst)\n                ]\n            else:\n                date_identifier = re.compile(r\"^(\\d{4})(\\d{2})(\\d{2})$\")\n                if date_identifier.match(dt_str):\n                    dt_str = (\n                        dt_str\n                        if dt_str != \"latest\"\n                        else pd.Timestamp(\"today\").date().strftime(\"%Y%m%d\")\n                    )\n                    dt_str = pd.Timestamp(dt_str).date().strftime(\"%Y%m%d\")\n\n                if catalog not in fs_fusion.ls(\"\") or dataset not in [\n                    i.split(\"/\")[-1] for i in fs_fusion.ls(f\"{catalog}/datasets\")\n                ]:\n                    msg = (\n                        f\"File file has not been uploaded, one of the catalog: {catalog} \"\n                        f\"or dataset: {dataset} does not exit.\"\n                    )\n                    warnings.warn(msg)\n                    return [(False, path, Exception(msg))]\n                is_raw = js.loads(fs_fusion.cat(f\"{catalog}/datasets/{dataset}\"))[\n                    \"isRawData\"\n                ]\n                file_format = path.split(\".\")[-1]\n                local_url_eqiv = [\n                    path_to_url(f\"{dataset}__{catalog}__{dt_str}.{file_format}\", is_raw)\n                ]\n\n        df = pd.DataFrame([file_path_lst, local_url_eqiv]).T\n        df.columns = [\"path\", \"url\"]\n\n        n_par = cpu_count(n_par)\n        parallel = len(df) &gt; 1\n        res = upload_files(\n            fs_fusion,\n            self.fs,\n            df,\n            parallel=parallel,\n            n_par=n_par,\n            multipart=multipart,\n            chunk_size=chunk_size,\n            show_progress=show_progress,\n            from_date=from_date,\n            to_date=to_date,\n        )\n\n        if not all(r[0] for r in res):\n            failed_res = [r for r in res if not r[0]]\n            msg = f\"Not all uploads were successfully completed. The following failed:\\n{failed_res}\"\n            logger.warning(msg)\n            warnings.warn(msg)\n\n        return res if return_paths else None\n\n    def from_bytes(\n        self,\n        data: BytesIO,\n        dataset: str = None,\n        series_member: str = \"latest\",\n        catalog: str = None,\n        distribution: str = \"parquet\",\n        show_progress: bool = True,\n        return_paths: bool = False,\n        chunk_size=5 * 2**20,\n        from_date=None,\n        to_date=None,\n    ):\n\"\"\"Uploads data from an object in memory.\n\n        Args:\n            data (str): an object in memory to upload\n            dataset (str, optional): Dataset name to which the file will be uploaded (for single file only).\n                                    If not provided the dataset will be implied from file's name.\n            series_member (str, optional): A single date or label. Defaults to 'latest' which will return the most recent.\n            catalog (str, optional): A catalog identifier. Defaults to 'common'.\n            distribution (str, optional): A distribution type, e.g. a file format or raw\n            show_progress (bool, optional): Display a progress bar during data download Defaults to True.\n            return_paths (bool, optional): Return paths and success statuses of the downloaded files.\n            chunk_size (int, optional): Maximum chunk size.\n            from_date (str, optional): start of the data date range contained in the distribution, defaults to upload date\n            to_date (str, optional): end of the data date range contained in the distribution, defaults to upload date.\n\n        Returns:\n\n\n        \"\"\"\n        catalog = self.__use_catalog(catalog)\n\n        fs_fusion = self.get_fusion_filesystem()\n\n        is_raw = js.loads(fs_fusion.cat(f\"{catalog}/datasets/{dataset}\"))[\"isRawData\"]\n        local_url_eqiv = path_to_url(\n            f\"{dataset}__{catalog}__{series_member}.{distribution}\", is_raw\n        )\n\n        df = pd.DataFrame([\"\", local_url_eqiv]).T\n        df.columns = [\"path\", \"url\"]\n\n        res = upload_files(\n            fs_fusion,\n            data,\n            df,\n            parallel=False,\n            n_par=1,\n            multipart=False,\n            chunk_size=chunk_size,\n            show_progress=show_progress,\n            from_date=from_date,\n            to_date=to_date,\n        )\n\n        if not all(r[0] for r in res):\n            failed_res = [r for r in res if not r[0]]\n            msg = f\"Not all uploads were successfully completed. The following failed:\\n{failed_res}\"\n            logger.warning(msg)\n            warnings.warn(msg)\n\n        return res if return_paths else None\n\n    def listen_to_events(\n        self,\n        last_event_id: str = None,\n        catalog: str = None,\n        url: str = \"https://fusion.jpmorgan.com/api/v1/\",\n    ) -&gt; Union[None, pd.DataFrame]:\n\"\"\"Run server sent event listener in the background. Retrieve results by running get_events.\n\n        Args:\n            last_event_id (str): Last event ID (exclusive).\n            catalog (str): catalog.\n            url (str): subscription url.\n        Returns:\n            Union[None, class:`pandas.DataFrame`]: If in_background is True then the function returns no output.\n                If in_background is set to False then pandas DataFrame is output upon keyboard termination.\n        \"\"\"\n\n        catalog = self.__use_catalog(catalog)\n        import asyncio\n        import json\n        import threading\n\n        from aiohttp_sse_client import client as sse_client\n\n        from .utils import get_client\n\n        kwargs = {}\n        if last_event_id:\n            kwargs = {\"headers\": {\"Last-Event-ID\": last_event_id}}\n\n        async def async_events():\n\"\"\"Events sync function.\n\n            Returns:\n                None\n            \"\"\"\n            timeout = 1e100\n            session = await get_client(self.credentials, timeout=timeout)\n            async with sse_client.EventSource(\n                f\"{url}catalogs/{catalog}/notifications/subscribe\",\n                session=session,\n                **kwargs,\n            ) as messages:\n                try:\n                    async for msg in messages:\n                        event = json.loads(msg.data)\n                        if self.events is None:\n                            self.events = pd.DataFrame([event])\n                        else:\n                            self.events = pd.concat(\n                                [self.events, pd.DataFrame(event)], ignore_index=True\n                            )\n                except TimeoutError as ex:\n                    raise ex\n                except Exception as e:\n                    raise Exception(e)\n\n        _ = self.list_catalogs()  # refresh token\n        if \"headers\" in kwargs:\n            kwargs[\"headers\"].update(\n                {\"authorization\": f\"bearer {self.credentials.bearer_token}\"}\n            )\n        else:\n            kwargs[\"headers\"] = {\n                \"authorization\": f\"bearer {self.credentials.bearer_token}\",\n            }\n        if \"http\" in self.credentials.proxies.keys():\n            kwargs[\"proxy\"] = self.credentials.proxies[\"http\"]\n        elif \"https\" in self.credentials.proxies.keys():\n            kwargs[\"proxy\"] = self.credentials.proxies[\"https\"]\n        th = threading.Thread(target=asyncio.run, args=(async_events(),), daemon=True)\n        th.start()\n        return None\n\n    def get_events(\n        self,\n        last_event_id: str = None,\n        catalog: str = None,\n        in_background: bool = True,\n        url: str = \"https://fusion.jpmorgan.com/api/v1/\",\n    ) -&gt; Union[None, pd.DataFrame]:\n\"\"\"Run server sent event listener and print out the new events. Keyboard terminate to stop.\n\n        Args:\n            last_event_id (str): id of the last event.\n            catalog (str): catalog.\n            in_background (bool): execute event monitoring in the background (default = True).\n            url (str): subscription url.\n        Returns:\n            Union[None, class:`pandas.DataFrame`]: If in_background is True then the function returns no output.\n                If in_background is set to False then pandas DataFrame is output upon keyboard termination.\n        \"\"\"\n\n        catalog = self.__use_catalog(catalog)\n        if not in_background:\n            from sseclient import SSEClient\n\n            _ = self.list_catalogs()  # refresh token\n            messages = SSEClient(\n                session=self.session,\n                url=f\"{url}catalogs/{catalog}/notifications/subscribe\",\n                last_id=last_event_id,\n                headers={\n                    \"authorization\": f\"bearer {self.credentials.bearer_token}\",\n                },\n            )\n            lst = []\n            try:\n                for msg in messages:\n                    event = js.loads(msg.data)\n                    print(event)\n                    if event[\"type\"] != \"HeartBeatNotification\":\n                        lst.append(event)\n            except KeyboardInterrupt:\n                return pd.DataFrame(lst)\n            except Exception as e:\n                raise Exception(e)\n            finally:\n                return None\n        else:\n            return self.events\n</code></pre>"},{"location":"api/#fusion.fusion.Fusion.default_catalog","title":"<code>default_catalog: str</code>  <code>property</code> <code>writable</code>","text":"<p>Returns the default catalog.</p> <p>Returns:</p> Type Description <code>str</code> <p>None</p>"},{"location":"api/#fusion.fusion.Fusion.__init__","title":"<code>__init__(credentials='config/client_credentials.json', root_url='https://fusion-api.jpmorgan.com/fusion/v1/', download_folder='downloads', log_level=logging.ERROR, fs=None, log_path='.')</code>","text":"<p>Constructor to instantiate a new Fusion object.</p> <p>Parameters:</p> Name Type Description Default <code>credentials</code> <code>Union[str, dict]</code> <p>A path to a credentials file or a dictionary containing the required keys. Defaults to 'config/client_credentials.json'.</p> <code>'config/client_credentials.json'</code> <code>root_url</code> <code>_type_</code> <p>The API root URL. Defaults to \"https://fusion-api.jpmorgan.com/fusion/v1/\".</p> <code>'https://fusion-api.jpmorgan.com/fusion/v1/'</code> <code>download_folder</code> <code>str</code> <p>The folder path where downloaded data files are saved. Defaults to \"downloads\".</p> <code>'downloads'</code> <code>log_level</code> <code>int</code> <p>Set the logging level. Defaults to logging.ERROR.</p> <code>logging.ERROR</code> <code>fs</code> <code>fsspec.filesystem</code> <p>filesystem.</p> <code>None</code> <code>log_path</code> <code>str</code> <p>The folder path where the log is stored.</p> <code>'.'</code> Source code in <code>fusion/fusion.py</code> <pre><code>def __init__(\n    self,\n    credentials: Union[str, dict] = \"config/client_credentials.json\",\n    root_url: str = \"https://fusion-api.jpmorgan.com/fusion/v1/\",\n    download_folder: str = \"downloads\",\n    log_level: int = logging.ERROR,\n    fs=None,\n    log_path: str = \".\",\n) -&gt; None:\n\"\"\"Constructor to instantiate a new Fusion object.\n\n    Args:\n        credentials (Union[str, dict], optional): A path to a credentials file or\n            a dictionary containing the required keys.\n            Defaults to 'config/client_credentials.json'.\n        root_url (_type_, optional): The API root URL.\n            Defaults to \"https://fusion-api.jpmorgan.com/fusion/v1/\".\n        download_folder (str, optional): The folder path where downloaded data files\n            are saved. Defaults to \"downloads\".\n        log_level (int, optional): Set the logging level. Defaults to logging.ERROR.\n        fs (fsspec.filesystem): filesystem.\n        log_path (str, optional): The folder path where the log is stored.\n    \"\"\"\n    self._default_catalog = \"common\"\n\n    self.root_url = root_url\n    self.download_folder = download_folder\n    Path(download_folder).mkdir(parents=True, exist_ok=True)\n\n    if logger.hasHandlers():\n        logger.handlers.clear()\n    file_handler = logging.FileHandler(filename=f\"{log_path}/fusion_sdk.log\")\n    logging.addLevelName(VERBOSE_LVL, \"VERBOSE\")\n    stdout_handler = logging.StreamHandler(sys.stdout)\n    formatter = logging.Formatter(\n        \"%(asctime)s.%(msecs)03d %(name)s:%(levelname)s %(message)s\",\n        datefmt=\"%Y-%m-%d %H:%M:%S\",\n    )\n    stdout_handler.setFormatter(formatter)\n    logger.addHandler(stdout_handler)\n    logger.addHandler(file_handler)\n    logger.setLevel(log_level)\n\n    if isinstance(credentials, FusionCredentials):\n        self.credentials = credentials\n    else:\n        self.credentials = FusionCredentials.from_object(credentials)\n\n    self.session = get_session(self.credentials, self.root_url)\n    self.fs = fs if fs else get_default_fs()\n    self.events = None\n</code></pre>"},{"location":"api/#fusion.fusion.Fusion.__repr__","title":"<code>__repr__()</code>","text":"<p>Object representation to list all available methods.</p> Source code in <code>fusion/fusion.py</code> <pre><code>def __repr__(self):\n\"\"\"Object representation to list all available methods.\"\"\"\n    return \"Fusion object \\nAvailable methods:\\n\" + tabulate(\n        pd.DataFrame(\n            [\n                [\n                    method_name\n                    for method_name in dir(Fusion)\n                    if callable(getattr(Fusion, method_name))\n                    and not method_name.startswith(\"_\")\n                ]\n                + [\n                    p\n                    for p in dir(Fusion)\n                    if isinstance(getattr(Fusion, p), property)\n                ],\n                [\n                    getattr(Fusion, method_name).__doc__.split(\"\\n\")[0]\n                    for method_name in dir(Fusion)\n                    if callable(getattr(Fusion, method_name))\n                    and not method_name.startswith(\"_\")\n                ]\n                + [\n                    getattr(Fusion, p).__doc__.split(\"\\n\")[0]\n                    for p in dir(Fusion)\n                    if isinstance(getattr(Fusion, p), property)\n                ],\n            ]\n        ).T.set_index(0),\n        tablefmt=\"psql\",\n    )\n</code></pre>"},{"location":"api/#fusion.fusion.Fusion.__use_catalog","title":"<code>__use_catalog(catalog)</code>","text":"<p>Determine which catalog to use in an API call.</p> <p>Parameters:</p> Name Type Description Default <code>catalog</code> <code>str</code> <p>The catalog value passed as an argument to an API function wrapper.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The catalog to use</p> Source code in <code>fusion/fusion.py</code> <pre><code>def __use_catalog(self, catalog):\n\"\"\"Determine which catalog to use in an API call.\n\n    Args:\n        catalog (str): The catalog value passed as an argument to an API function wrapper.\n\n    Returns:\n        str: The catalog to use\n    \"\"\"\n    if catalog is None:\n        return self.default_catalog\n\n    return catalog\n</code></pre>"},{"location":"api/#fusion.fusion.Fusion.catalog_resources","title":"<code>catalog_resources(catalog=None, output=False)</code>","text":"<p>List the resources contained within the catalog, for example products and datasets.</p> <p>Parameters:</p> Name Type Description Default <code>catalog</code> <code>str</code> <p>A catalog identifier. Defaults to 'common'.</p> <code>None</code> <code>output</code> <code>bool</code> <p>If True then print the dataframe. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>class</code> <code>pd.DataFrame</code> <p><code>pandas.DataFrame</code>: A dataframe with a row for each resource within the catalog</p> Source code in <code>fusion/fusion.py</code> <pre><code>def catalog_resources(\n    self, catalog: str = None, output: bool = False\n) -&gt; pd.DataFrame:\n\"\"\"List the resources contained within the catalog, for example products and datasets.\n\n    Args:\n        catalog (str, optional): A catalog identifier. Defaults to 'common'.\n        output (bool, optional): If True then print the dataframe. Defaults to False.\n\n    Returns:\n       class:`pandas.DataFrame`: A dataframe with a row for each resource within the catalog\n    \"\"\"\n    catalog = self.__use_catalog(catalog)\n\n    url = f\"{self.root_url}catalogs/{catalog}\"\n    df = Fusion._call_for_dataframe(url, self.session)\n\n    if output:\n        print(tabulate(df, headers=\"keys\", tablefmt=\"psql\", maxcolwidths=30))\n\n    return df\n</code></pre>"},{"location":"api/#fusion.fusion.Fusion.dataset_resources","title":"<code>dataset_resources(dataset, catalog=None, output=False)</code>","text":"<p>List the resources available for a dataset, currently this will always be a datasetseries.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>str</code> <p>A dataset identifier</p> required <code>catalog</code> <code>str</code> <p>A catalog identifier. Defaults to 'common'.</p> <code>None</code> <code>output</code> <code>bool</code> <p>If True then print the dataframe. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>class</code> <code>pd.DataFrame</code> <p><code>pandas.DataFrame</code>: A dataframe with a row for each resource</p> Source code in <code>fusion/fusion.py</code> <pre><code>def dataset_resources(\n    self, dataset: str, catalog: str = None, output: bool = False\n) -&gt; pd.DataFrame:\n\"\"\"List the resources available for a dataset, currently this will always be a datasetseries.\n\n    Args:\n        dataset (str): A dataset identifier\n        catalog (str, optional): A catalog identifier. Defaults to 'common'.\n        output (bool, optional): If True then print the dataframe. Defaults to False.\n\n    Returns:\n        class:`pandas.DataFrame`: A dataframe with a row for each resource\n    \"\"\"\n    catalog = self.__use_catalog(catalog)\n\n    url = f\"{self.root_url}catalogs/{catalog}/datasets/{dataset}\"\n    df = Fusion._call_for_dataframe(url, self.session)\n\n    if output:\n        print(tabulate(df, headers=\"keys\", tablefmt=\"psql\"))\n\n    return df\n</code></pre>"},{"location":"api/#fusion.fusion.Fusion.datasetmember_resources","title":"<code>datasetmember_resources(dataset, series, catalog=None, output=False)</code>","text":"<p>List the available resources for a datasetseries member.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>str</code> <p>A dataset identifier</p> required <code>series</code> <code>str</code> <p>The datasetseries identifier</p> required <code>catalog</code> <code>str</code> <p>A catalog identifier. Defaults to 'common'.</p> <code>None</code> <code>output</code> <code>bool</code> <p>If True then print the dataframe. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>class</code> <code>pd.DataFrame</code> <p><code>pandas.DataFrame</code>: A dataframe with a row for each datasetseries member resource. Currently, this will always be distributions.</p> Source code in <code>fusion/fusion.py</code> <pre><code>def datasetmember_resources(\n    self, dataset: str, series: str, catalog: str = None, output: bool = False\n) -&gt; pd.DataFrame:\n\"\"\"List the available resources for a datasetseries member.\n\n    Args:\n        dataset (str): A dataset identifier\n        series (str): The datasetseries identifier\n        catalog (str, optional): A catalog identifier. Defaults to 'common'.\n        output (bool, optional): If True then print the dataframe. Defaults to False.\n\n    Returns:\n        class:`pandas.DataFrame`: A dataframe with a row for each datasetseries member resource.\n            Currently, this will always be distributions.\n    \"\"\"\n    catalog = self.__use_catalog(catalog)\n\n    url = f\"{self.root_url}catalogs/{catalog}/datasets/{dataset}/datasetseries/{series}\"\n    df = Fusion._call_for_dataframe(url, self.session)\n\n    if output:\n        print(tabulate(df, headers=\"keys\", tablefmt=\"psql\"))\n\n    return df\n</code></pre>"},{"location":"api/#fusion.fusion.Fusion.download","title":"<code>download(dataset, dt_str='latest', dataset_format='parquet', catalog=None, n_par=None, show_progress=True, force_download=False, download_folder=None, return_paths=False, partitioning=None)</code>","text":"<p>Downloads the requested distributions of a dataset to disk.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>str</code> <p>A dataset identifier</p> required <code>dt_str</code> <code>str</code> <p>Either a single date or a range identified by a start or end date, or both separated with a \":\". Defaults to 'latest' which will return the most recent instance of the dataset.</p> <code>'latest'</code> <code>dataset_format</code> <code>str</code> <p>The file format, e.g. CSV or Parquet. Defaults to 'parquet'.</p> <code>'parquet'</code> <code>catalog</code> <code>str</code> <p>A catalog identifier. Defaults to 'common'.</p> <code>None</code> <code>n_par</code> <code>int</code> <p>Specify how many distributions to download in parallel. Defaults to all cpus available.</p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>Display a progress bar during data download Defaults to True.</p> <code>True</code> <code>force_download</code> <code>bool</code> <p>If True then will always download a file even if it is already on disk. Defaults to True.</p> <code>False</code> <code>download_folder</code> <code>str</code> <p>The path, absolute or relative, where downloaded files are saved. Defaults to download_folder as set in init</p> <code>None</code> <code>return_paths</code> <code>bool</code> <p>Return paths and success statuses of the downloaded files.</p> <code>False</code> <code>partitioning</code> <code>str</code> <p>Partitioning specification.</p> <code>None</code> Source code in <code>fusion/fusion.py</code> <pre><code>def download(\n    self,\n    dataset: str,\n    dt_str: str = \"latest\",\n    dataset_format: str = \"parquet\",\n    catalog: str = None,\n    n_par: int = None,\n    show_progress: bool = True,\n    force_download: bool = False,\n    download_folder: str = None,\n    return_paths: bool = False,\n    partitioning: str = None,\n):\n\"\"\"Downloads the requested distributions of a dataset to disk.\n\n    Args:\n        dataset (str): A dataset identifier\n        dt_str (str, optional): Either a single date or a range identified by a start or end date,\n            or both separated with a \":\". Defaults to 'latest' which will return the most recent\n            instance of the dataset.\n        dataset_format (str, optional): The file format, e.g. CSV or Parquet. Defaults to 'parquet'.\n        catalog (str, optional): A catalog identifier. Defaults to 'common'.\n        n_par (int, optional): Specify how many distributions to download in parallel.\n            Defaults to all cpus available.\n        show_progress (bool, optional): Display a progress bar during data download Defaults to True.\n        force_download (bool, optional): If True then will always download a file even\n            if it is already on disk. Defaults to True.\n        download_folder (str, optional): The path, absolute or relative, where downloaded files are saved.\n            Defaults to download_folder as set in __init__\n        return_paths (bool, optional): Return paths and success statuses of the downloaded files.\n        partitioning (str, optional): Partitioning specification.\n\n    Returns:\n\n    \"\"\"\n    catalog = self.__use_catalog(catalog)\n\n    valid_date_range = re.compile(\n        r\"^(\\d{4}\\d{2}\\d{2})$|^((\\d{4}\\d{2}\\d{2})?([:])(\\d{4}\\d{2}\\d{2})?)$\"\n    )\n\n    if valid_date_range.match(dt_str) or dt_str == \"latest\":\n        required_series = self._resolve_distro_tuples(\n            dataset, dt_str, dataset_format, catalog\n        )\n    else:\n        # sample data is limited to csv\n        if dt_str == \"sample\":\n            dataset_format = \"csv\"\n        required_series = [(catalog, dataset, dt_str, dataset_format)]\n\n    if not download_folder:\n        download_folder = self.download_folder\n\n    download_folders = [download_folder] * len(required_series)\n\n    if partitioning == \"hive\":\n        members = [series[2].strip(\"/\") for series in required_series]\n        download_folders = [\n            f\"{download_folders[i]}/{series[0]}/{series[1]}/{members[i]}\"\n            for i, series in enumerate(required_series)\n        ]\n\n    for d in download_folders:\n        if not self.fs.exists(d):\n            self.fs.mkdir(d, create_parents=True)\n\n    if len(required_series) == 1:\n        n_par = cpu_count(n_par, is_threading=True)\n        with tqdm(total=1) as pbar:\n            output_file = distribution_to_filename(\n                download_folders[0],\n                required_series[0][1],\n                required_series[0][2],\n                required_series[0][3],\n                required_series[0][0],\n                partitioning=partitioning,\n            )\n            res = download_single_file_threading(\n                self.credentials,\n                distribution_to_url(\n                    self.root_url,\n                    required_series[0][1],\n                    required_series[0][2],\n                    required_series[0][3],\n                    required_series[0][0],\n                    is_download=True,\n                ),\n                output_file,\n                fs=self.fs,\n                max_threads=n_par,\n            )\n            if (len(res) &gt; 0) and all((r[0] for r in res)):\n                pbar.update(1)\n                res = [(res[0][0], output_file, res[0][2])]\n\n    else:\n        n_par = cpu_count(n_par)\n        download_spec = [\n            {\n                \"credentials\": self.credentials,\n                \"url\": distribution_to_url(\n                    self.root_url,\n                    series[1],\n                    series[2],\n                    series[3],\n                    series[0],\n                    is_download=True,\n                ),\n                \"output_file\": distribution_to_filename(\n                    download_folders[i],\n                    series[1],\n                    series[2],\n                    series[3],\n                    series[0],\n                    partitioning=partitioning,\n                ),\n                \"overwrite\": force_download,\n                \"fs\": self.fs,\n            }\n            for i, series in enumerate(required_series)\n        ]\n\n        logger.log(\n            VERBOSE_LVL,\n            f\"Beginning {len(download_spec)} downloads in batches of {n_par}\",\n        )\n        if show_progress:\n            with tqdm_joblib(tqdm(total=len(download_spec))) as _:\n                res = Parallel(n_jobs=n_par)(\n                    delayed(stream_single_file_new_session)(**spec)\n                    for spec in download_spec\n                )\n        else:\n            res = Parallel(n_jobs=n_par)(\n                delayed(stream_single_file_new_session)(**spec)\n                for spec in download_spec\n            )\n\n    if (len(res) &gt; 0) and (not all((r[0] for r in res))):\n        for r in res:\n            if not r[0]:\n                warnings.warn(f\"The download of {r[1]} was not successful\")\n    return res if return_paths else None\n</code></pre>"},{"location":"api/#fusion.fusion.Fusion.from_bytes","title":"<code>from_bytes(data, dataset=None, series_member='latest', catalog=None, distribution='parquet', show_progress=True, return_paths=False, chunk_size=5 * 2 ** 20, from_date=None, to_date=None)</code>","text":"<p>Uploads data from an object in memory.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>an object in memory to upload</p> required <code>dataset</code> <code>str</code> <p>Dataset name to which the file will be uploaded (for single file only).                     If not provided the dataset will be implied from file's name.</p> <code>None</code> <code>series_member</code> <code>str</code> <p>A single date or label. Defaults to 'latest' which will return the most recent.</p> <code>'latest'</code> <code>catalog</code> <code>str</code> <p>A catalog identifier. Defaults to 'common'.</p> <code>None</code> <code>distribution</code> <code>str</code> <p>A distribution type, e.g. a file format or raw</p> <code>'parquet'</code> <code>show_progress</code> <code>bool</code> <p>Display a progress bar during data download Defaults to True.</p> <code>True</code> <code>return_paths</code> <code>bool</code> <p>Return paths and success statuses of the downloaded files.</p> <code>False</code> <code>chunk_size</code> <code>int</code> <p>Maximum chunk size.</p> <code>5 * 2 ** 20</code> <code>from_date</code> <code>str</code> <p>start of the data date range contained in the distribution, defaults to upload date</p> <code>None</code> <code>to_date</code> <code>str</code> <p>end of the data date range contained in the distribution, defaults to upload date.</p> <code>None</code> Source code in <code>fusion/fusion.py</code> <pre><code>def from_bytes(\n    self,\n    data: BytesIO,\n    dataset: str = None,\n    series_member: str = \"latest\",\n    catalog: str = None,\n    distribution: str = \"parquet\",\n    show_progress: bool = True,\n    return_paths: bool = False,\n    chunk_size=5 * 2**20,\n    from_date=None,\n    to_date=None,\n):\n\"\"\"Uploads data from an object in memory.\n\n    Args:\n        data (str): an object in memory to upload\n        dataset (str, optional): Dataset name to which the file will be uploaded (for single file only).\n                                If not provided the dataset will be implied from file's name.\n        series_member (str, optional): A single date or label. Defaults to 'latest' which will return the most recent.\n        catalog (str, optional): A catalog identifier. Defaults to 'common'.\n        distribution (str, optional): A distribution type, e.g. a file format or raw\n        show_progress (bool, optional): Display a progress bar during data download Defaults to True.\n        return_paths (bool, optional): Return paths and success statuses of the downloaded files.\n        chunk_size (int, optional): Maximum chunk size.\n        from_date (str, optional): start of the data date range contained in the distribution, defaults to upload date\n        to_date (str, optional): end of the data date range contained in the distribution, defaults to upload date.\n\n    Returns:\n\n\n    \"\"\"\n    catalog = self.__use_catalog(catalog)\n\n    fs_fusion = self.get_fusion_filesystem()\n\n    is_raw = js.loads(fs_fusion.cat(f\"{catalog}/datasets/{dataset}\"))[\"isRawData\"]\n    local_url_eqiv = path_to_url(\n        f\"{dataset}__{catalog}__{series_member}.{distribution}\", is_raw\n    )\n\n    df = pd.DataFrame([\"\", local_url_eqiv]).T\n    df.columns = [\"path\", \"url\"]\n\n    res = upload_files(\n        fs_fusion,\n        data,\n        df,\n        parallel=False,\n        n_par=1,\n        multipart=False,\n        chunk_size=chunk_size,\n        show_progress=show_progress,\n        from_date=from_date,\n        to_date=to_date,\n    )\n\n    if not all(r[0] for r in res):\n        failed_res = [r for r in res if not r[0]]\n        msg = f\"Not all uploads were successfully completed. The following failed:\\n{failed_res}\"\n        logger.warning(msg)\n        warnings.warn(msg)\n\n    return res if return_paths else None\n</code></pre>"},{"location":"api/#fusion.fusion.Fusion.get_events","title":"<code>get_events(last_event_id=None, catalog=None, in_background=True, url='https://fusion.jpmorgan.com/api/v1/')</code>","text":"<p>Run server sent event listener and print out the new events. Keyboard terminate to stop.</p> <p>Parameters:</p> Name Type Description Default <code>last_event_id</code> <code>str</code> <p>id of the last event.</p> <code>None</code> <code>catalog</code> <code>str</code> <p>catalog.</p> <code>None</code> <code>in_background</code> <code>bool</code> <p>execute event monitoring in the background (default = True).</p> <code>True</code> <code>url</code> <code>str</code> <p>subscription url.</p> <code>'https://fusion.jpmorgan.com/api/v1/'</code> <p>Returns:</p> Type Description <code>Union[None, pd.DataFrame]</code> <p>Union[None, class:<code>pandas.DataFrame</code>]: If in_background is True then the function returns no output. If in_background is set to False then pandas DataFrame is output upon keyboard termination.</p> Source code in <code>fusion/fusion.py</code> <pre><code>def get_events(\n    self,\n    last_event_id: str = None,\n    catalog: str = None,\n    in_background: bool = True,\n    url: str = \"https://fusion.jpmorgan.com/api/v1/\",\n) -&gt; Union[None, pd.DataFrame]:\n\"\"\"Run server sent event listener and print out the new events. Keyboard terminate to stop.\n\n    Args:\n        last_event_id (str): id of the last event.\n        catalog (str): catalog.\n        in_background (bool): execute event monitoring in the background (default = True).\n        url (str): subscription url.\n    Returns:\n        Union[None, class:`pandas.DataFrame`]: If in_background is True then the function returns no output.\n            If in_background is set to False then pandas DataFrame is output upon keyboard termination.\n    \"\"\"\n\n    catalog = self.__use_catalog(catalog)\n    if not in_background:\n        from sseclient import SSEClient\n\n        _ = self.list_catalogs()  # refresh token\n        messages = SSEClient(\n            session=self.session,\n            url=f\"{url}catalogs/{catalog}/notifications/subscribe\",\n            last_id=last_event_id,\n            headers={\n                \"authorization\": f\"bearer {self.credentials.bearer_token}\",\n            },\n        )\n        lst = []\n        try:\n            for msg in messages:\n                event = js.loads(msg.data)\n                print(event)\n                if event[\"type\"] != \"HeartBeatNotification\":\n                    lst.append(event)\n        except KeyboardInterrupt:\n            return pd.DataFrame(lst)\n        except Exception as e:\n            raise Exception(e)\n        finally:\n            return None\n    else:\n        return self.events\n</code></pre>"},{"location":"api/#fusion.fusion.Fusion.get_fusion_filesystem","title":"<code>get_fusion_filesystem()</code>","text":"<p>Creates Fusion Filesystem.</p> Source code in <code>fusion/fusion.py</code> <pre><code>def get_fusion_filesystem(self):\n\"\"\"Creates Fusion Filesystem.\n\n    Returns: Fusion Filesystem\n\n    \"\"\"\n    return FusionHTTPFileSystem(\n        client_kwargs={\"root_url\": self.root_url, \"credentials\": self.credentials}\n    )\n</code></pre>"},{"location":"api/#fusion.fusion.Fusion.list_catalogs","title":"<code>list_catalogs(output=False)</code>","text":"<p>Lists the catalogs available to the API account.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>bool</code> <p>If True then print the dataframe. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>class</code> <code>pd.DataFrame</code> <p><code>pandas.DataFrame</code>: A dataframe with a row for each catalog</p> Source code in <code>fusion/fusion.py</code> <pre><code>def list_catalogs(self, output: bool = False) -&gt; pd.DataFrame:\n\"\"\"Lists the catalogs available to the API account.\n\n    Args:\n        output (bool, optional): If True then print the dataframe. Defaults to False.\n\n    Returns:\n        class:`pandas.DataFrame`: A dataframe with a row for each catalog\n    \"\"\"\n    url = f\"{self.root_url}catalogs/\"\n    df = Fusion._call_for_dataframe(url, self.session)\n\n    if output:\n        print(tabulate(df, headers=\"keys\", tablefmt=\"psql\"))\n\n    return df\n</code></pre>"},{"location":"api/#fusion.fusion.Fusion.list_dataset_attributes","title":"<code>list_dataset_attributes(dataset, catalog=None, output=False, display_all_columns=False)</code>","text":"<p>Returns the list of attributes that are in the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>str</code> <p>A dataset identifier</p> required <code>catalog</code> <code>str</code> <p>A catalog identifier. Defaults to 'common'.</p> <code>None</code> <code>output</code> <code>bool</code> <p>If True then print the dataframe. Defaults to False.</p> <code>False</code> <code>display_all_columns</code> <code>bool</code> <p>If True displays all columns returned by the API, otherwise only the key columns are displayed</p> <code>False</code> <p>Returns:</p> Name Type Description <code>class</code> <code>pd.DataFrame</code> <p><code>pandas.DataFrame</code>: A dataframe with a row for each attribute</p> Source code in <code>fusion/fusion.py</code> <pre><code>def list_dataset_attributes(\n    self,\n    dataset: str,\n    catalog: str = None,\n    output: bool = False,\n    display_all_columns: bool = False,\n) -&gt; pd.DataFrame:\n\"\"\"Returns the list of attributes that are in the dataset.\n\n    Args:\n        dataset (str): A dataset identifier\n        catalog (str, optional): A catalog identifier. Defaults to 'common'.\n        output (bool, optional): If True then print the dataframe. Defaults to False.\n        display_all_columns (bool, optional): If True displays all columns returned by the API,\n            otherwise only the key columns are displayed\n\n    Returns:\n        class:`pandas.DataFrame`: A dataframe with a row for each attribute\n    \"\"\"\n    catalog = self.__use_catalog(catalog)\n\n    url = f\"{self.root_url}catalogs/{catalog}/datasets/{dataset}/attributes\"\n    df = (\n        Fusion._call_for_dataframe(url, self.session)\n        .sort_values(by=\"index\")\n        .reset_index(drop=True)\n    )\n\n    if not display_all_columns:\n        df = df[\n            df.columns.intersection(\n                [\n                    \"identifier\",\n                    \"title\",\n                    \"dataType\",\n                    \"isDatasetKey\",\n                    \"description\",\n                    \"source\",\n                ]\n            )\n        ]\n\n    if output:\n        print(tabulate(df, headers=\"keys\", tablefmt=\"psql\", maxcolwidths=30))\n\n    return df\n</code></pre>"},{"location":"api/#fusion.fusion.Fusion.list_datasetmembers","title":"<code>list_datasetmembers(dataset, catalog=None, output=False, max_results=-1)</code>","text":"<p>List the available members in the dataset series.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>str</code> <p>A dataset identifier</p> required <code>catalog</code> <code>str</code> <p>A catalog identifier. Defaults to 'common'.</p> <code>None</code> <code>output</code> <code>bool</code> <p>If True then print the dataframe. Defaults to False.</p> <code>False</code> <code>max_results</code> <code>int</code> <p>Limit the number of rows returned in the dataframe. Defaults to -1 which returns all results.</p> <code>-1</code> <p>Returns:</p> Name Type Description <code>class</code> <code>pd.DataFrame</code> <p><code>pandas.DataFrame</code>: a dataframe with a row for each dataset member.</p> Source code in <code>fusion/fusion.py</code> <pre><code>def list_datasetmembers(\n    self,\n    dataset: str,\n    catalog: str = None,\n    output: bool = False,\n    max_results: int = -1,\n) -&gt; pd.DataFrame:\n\"\"\"List the available members in the dataset series.\n\n    Args:\n        dataset (str): A dataset identifier\n        catalog (str, optional): A catalog identifier. Defaults to 'common'.\n        output (bool, optional): If True then print the dataframe. Defaults to False.\n        max_results (int, optional): Limit the number of rows returned in the dataframe.\n            Defaults to -1 which returns all results.\n\n    Returns:\n        class:`pandas.DataFrame`: a dataframe with a row for each dataset member.\n    \"\"\"\n    catalog = self.__use_catalog(catalog)\n\n    url = f\"{self.root_url}catalogs/{catalog}/datasets/{dataset}/datasetseries\"\n    df = Fusion._call_for_dataframe(url, self.session)\n\n    if max_results &gt; -1:\n        df = df[0:max_results]\n\n    if output:\n        print(tabulate(df, headers=\"keys\", tablefmt=\"psql\"))\n\n    return df\n</code></pre>"},{"location":"api/#fusion.fusion.Fusion.list_datasets","title":"<code>list_datasets(contains=None, id_contains=False, product=None, catalog=None, output=False, max_results=-1, display_all_columns=False, status=None)</code>","text":"<p>Get the datasets contained in a catalog.</p> <p>Parameters:</p> Name Type Description Default <code>contains</code> <code>Union[str, list]</code> <p>A string or a list of strings that are dataset identifiers to filter the datasets list. If a list is provided then it will return datasets whose identifier matches any of the strings. Defaults to None.</p> <code>None</code> <code>id_contains</code> <code>bool</code> <p>Filter datasets only where the string(s) are contained in the identifier, ignoring description.</p> <code>False</code> <code>product</code> <code>Union[str, list]</code> <p>A string or a list of strings that are product identifiers to filter the datasets list. Defaults to None.</p> <code>None</code> <code>catalog</code> <code>str</code> <p>A catalog identifier. Defaults to 'common'.</p> <code>None</code> <code>output</code> <code>bool</code> <p>If True then print the dataframe. Defaults to False.</p> <code>False</code> <code>max_results</code> <code>int</code> <p>Limit the number of rows returned in the dataframe. Defaults to -1 which returns all results.</p> <code>-1</code> <code>display_all_columns</code> <code>bool</code> <p>If True displays all columns returned by the API, otherwise only the key columns are displayed</p> <code>False</code> <code>status</code> <code>str</code> <p>filter the datasets by status, default is to show all results.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>class</code> <code>pd.DataFrame</code> <p><code>pandas.DataFrame</code>: a dataframe with a row for each dataset.</p> Source code in <code>fusion/fusion.py</code> <pre><code>def list_datasets(\n    self,\n    contains: Union[str, list] = None,\n    id_contains: bool = False,\n    product: Union[str, list] = None,\n    catalog: str = None,\n    output: bool = False,\n    max_results: int = -1,\n    display_all_columns: bool = False,\n    status: str = None,\n) -&gt; pd.DataFrame:\n\"\"\"Get the datasets contained in a catalog.\n\n    Args:\n        contains (Union[str, list], optional): A string or a list of strings that are dataset\n            identifiers to filter the datasets list. If a list is provided then it will return\n            datasets whose identifier matches any of the strings. Defaults to None.\n        id_contains (bool): Filter datasets only where the string(s) are contained in the identifier,\n            ignoring description.\n        product (Union[str, list], optional): A string or a list of strings that are product\n            identifiers to filter the datasets list. Defaults to None.\n        catalog (str, optional): A catalog identifier. Defaults to 'common'.\n        output (bool, optional): If True then print the dataframe. Defaults to False.\n        max_results (int, optional): Limit the number of rows returned in the dataframe.\n            Defaults to -1 which returns all results.\n        display_all_columns (bool, optional): If True displays all columns returned by the API,\n            otherwise only the key columns are displayed\n        status (str, optional): filter the datasets by status, default is to show all results.\n\n    Returns:\n        class:`pandas.DataFrame`: a dataframe with a row for each dataset.\n    \"\"\"\n    catalog = self.__use_catalog(catalog)\n\n    url = f\"{self.root_url}catalogs/{catalog}/datasets\"\n    df = Fusion._call_for_dataframe(url, self.session)\n\n    if contains:\n        if isinstance(contains, list):\n            contains = \"|\".join(f\"{s}\" for s in contains)\n        if id_contains:\n            df = df[df[\"identifier\"].str.contains(contains, case=False)]\n        else:\n            df = df[\n                df[\"identifier\"].str.contains(contains, case=False)\n                | df[\"description\"].str.contains(contains, case=False)\n            ]\n\n    if product:\n        url = f\"{self.root_url}catalogs/{catalog}/productDatasets\"\n        df_pr = Fusion._call_for_dataframe(url, self.session)\n        df_pr = (\n            df_pr[df_pr[\"product\"] == product]\n            if isinstance(product, str)\n            else df_pr[df_pr[\"product\"].isin(product)]\n        )\n        df = df[df[\"identifier\"].isin(df_pr[\"dataset\"])].reset_index(drop=True)\n\n    if max_results &gt; -1:\n        df = df[0:max_results]\n\n    df[\"category\"] = df.category.str.join(\", \")\n    df[\"region\"] = df.region.str.join(\", \")\n    if not display_all_columns:\n        cols = [\n            \"identifier\",\n            \"title\",\n            \"containerType\",\n            \"region\",\n            \"category\",\n            \"coverageStartDate\",\n            \"coverageEndDate\",\n            \"description\",\n            \"status\",\n        ]\n        cols = [c for c in cols if c in df.columns]\n        df = df[cols]\n\n    if status is not None:\n        df = df[df[\"status\"] == status]\n\n    if output:\n        print(tabulate(df, headers=\"keys\", tablefmt=\"psql\", maxcolwidths=30))\n\n    return df\n</code></pre>"},{"location":"api/#fusion.fusion.Fusion.list_distributions","title":"<code>list_distributions(dataset, series, catalog=None, output=False)</code>","text":"<p>List the available distributions (downloadable instances of the dataset with a format type).</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>str</code> <p>A dataset identifier</p> required <code>series</code> <code>str</code> <p>The datasetseries identifier</p> required <code>catalog</code> <code>str</code> <p>A catalog identifier. Defaults to 'common'.</p> <code>None</code> <code>output</code> <code>bool</code> <p>If True then print the dataframe. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>class</code> <code>pd.DataFrame</code> <p><code>pandas.DataFrame</code>: A dataframe with a row for each distribution.</p> Source code in <code>fusion/fusion.py</code> <pre><code>def list_distributions(\n    self, dataset: str, series: str, catalog: str = None, output: bool = False\n) -&gt; pd.DataFrame:\n\"\"\"List the available distributions (downloadable instances of the dataset with a format type).\n\n    Args:\n        dataset (str): A dataset identifier\n        series (str): The datasetseries identifier\n        catalog (str, optional): A catalog identifier. Defaults to 'common'.\n        output (bool, optional): If True then print the dataframe. Defaults to False.\n\n    Returns:\n        class:`pandas.DataFrame`: A dataframe with a row for each distribution.\n    \"\"\"\n    catalog = self.__use_catalog(catalog)\n\n    url = f\"{self.root_url}catalogs/{catalog}/datasets/{dataset}/datasetseries/{series}/distributions\"\n    df = Fusion._call_for_dataframe(url, self.session)\n\n    if output:\n        print(tabulate(df, headers=\"keys\", tablefmt=\"psql\"))\n\n    return df\n</code></pre>"},{"location":"api/#fusion.fusion.Fusion.list_products","title":"<code>list_products(contains=None, id_contains=False, catalog=None, output=False, max_results=-1, display_all_columns=False)</code>","text":"<p>Get the products contained in a catalog. A product is a grouping of datasets.</p> <p>Parameters:</p> Name Type Description Default <code>contains</code> <code>Union[str, list]</code> <p>A string or a list of strings that are product identifiers to filter the products list. If a list is provided then it will return products whose identifier matches any of the strings. Defaults to None.</p> <code>None</code> <code>id_contains</code> <code>bool</code> <p>Filter datasets only where the string(s) are contained in the identifier, ignoring description.</p> <code>False</code> <code>catalog</code> <code>str</code> <p>A catalog identifier. Defaults to 'common'.</p> <code>None</code> <code>output</code> <code>bool</code> <p>If True then print the dataframe. Defaults to False.</p> <code>False</code> <code>max_results</code> <code>int</code> <p>Limit the number of rows returned in the dataframe. Defaults to -1 which returns all results.</p> <code>-1</code> <code>display_all_columns</code> <code>bool</code> <p>If True displays all columns returned by the API, otherwise only the key columns are displayed</p> <code>False</code> <p>Returns:</p> Name Type Description <code>class</code> <code>pd.DataFrame</code> <p><code>pandas.DataFrame</code>: a dataframe with a row for each product</p> Source code in <code>fusion/fusion.py</code> <pre><code>def list_products(\n    self,\n    contains: Union[str, list] = None,\n    id_contains: bool = False,\n    catalog: str = None,\n    output: bool = False,\n    max_results: int = -1,\n    display_all_columns: bool = False,\n) -&gt; pd.DataFrame:\n\"\"\"Get the products contained in a catalog. A product is a grouping of datasets.\n\n    Args:\n        contains (Union[str, list], optional): A string or a list of strings that are product\n            identifiers to filter the products list. If a list is provided then it will return\n            products whose identifier matches any of the strings. Defaults to None.\n        id_contains (bool): Filter datasets only where the string(s) are contained in the identifier,\n            ignoring description.\n        catalog (str, optional): A catalog identifier. Defaults to 'common'.\n        output (bool, optional): If True then print the dataframe. Defaults to False.\n        max_results (int, optional): Limit the number of rows returned in the dataframe.\n            Defaults to -1 which returns all results.\n        display_all_columns (bool, optional): If True displays all columns returned by the API,\n            otherwise only the key columns are displayed\n\n    Returns:\n        class:`pandas.DataFrame`: a dataframe with a row for each product\n    \"\"\"\n    catalog = self.__use_catalog(catalog)\n\n    url = f\"{self.root_url}catalogs/{catalog}/products\"\n    df: pd.DataFrame = Fusion._call_for_dataframe(url, self.session)\n\n    if contains:\n        if isinstance(contains, list):\n            contains = \"|\".join(f\"{s}\" for s in contains)\n        if id_contains:\n            df = df[df[\"identifier\"].str.contains(contains, case=False)]\n        else:\n            df = df[\n                df[\"identifier\"].str.contains(contains, case=False)\n                | df[\"description\"].str.contains(contains, case=False)\n            ]\n\n    df[\"category\"] = df.category.str.join(\", \")\n    df[\"region\"] = df.region.str.join(\", \")\n    if not display_all_columns:\n        df = df[\n            df.columns.intersection(\n                [\n                    \"identifier\",\n                    \"title\",\n                    \"region\",\n                    \"category\",\n                    \"status\",\n                    \"description\",\n                ]\n            )\n        ]\n\n    if max_results &gt; -1:\n        df = df[0:max_results]\n\n    if output:\n        print(tabulate(df, headers=\"keys\", tablefmt=\"psql\", maxcolwidths=30))\n\n    return df\n</code></pre>"},{"location":"api/#fusion.fusion.Fusion.listen_to_events","title":"<code>listen_to_events(last_event_id=None, catalog=None, url='https://fusion.jpmorgan.com/api/v1/')</code>","text":"<p>Run server sent event listener in the background. Retrieve results by running get_events.</p> <p>Parameters:</p> Name Type Description Default <code>last_event_id</code> <code>str</code> <p>Last event ID (exclusive).</p> <code>None</code> <code>catalog</code> <code>str</code> <p>catalog.</p> <code>None</code> <code>url</code> <code>str</code> <p>subscription url.</p> <code>'https://fusion.jpmorgan.com/api/v1/'</code> <p>Returns:</p> Type Description <code>Union[None, pd.DataFrame]</code> <p>Union[None, class:<code>pandas.DataFrame</code>]: If in_background is True then the function returns no output. If in_background is set to False then pandas DataFrame is output upon keyboard termination.</p> Source code in <code>fusion/fusion.py</code> <pre><code>def listen_to_events(\n    self,\n    last_event_id: str = None,\n    catalog: str = None,\n    url: str = \"https://fusion.jpmorgan.com/api/v1/\",\n) -&gt; Union[None, pd.DataFrame]:\n\"\"\"Run server sent event listener in the background. Retrieve results by running get_events.\n\n    Args:\n        last_event_id (str): Last event ID (exclusive).\n        catalog (str): catalog.\n        url (str): subscription url.\n    Returns:\n        Union[None, class:`pandas.DataFrame`]: If in_background is True then the function returns no output.\n            If in_background is set to False then pandas DataFrame is output upon keyboard termination.\n    \"\"\"\n\n    catalog = self.__use_catalog(catalog)\n    import asyncio\n    import json\n    import threading\n\n    from aiohttp_sse_client import client as sse_client\n\n    from .utils import get_client\n\n    kwargs = {}\n    if last_event_id:\n        kwargs = {\"headers\": {\"Last-Event-ID\": last_event_id}}\n\n    async def async_events():\n\"\"\"Events sync function.\n\n        Returns:\n            None\n        \"\"\"\n        timeout = 1e100\n        session = await get_client(self.credentials, timeout=timeout)\n        async with sse_client.EventSource(\n            f\"{url}catalogs/{catalog}/notifications/subscribe\",\n            session=session,\n            **kwargs,\n        ) as messages:\n            try:\n                async for msg in messages:\n                    event = json.loads(msg.data)\n                    if self.events is None:\n                        self.events = pd.DataFrame([event])\n                    else:\n                        self.events = pd.concat(\n                            [self.events, pd.DataFrame(event)], ignore_index=True\n                        )\n            except TimeoutError as ex:\n                raise ex\n            except Exception as e:\n                raise Exception(e)\n\n    _ = self.list_catalogs()  # refresh token\n    if \"headers\" in kwargs:\n        kwargs[\"headers\"].update(\n            {\"authorization\": f\"bearer {self.credentials.bearer_token}\"}\n        )\n    else:\n        kwargs[\"headers\"] = {\n            \"authorization\": f\"bearer {self.credentials.bearer_token}\",\n        }\n    if \"http\" in self.credentials.proxies.keys():\n        kwargs[\"proxy\"] = self.credentials.proxies[\"http\"]\n    elif \"https\" in self.credentials.proxies.keys():\n        kwargs[\"proxy\"] = self.credentials.proxies[\"https\"]\n    th = threading.Thread(target=asyncio.run, args=(async_events(),), daemon=True)\n    th.start()\n    return None\n</code></pre>"},{"location":"api/#fusion.fusion.Fusion.to_bytes","title":"<code>to_bytes(dataset, series_member, dataset_format='parquet', catalog=None)</code>","text":"<p>Returns an instance of dataset (the distribution) as a bytes object.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>str</code> <p>A dataset identifier</p> required <code>series_member</code> <code>str</code> <p>A dataset series member identifier</p> required <code>dataset_format</code> <code>str</code> <p>The file format, e.g. CSV or Parquet. Defaults to 'parquet'.</p> <code>'parquet'</code> <code>catalog</code> <code>str</code> <p>A catalog identifier. Defaults to 'common'.</p> <code>None</code> Source code in <code>fusion/fusion.py</code> <pre><code>def to_bytes(\n    self,\n    dataset: str,\n    series_member: str,\n    dataset_format: str = \"parquet\",\n    catalog: str = None,\n) -&gt; BytesIO:\n\"\"\"Returns an instance of dataset (the distribution) as a bytes object.\n\n    Args:\n        dataset (str): A dataset identifier\n        series_member (str,): A dataset series member identifier\n        dataset_format (str, optional): The file format, e.g. CSV or Parquet. Defaults to 'parquet'.\n        catalog (str, optional): A catalog identifier. Defaults to 'common'.\n    \"\"\"\n\n    catalog = self.__use_catalog(catalog)\n\n    url = distribution_to_url(\n        self.root_url, dataset, series_member, dataset_format, catalog  # type: ignore\n    )\n\n    return Fusion._call_for_bytes_object(url, self.session)\n</code></pre>"},{"location":"api/#fusion.fusion.Fusion.to_df","title":"<code>to_df(dataset, dt_str='latest', dataset_format='parquet', catalog=None, n_par=None, show_progress=True, columns=None, filters=None, force_download=False, download_folder=None, dataframe_type='pandas', **kwargs)</code>","text":"<p>Gets distributions for a specified date or date range and returns the data as a dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>str</code> <p>A dataset identifier</p> required <code>dt_str</code> <code>str</code> <p>Either a single date or a range identified by a start or end date, or both separated with a \":\". Defaults to 'latest' which will return the most recent instance of the dataset.</p> <code>'latest'</code> <code>dataset_format</code> <code>str</code> <p>The file format, e.g. CSV or Parquet. Defaults to 'parquet'.</p> <code>'parquet'</code> <code>catalog</code> <code>str</code> <p>A catalog identifier. Defaults to 'common'.</p> <code>None</code> <code>n_par</code> <code>int</code> <p>Specify how many distributions to download in parallel. Defaults to all cpus available.</p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>Display a progress bar during data download Defaults to True.</p> <code>True</code> <code>columns</code> <code>List</code> <p>A list of columns to return from a parquet file. Defaults to None</p> <code>None</code> <code>filters</code> <code>List</code> <p>List[Tuple] or List[List[Tuple]] or None (default) Rows which do not match the filter predicate will be removed from scanned data. Partition keys embedded in a nested directory structure will be exploited to avoid loading files at all if they contain no matching rows. If use_legacy_dataset is True, filters can only reference partition keys and only a hive-style directory structure is supported. When setting use_legacy_dataset to False, also within-file level filtering and different partitioning schemes are supported. More on https://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetDataset.html</p> <code>None</code> <code>force_download</code> <code>bool</code> <p>If True then will always download a file even if it is already on disk. Defaults to False.</p> <code>False</code> <code>download_folder</code> <code>str</code> <p>The path, absolute or relative, where downloaded files are saved. Defaults to download_folder as set in init</p> <code>None</code> <code>dataframe_type</code> <code>str</code> <p>Type</p> <code>'pandas'</code> <p>Returns:</p> Name Type Description <code>class</code> <code>pd.DataFrame</code> <p><code>pandas.DataFrame</code>: a dataframe containing the requested data. If multiple dataset instances are retrieved then these are concatenated first.</p> Source code in <code>fusion/fusion.py</code> <pre><code>def to_df(\n    self,\n    dataset: str,\n    dt_str: str = \"latest\",\n    dataset_format: str = \"parquet\",\n    catalog: str = None,\n    n_par: int = None,\n    show_progress: bool = True,\n    columns: List = None,\n    filters: List = None,\n    force_download: bool = False,\n    download_folder: str = None,\n    dataframe_type: str = \"pandas\",\n    **kwargs,\n) -&gt; pd.DataFrame:\n\"\"\"Gets distributions for a specified date or date range and returns the data as a dataframe.\n\n    Args:\n        dataset (str): A dataset identifier\n        dt_str (str, optional): Either a single date or a range identified by a start or end date,\n            or both separated with a \":\". Defaults to 'latest' which will return the most recent\n            instance of the dataset.\n        dataset_format (str, optional): The file format, e.g. CSV or Parquet. Defaults to 'parquet'.\n        catalog (str, optional): A catalog identifier. Defaults to 'common'.\n        n_par (int, optional): Specify how many distributions to download in parallel.\n            Defaults to all cpus available.\n        show_progress (bool, optional): Display a progress bar during data download Defaults to True.\n        columns (List, optional): A list of columns to return from a parquet file. Defaults to None\n        filters (List, optional): List[Tuple] or List[List[Tuple]] or None (default)\n            Rows which do not match the filter predicate will be removed from scanned data.\n            Partition keys embedded in a nested directory structure will be exploited to avoid\n            loading files at all if they contain no matching rows. If use_legacy_dataset is True,\n            filters can only reference partition keys and only a hive-style directory structure\n            is supported. When setting use_legacy_dataset to False, also within-file level filtering\n            and different partitioning schemes are supported.\n            More on https://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetDataset.html\n        force_download (bool, optional): If True then will always download a file even\n            if it is already on disk. Defaults to False.\n        download_folder (str, optional): The path, absolute or relative, where downloaded files are saved.\n            Defaults to download_folder as set in __init__\n        dataframe_type (str, optional): Type\n    Returns:\n        class:`pandas.DataFrame`: a dataframe containing the requested data.\n            If multiple dataset instances are retrieved then these are concatenated first.\n    \"\"\"\n    catalog = self.__use_catalog(catalog)\n\n    # sample data is limited to csv\n    if dt_str == \"sample\":\n        dataset_format = \"csv\"\n\n    if not download_folder:\n        download_folder = self.download_folder\n    download_res = self.download(\n        dataset,\n        dt_str,\n        dataset_format,\n        catalog,\n        n_par,\n        show_progress,\n        force_download,\n        download_folder,\n        return_paths=True,\n    )\n\n    if not all(res[0] for res in download_res):\n        failed_res = [res for res in download_res if not res[0]]\n        raise Exception(\n            f\"Not all downloads were successfully completed. \"\n            f\"Re-run to collect missing files. The following failed:\\n{failed_res}\"\n        )\n\n    files = [res[1] for res in download_res]\n\n    pd_read_fn_map = {\n        \"csv\": read_csv,\n        \"parquet\": read_parquet,\n        \"parq\": read_parquet,\n        \"json\": read_json,\n        \"raw\": read_csv,\n    }\n\n    pd_read_default_kwargs: Dict[str, Dict[str, object]] = {\n        \"csv\": {\n            \"columns\": columns,\n            \"filters\": filters,\n            \"fs\": self.fs,\n            \"dataframe_type\": dataframe_type,\n        },\n        \"parquet\": {\n            \"columns\": columns,\n            \"filters\": filters,\n            \"fs\": self.fs,\n            \"dataframe_type\": dataframe_type,\n        },\n        \"json\": {\n            \"columns\": columns,\n            \"filters\": filters,\n            \"fs\": self.fs,\n            \"dataframe_type\": dataframe_type,\n        },\n        \"raw\": {\n            \"columns\": columns,\n            \"filters\": filters,\n            \"fs\": self.fs,\n            \"dataframe_type\": dataframe_type,\n        },\n    }\n\n    pd_read_default_kwargs[\"parq\"] = pd_read_default_kwargs[\"parquet\"]\n\n    pd_reader = pd_read_fn_map.get(dataset_format)\n    pd_read_kwargs = pd_read_default_kwargs.get(dataset_format, {})\n    if not pd_reader:\n        raise Exception(\n            f\"No pandas function to read file in format {dataset_format}\"\n        )\n\n    pd_read_kwargs.update(kwargs)\n\n    if len(files) == 0:\n        raise APIResponseError(\n            f\"No series members for dataset: {dataset} \"\n            f\"in date or date range: {dt_str} and format: {dataset_format}\"\n        )\n    if dataset_format in [\"parquet\", \"parq\"]:\n        df = pd_reader(files, **pd_read_kwargs)  # type: ignore\n    elif dataset_format == \"raw\":\n        dataframes = (\n            pd.concat(\n                [pd_reader(ZipFile(f).open(p), **pd_read_kwargs) for p in ZipFile(f).namelist()], ignore_index=True  # type: ignore\n            )  # type: ignore\n            for f in files\n        )  # type: ignore\n        df = pd.concat(dataframes, ignore_index=True)\n    else:\n        dataframes = (pd_reader(f, **pd_read_kwargs) for f in files)  # type: ignore\n        if dataframe_type == \"pandas\":\n            df = pd.concat(dataframes, ignore_index=True)\n        if dataframe_type == \"polars\":\n            import polars as pl\n\n            df = pl.concat(dataframes, how=\"diagonal\")\n\n    return df\n</code></pre>"},{"location":"api/#fusion.fusion.Fusion.to_table","title":"<code>to_table(dataset, dt_str='latest', dataset_format='parquet', catalog=None, n_par=None, show_progress=True, columns=None, filters=None, force_download=False, download_folder=None, **kwargs)</code>","text":"<p>Gets distributions for a specified date or date range and returns the data as an arrow table.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>str</code> <p>A dataset identifier</p> required <code>dt_str</code> <code>str</code> <p>Either a single date or a range identified by a start or end date, or both separated with a \":\". Defaults to 'latest' which will return the most recent instance of the dataset.</p> <code>'latest'</code> <code>dataset_format</code> <code>str</code> <p>The file format, e.g. CSV or Parquet. Defaults to 'parquet'.</p> <code>'parquet'</code> <code>catalog</code> <code>str</code> <p>A catalog identifier. Defaults to 'common'.</p> <code>None</code> <code>n_par</code> <code>int</code> <p>Specify how many distributions to download in parallel. Defaults to all cpus available.</p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>Display a progress bar during data download Defaults to True.</p> <code>True</code> <code>columns</code> <code>List</code> <p>A list of columns to return from a parquet file. Defaults to None</p> <code>None</code> <code>filters</code> <code>List</code> <p>List[Tuple] or List[List[Tuple]] or None (default) Rows which do not match the filter predicate will be removed from scanned data. Partition keys embedded in a nested directory structure will be exploited to avoid loading files at all if they contain no matching rows. If use_legacy_dataset is True, filters can only reference partition keys and only a hive-style directory structure is supported. When setting use_legacy_dataset to False, also within-file level filtering and different partitioning schemes are supported. More on https://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetDataset.html</p> <code>None</code> <code>force_download</code> <code>bool</code> <p>If True then will always download a file even if it is already on disk. Defaults to False.</p> <code>False</code> <code>download_folder</code> <code>str</code> <p>The path, absolute or relative, where downloaded files are saved. Defaults to download_folder as set in init</p> <code>None</code> <p>Returns:</p> Name Type Description <code>class</code> <code>pd.DataFrame</code> <p><code>pandas.DataFrame</code>: a dataframe containing the requested data. If multiple dataset instances are retrieved then these are concatenated first.</p> Source code in <code>fusion/fusion.py</code> <pre><code>def to_table(\n    self,\n    dataset: str,\n    dt_str: str = \"latest\",\n    dataset_format: str = \"parquet\",\n    catalog: str = None,\n    n_par: int = None,\n    show_progress: bool = True,\n    columns: List = None,\n    filters: List = None,\n    force_download: bool = False,\n    download_folder: str = None,\n    **kwargs,\n) -&gt; pd.DataFrame:\n\"\"\"Gets distributions for a specified date or date range and returns the data as an arrow table.\n\n    Args:\n        dataset (str): A dataset identifier\n        dt_str (str, optional): Either a single date or a range identified by a start or end date,\n            or both separated with a \":\". Defaults to 'latest' which will return the most recent\n            instance of the dataset.\n        dataset_format (str, optional): The file format, e.g. CSV or Parquet. Defaults to 'parquet'.\n        catalog (str, optional): A catalog identifier. Defaults to 'common'.\n        n_par (int, optional): Specify how many distributions to download in parallel.\n            Defaults to all cpus available.\n        show_progress (bool, optional): Display a progress bar during data download Defaults to True.\n        columns (List, optional): A list of columns to return from a parquet file. Defaults to None\n        filters (List, optional): List[Tuple] or List[List[Tuple]] or None (default)\n            Rows which do not match the filter predicate will be removed from scanned data.\n            Partition keys embedded in a nested directory structure will be exploited to avoid\n            loading files at all if they contain no matching rows. If use_legacy_dataset is True,\n            filters can only reference partition keys and only a hive-style directory structure\n            is supported. When setting use_legacy_dataset to False, also within-file level filtering\n            and different partitioning schemes are supported.\n            More on https://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetDataset.html\n        force_download (bool, optional): If True then will always download a file even\n            if it is already on disk. Defaults to False.\n        download_folder (str, optional): The path, absolute or relative, where downloaded files are saved.\n            Defaults to download_folder as set in __init__\n    Returns:\n        class:`pandas.DataFrame`: a dataframe containing the requested data.\n            If multiple dataset instances are retrieved then these are concatenated first.\n    \"\"\"\n    catalog = self.__use_catalog(catalog)\n    n_par = cpu_count(n_par)\n    if not download_folder:\n        download_folder = self.download_folder\n    download_res = self.download(\n        dataset,\n        dt_str,\n        dataset_format,\n        catalog,\n        n_par,\n        show_progress,\n        force_download,\n        download_folder,\n        return_paths=True,\n    )\n\n    if not all(res[0] for res in download_res):\n        failed_res = [res for res in download_res if not res[0]]\n        raise RuntimeError(\n            f\"Not all downloads were successfully completed. \"\n            f\"Re-run to collect missing files. The following failed:\\n{failed_res}\"\n        )\n\n    files = [res[1] for res in download_res]\n\n    read_fn_map = {\n        \"csv\": csv_to_table,\n        \"parquet\": parquet_to_table,\n        \"parq\": parquet_to_table,\n        \"json\": json_to_table,\n        \"raw\": csv_to_table,\n    }\n\n    read_default_kwargs: Dict[str, Dict[str, object]] = {\n        \"csv\": {\"columns\": columns, \"filters\": filters, \"fs\": self.fs},\n        \"parquet\": {\"columns\": columns, \"filters\": filters, \"fs\": self.fs},\n        \"json\": {\"columns\": columns, \"filters\": filters, \"fs\": self.fs},\n        \"raw\": {\"columns\": columns, \"filters\": filters, \"fs\": self.fs},\n    }\n\n    read_default_kwargs[\"parq\"] = read_default_kwargs[\"parquet\"]\n\n    reader = read_fn_map.get(dataset_format)\n    read_kwargs = read_default_kwargs.get(dataset_format, {})\n    if not reader:\n        raise AssertionError(f\"No function to read file in format {dataset_format}\")\n\n    read_kwargs.update(kwargs)\n\n    if len(files) == 0:\n        raise APIResponseError(\n            f\"No series members for dataset: {dataset} \"\n            f\"in date or date range: {dt_str} and format: {dataset_format}\"\n        )\n    if dataset_format in [\"parquet\", \"parq\"]:\n        tbl = reader(files, **read_kwargs)  # type: ignore\n    else:\n        tbl = (reader(f, **read_kwargs) for f in files)  # type: ignore\n        tbl = pa.concat_tables(tbl)\n\n    return tbl\n</code></pre>"},{"location":"api/#fusion.fusion.Fusion.upload","title":"<code>upload(path, dataset=None, dt_str='latest', catalog=None, n_par=None, show_progress=True, return_paths=False, multipart=True, chunk_size=5 * 2 ** 20, from_date=None, to_date=None)</code>","text":"<p>Uploads the requested files/files to Fusion.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>path to a file or a folder with files</p> required <code>dataset</code> <code>str</code> <p>Dataset name to which the file will be uplaoded (for single file only).                     If not provided the dataset will be implied from file's name.</p> <code>None</code> <code>dt_str</code> <code>str</code> <p>A single date. Defaults to 'latest' which will return the most recent.                     Relevant for a single file upload only. If not provided the dataset will                     be implied from file's name.</p> <code>'latest'</code> <code>catalog</code> <code>str</code> <p>A catalog identifier. Defaults to 'common'.</p> <code>None</code> <code>n_par</code> <code>int</code> <p>Specify how many distributions to download in parallel. Defaults to all cpus available.</p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>Display a progress bar during data download Defaults to True.</p> <code>True</code> <code>return_paths</code> <code>bool</code> <p>Return paths and success statuses of the downloaded files.</p> <code>False</code> <code>multipart</code> <code>bool</code> <p>Is multipart upload.</p> <code>True</code> <code>chunk_size</code> <code>int</code> <p>Maximum chunk size.</p> <code>5 * 2 ** 20</code> <code>from_date</code> <code>str</code> <p>start of the data date range contained in the distribution, defaults to upoad date</p> <code>None</code> <code>to_date</code> <code>str</code> <p>end of the data date range contained in the distribution, defaults to upload date.</p> <code>None</code> Source code in <code>fusion/fusion.py</code> <pre><code>def upload(\n    self,\n    path: str,\n    dataset: str = None,\n    dt_str: str = \"latest\",\n    catalog: str = None,\n    n_par: int = None,\n    show_progress: bool = True,\n    return_paths: bool = False,\n    multipart=True,\n    chunk_size=5 * 2**20,\n    from_date=None,\n    to_date=None,\n):\n\"\"\"Uploads the requested files/files to Fusion.\n\n    Args:\n        path (str): path to a file or a folder with files\n        dataset (str, optional): Dataset name to which the file will be uplaoded (for single file only).\n                                If not provided the dataset will be implied from file's name.\n        dt_str (str, optional): A single date. Defaults to 'latest' which will return the most recent.\n                                Relevant for a single file upload only. If not provided the dataset will\n                                be implied from file's name.\n        catalog (str, optional): A catalog identifier. Defaults to 'common'.\n        n_par (int, optional): Specify how many distributions to download in parallel.\n            Defaults to all cpus available.\n        show_progress (bool, optional): Display a progress bar during data download Defaults to True.\n        return_paths (bool, optional): Return paths and success statuses of the downloaded files.\n        multipart (bool, optional): Is multipart upload.\n        chunk_size (int, optional): Maximum chunk size.\n        from_date (str, optional): start of the data date range contained in the distribution, defaults to upoad date\n        to_date (str, optional): end of the data date range contained in the distribution, defaults to upload date.\n\n    Returns:\n\n\n    \"\"\"\n    catalog = self.__use_catalog(catalog)\n\n    if not self.fs.exists(path):\n        raise RuntimeError(\"The provided path does not exist\")\n\n    fs_fusion = self.get_fusion_filesystem()\n    if self.fs.info(path)[\"type\"] == \"directory\":\n        file_path_lst = self.fs.find(path)\n        local_file_validation = validate_file_names(file_path_lst, fs_fusion)\n        file_path_lst = [\n            f for flag, f in zip(local_file_validation, file_path_lst) if flag\n        ]\n        is_raw_lst = is_dataset_raw(file_path_lst, fs_fusion)\n        local_url_eqiv = [\n            path_to_url(i, r) for i, r in zip(file_path_lst, is_raw_lst)\n        ]\n    else:\n        file_path_lst = [path]\n        if not catalog or not dataset:\n            local_file_validation = validate_file_names(file_path_lst, fs_fusion)\n            file_path_lst = [\n                f for flag, f in zip(local_file_validation, file_path_lst) if flag\n            ]\n            is_raw_lst = is_dataset_raw(file_path_lst, fs_fusion)\n            local_url_eqiv = [\n                path_to_url(i, r) for i, r in zip(file_path_lst, is_raw_lst)\n            ]\n        else:\n            date_identifier = re.compile(r\"^(\\d{4})(\\d{2})(\\d{2})$\")\n            if date_identifier.match(dt_str):\n                dt_str = (\n                    dt_str\n                    if dt_str != \"latest\"\n                    else pd.Timestamp(\"today\").date().strftime(\"%Y%m%d\")\n                )\n                dt_str = pd.Timestamp(dt_str).date().strftime(\"%Y%m%d\")\n\n            if catalog not in fs_fusion.ls(\"\") or dataset not in [\n                i.split(\"/\")[-1] for i in fs_fusion.ls(f\"{catalog}/datasets\")\n            ]:\n                msg = (\n                    f\"File file has not been uploaded, one of the catalog: {catalog} \"\n                    f\"or dataset: {dataset} does not exit.\"\n                )\n                warnings.warn(msg)\n                return [(False, path, Exception(msg))]\n            is_raw = js.loads(fs_fusion.cat(f\"{catalog}/datasets/{dataset}\"))[\n                \"isRawData\"\n            ]\n            file_format = path.split(\".\")[-1]\n            local_url_eqiv = [\n                path_to_url(f\"{dataset}__{catalog}__{dt_str}.{file_format}\", is_raw)\n            ]\n\n    df = pd.DataFrame([file_path_lst, local_url_eqiv]).T\n    df.columns = [\"path\", \"url\"]\n\n    n_par = cpu_count(n_par)\n    parallel = len(df) &gt; 1\n    res = upload_files(\n        fs_fusion,\n        self.fs,\n        df,\n        parallel=parallel,\n        n_par=n_par,\n        multipart=multipart,\n        chunk_size=chunk_size,\n        show_progress=show_progress,\n        from_date=from_date,\n        to_date=to_date,\n    )\n\n    if not all(r[0] for r in res):\n        failed_res = [r for r in res if not r[0]]\n        msg = f\"Not all uploads were successfully completed. The following failed:\\n{failed_res}\"\n        logger.warning(msg)\n        warnings.warn(msg)\n\n    return res if return_paths else None\n</code></pre>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#changelog","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#1023-2024-04-12","title":"[1.0.23] - 2024-04-12","text":"<ul> <li>bug fix to unlock the upload functionality</li> </ul>"},{"location":"changelog/#1022-2024-04-06","title":"[1.0.22] - 2024-04-06","text":"<ul> <li>limit number of threads for download to default to 10</li> </ul>"},{"location":"changelog/#1021-2024-03-27","title":"[1.0.21] - 2024-03-27","text":"<ul> <li>fix to_df for parquet files</li> </ul>"},{"location":"changelog/#1020-2024-03-27","title":"[1.0.20] - 2024-03-27","text":"<ul> <li>multipart download for a single file download</li> </ul>"},{"location":"changelog/#1019-2024-03-12","title":"[1.0.19] - 2024-03-12","text":"<ul> <li>eliminate dependency on async-retrying</li> </ul>"},{"location":"changelog/#1018-2024-03-11","title":"[1.0.18] - 2024-03-11","text":"<ul> <li>upload from s3 to API fix</li> </ul>"},{"location":"changelog/#1017-2024-02-26","title":"[1.0.17] - 2024-02-26","text":"<ul> <li>allow datetime dataseries members</li> <li>to_bytes method</li> <li>encode dataset name in the changes endpoint</li> </ul>"},{"location":"changelog/#1016-2024-02-19","title":"[1.0.16] - 2024-02-19","text":"<ul> <li>fix multi-dataset fsync</li> </ul>"},{"location":"changelog/#1015-2024-02-08","title":"[1.0.15] - 2024-02-08","text":"<ul> <li>fix get_events</li> <li>support for bytes-range requests in fusion filesystem</li> <li>support for per column downloads via pyarrow parquet dataset</li> </ul>"},{"location":"changelog/#1014-2023-12-13","title":"[1.0.14] - 2023-12-13","text":"<ul> <li>progress bar fix</li> <li>upload error propagation</li> </ul>"},{"location":"changelog/#1013-2023-12-13","title":"[1.0.13] - 2023-12-13","text":"<ul> <li>polars integration</li> <li>file size in fs.info function</li> <li>progress bar improvement to capture exceptions</li> <li>sample dataset download</li> <li>server events functionality</li> </ul>"},{"location":"changelog/#1012-2023-06-12","title":"[1.0.12] - 2023-06-12","text":"<ul> <li>minor bug fixes</li> </ul>"},{"location":"changelog/#1011-2023-05-10","title":"[1.0.11] - 2023-05-10","text":"<ul> <li>support bearer token authentication</li> <li>fix proxy support to aiohttp</li> <li>fix filtering support for csv and json</li> </ul>"},{"location":"changelog/#1010-2023-03-23","title":"[1.0.10] - 2023-03-23","text":"<ul> <li>md5 to sha256 convention change</li> <li>fsync continuous updates bug fix</li> <li>to_table function addition</li> <li>saving files in a hive friendly folder structure</li> <li>new bearer token add for download/upload operations</li> <li>raw data upload functionality fix</li> </ul>"},{"location":"changelog/#109-2023-01-23","title":"[1.0.9] - 2023-01-23","text":"<ul> <li>operational enhancements</li> </ul>"},{"location":"changelog/#108-2023-01-19","title":"[1.0.8] - 2023-01-19","text":"<ul> <li>cloud storage compatibility</li> </ul>"},{"location":"changelog/#107-2023-01-12","title":"[1.0.7] - 2023-01-12","text":"<ul> <li>Multi-part upload</li> <li>fsync</li> </ul>"},{"location":"changelog/#106-2022-11-21","title":"[1.0.6] - 2022-11-21","text":"<ul> <li>Support setting of the default catalog</li> <li>Fusion filesystem module</li> <li>Upload functionality</li> <li>Folder traversing for credentials</li> <li>Filters for parquet and csv file opening</li> </ul>"},{"location":"changelog/#105-2022-06-22","title":"[1.0.5] - 2022-06-22","text":"<ul> <li>Add support for internal auth methods</li> </ul>"},{"location":"changelog/#104-2022-05-19","title":"[1.0.4] - 2022-05-19","text":"<ul> <li>Support proxy servers in auth post requests</li> <li>Add back support for '2020-01-01' and '20200101' date formats</li> <li>Various bug fixes</li> <li>Streamline credentials creation</li> </ul>"},{"location":"changelog/#103-2022-05-12","title":"[1.0.3] - 2022-05-12","text":"<ul> <li>Add support for 'latest' datasets</li> </ul>"},{"location":"changelog/#102-2022-05-12","title":"[1.0.2] - 2022-05-12","text":"<ul> <li>Integrate build with docs</li> </ul>"},{"location":"changelog/#101-2022-05-12","title":"[1.0.1] - 2022-05-12","text":"<ul> <li>First live release on JPMC gitub</li> </ul>"},{"location":"contributing/","title":"Contributing","text":""},{"location":"contributing/#contributing","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/{{ cookiecutter.github_username }}/{{ cookiecutter.project_slug }}/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>{{ cookiecutter.project_name }} could always use more documentation, whether as part of the official {{ cookiecutter.project_name }} docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/{{ cookiecutter.github_username }}/{{ cookiecutter.project_slug }}/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions   are welcome :)</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up <code>{{ cookiecutter.project_slug }}</code> for local development.</p> <ol> <li>Fork the <code>{{ cookiecutter.project_slug }}</code> repo on GitHub.</li> <li> <p>Clone your fork locally</p> <pre><code>$ git clone git@github.com:your_name_here/{{ cookiecutter.project_slug }}.git\n</code></pre> </li> <li> <p>Ensure poetry is installed.</p> </li> <li> <p>Install dependencies and start your virtualenv:</p> <pre><code>$ poetry install -E test -E doc -E dev\n</code></pre> </li> <li> <p>Create a branch for local development:</p> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> </li> <li> <p>When you're done making changes, check that your changes pass the    tests, including testing other Python versions, with tox:</p> <pre><code>$ poetry run tox\n</code></pre> </li> <li> <p>Commit your changes and push your branch to GitHub:</p> <pre><code>$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> </li> <li> <p>Submit a pull request through the GitHub website.</p> </li> </ol>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated. Put    your new functionality into a function with a docstring, and add the    feature to the list in README.md.</li> <li>The pull request should work for Python 3.7, 3.8 and 3.9. Check    https://github.com/{{ cookiecutter.github_username }}/{{ cookiecutter.project_slug }}/actions    and make sure that the tests pass for all supported Python versions.</li> </ol>"},{"location":"contributing/#tips","title":"Tips","text":"<pre><code>$ poetry run pytest tests/test_{{ cookiecutter.pkg_name }}.py\n</code></pre> <p>To run a subset of tests.</p>"},{"location":"contributing/#deploying","title":"Deploying","text":"<p>A reminder for the maintainers on how to deploy. Make sure all your changes are committed (including an entry in CHANGELOG.md). Then run:</p> <pre><code>$ poetry run bump2version patch # possible: major / minor / patch\n$ git push\n$ git push --tags\n</code></pre> <p>GitHub Actions will then deploy to PyPI if tests pass.</p>"},{"location":"get_started/","title":"Fusion - get started","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom fusion import Fusion\n\nplt.style.use(\"bmh\")\n</pre> import pandas as pd import matplotlib.pyplot as plt from fusion import Fusion  plt.style.use(\"bmh\") In\u00a0[2]: Copied! <pre>fusion = Fusion()\n</pre> fusion = Fusion() In\u00a0[3]: Copied! <pre>fusion\n</pre> fusion Out[3]: <pre>Fusion object \nAvailable methods:\n+-------------------------+-----------------------------------------------------------------------------------------------+\n| catalog_resources       | List the resources contained within the catalog, for example products and datasets.           |\n| dataset_resources       | List the resources available for a dataset, currently this will always be a datasetseries.    |\n| datasetmember_resources | List the available resources for a datasetseries member.                                      |\n| download                | Downloads the requested distributions of a dataset to disk.                                   |\n| from_bytes              | Uploads data from an object in memory.                                                        |\n| get_events              | Run server sent event listener and print out the new events. Keyboard terminate to stop.      |\n| get_fusion_filesystem   | Creates Fusion Filesystem.                                                                    |\n| list_catalogs           | Lists the catalogs available to the API account.                                              |\n| list_dataset_attributes | Returns the list of attributes that are in the dataset.                                       |\n| list_datasetmembers     | List the available members in the dataset series.                                             |\n| list_datasets           | Get the datasets contained in a catalog.                                                      |\n| list_distributions      | List the available distributions (downloadable instances of the dataset with a format type).  |\n| list_products           | Get the products contained in a catalog. A product is a grouping of datasets.                 |\n| listen_to_events        | Run server sent event listener in the background. Retrieve results by running get_events.     |\n| to_bytes                | Returns an instance of dataset (the distribution) as a bytes object.                          |\n| to_df                   | Gets distributions for a specified date or date range and returns the data as a dataframe.    |\n| to_table                | Gets distributions for a specified date or date range and returns the data as an arrow table. |\n| upload                  | Uploads the requested files/files to Fusion.                                                  |\n| default_catalog         | Returns the default catalog.                                                                  |\n+-------------------------+-----------------------------------------------------------------------------------------------+</pre> In\u00a0[4]: Copied! <pre>fusion.to_df?\n</pre> fusion.to_df? <pre>Signature:\nfusion.to_df(\n    dataset: str,\n    dt_str: str = 'latest',\n    dataset_format: str = 'parquet',\n    catalog: str = None,\n    n_par: int = None,\n    show_progress: bool = True,\n    columns: List = None,\n    filters: List = None,\n    force_download: bool = False,\n    download_folder: str = None,\n    dataframe_type: str = 'pandas',\n**kwargs,\n) -&gt; pandas.core.frame.DataFrame\nDocstring:\nGets distributions for a specified date or date range and returns the data as a dataframe.\n\nArgs:\n    dataset (str): A dataset identifier\n    dt_str (str, optional): Either a single date or a range identified by a start or end date,\n        or both separated with a \":\". Defaults to 'latest' which will return the most recent\n        instance of the dataset.\n    dataset_format (str, optional): The file format, e.g. CSV or Parquet. Defaults to 'parquet'.\n    catalog (str, optional): A catalog identifier. Defaults to 'common'.\n    n_par (int, optional): Specify how many distributions to download in parallel.\n        Defaults to all cpus available.\n    show_progress (bool, optional): Display a progress bar during data download Defaults to True.\n    columns (List, optional): A list of columns to return from a parquet file. Defaults to None\n    filters (List, optional): List[Tuple] or List[List[Tuple]] or None (default)\n        Rows which do not match the filter predicate will be removed from scanned data.\n        Partition keys embedded in a nested directory structure will be exploited to avoid\n        loading files at all if they contain no matching rows. If use_legacy_dataset is True,\n        filters can only reference partition keys and only a hive-style directory structure\n        is supported. When setting use_legacy_dataset to False, also within-file level filtering\n        and different partitioning schemes are supported.\n        More on https://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetDataset.html\n    force_download (bool, optional): If True then will always download a file even\n        if it is already on disk. Defaults to False.\n    download_folder (str, optional): The path, absolute or relative, where downloaded files are saved.\n        Defaults to download_folder as set in __init__\n    dataframe_type (str, optional): Type\nReturns:\n    class:`pandas.DataFrame`: a dataframe containing the requested data.\n        If multiple dataset instances are retrieved then these are concatenated first.\nFile:      c:\\dev\\fusion\\fusion\\fusion.py\nType:      method</pre> In\u00a0[5]: Copied! <pre>fusion.list_datasets(\"FX\")\n</pre> fusion.list_datasets(\"FX\") Out[5]: identifier title containerType region category coverageStartDate coverageEndDate description status 12 FX_EASIDX Economic Activity Surprise Index (EASI) FX Snapshot-Full EMEA, North America, Emerging Markets, APAC, G... Economic 2019-01-01 2024-02-01 The Economic Activity Surprise Index is publis... Subscribed 22 FX_MEAN_IMM FX Mean Reversion Strategies IMM Snapshot-Full EMEA, North America, APAC, Emerging Markets, G... FX 2019-01-01 2024-02-01 The FX Mean Reversion, IMM dataset from J.P. M... Subscribed 28 FXO_RR FX Option Structure | Risk Reversal Snapshot-Full EMEA, North America, APAC, Emerging Markets, G... FX 2019-01-01 2024-02-01 Implied volatility for 10 and 25 delta FX opti... Subscribed 29 FXO_ST FX Option Structure | Strangles Snapshot-Full EMEA, North America, APAC, Emerging Markets, G... FX 2019-01-01 2024-02-01 Implied volatility for 10 and 25 delta FX opti... Subscribed 39 FXO_SP FX Cash Rate Snapshot-Full EMEA, North America, Emerging Markets, APAC, G... FX 2019-01-01 2024-04-11 This dataset includes FX spot rates for major ... Subscribed 53 STANDARD_VALUED_HOLDINGS_SUMMARY Sample: Valued Holdings Summary Snapshot-Full Global Middle Office NaN NaN Provides portfolio level valuation in a specif... Available 60 FX_ECONOMIC FX Specialized | Momentum Strategies (Economics) Snapshot-Full EMEA, North America, APAC, Emerging Markets, G... FX 2019-01-01 2024-01-29 Momentum signals in a trend following strategy... Subscribed 89 FX_JPM_TCI FX Passive Index Snapshot-Full EMEA, North America, APAC, Global FX 2019-01-01 2024-02-01 FX passive index level and currency sub-indices. Subscribed 90 FX_MEAN_HFFV FX Mean Reversion Strategies Hi Freq Fair Value Snapshot-Full EMEA, North America, APAC, Global FX 2019-01-01 2024-02-01 The FX High Frequency Fair Value dataset from ... Subscribed 126 STANDARD_VALUED_HOLDINGS Sample: Valued Holdings Snapshot-Full Global Middle Office NaN NaN Provides market value in local currency of the... Available 147 STANDARD_CLIENT_STANDING_INSTRUCTIONS Sample: Client Standing Instructions Snapshot-Full Global Custody NaN NaN Standard dataset detailing client standing ins... Available 148 STANDARD_CLS_FX_TRANSACTIONS Sample: CLS FX Transactions Snapshot-Full Global Custody NaN NaN Standard dataset detailing all CLS FX trade ac... Available 182 STANDARD_FUTURE_VALUED_FX_CONTRACTS Sample: Future Valued FX Contracts Snapshot-Full Global Custody NaN NaN Standard Future Valued FX Contracts dataset di... Available 210 STANDARD_MARKET_VALUATION_DETAIL_CCY_CONTRACTS Sample: Market Valuation Detail Currency Contr... Snapshot-Full Global Fund Accounting NaN NaN This dataset exclusively shows all pending spo... Available 222 FXO_VOL_EOD_G10 FXO End of Day Implied Volatility - G10 Snapshot-Full Global FX 2013-01-02 2024-04-11 J.P. Morgan\u2019s FXO End of Day Implied Volatilit... Subscribed 342 FXO_VOL_EOD_EM FXO End of Day Implied Volatility - EM Snapshot-Full Emerging Markets, Global FX 2013-01-02 2024-04-11 J.P. Morgan\u2019s FXO End of Day Implied Volatilit... Subscribed In\u00a0[6]: Copied! <pre>fusion.list_dataset_attributes(\"FXO_SP\")\n</pre> fusion.list_dataset_attributes(\"FXO_SP\") Out[6]: source dataType description identifier isDatasetKey title 0 Data Query String The instrument name instrument_name True Instrument Name 1 Data Query String The currency pair currency_pair False Currency Pair 2 Data Query String The time period of an investment, agreement or... term False Term 3 Data Query String The product identifier product False Product 4 Data Query String The snapshot date date False Date 5 Data Query Double The spot and forward fx rate fx_rate False FX Rate In\u00a0[7]: Copied! <pre>df = fusion.to_df(\"FXO_SP\", \"20220101:20221231\", columns=[\"currency_pair\", \"date\", \"fx_rate\"], filters=[(\"currency_pair\", \"=\", \"GBPUSD\")])\n</pre> df = fusion.to_df(\"FXO_SP\", \"20220101:20221231\", columns=[\"currency_pair\", \"date\", \"fx_rate\"], filters=[(\"currency_pair\", \"=\", \"GBPUSD\")]) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 258/258 [00:26&lt;00:00,  9.77it/s]\n</pre> In\u00a0[8]: Copied! <pre>df.head()\n</pre> df.head() Out[8]: currency_pair date fx_rate 0 GBPUSD 20221118 1.19300 1 GBPUSD 20220310 1.31295 2 GBPUSD 20220322 1.32590 3 GBPUSD 20220318 1.31705 4 GBPUSD 20220103 1.34475 In\u00a0[9]: Copied! <pre>df[\"date\"] = pd.to_datetime(df[\"date\"].astype(\"str\"))\ndf.sort_values(\"date\").set_index(\"date\").plot(grid=True);\n</pre> df[\"date\"] = pd.to_datetime(df[\"date\"].astype(\"str\")) df.sort_values(\"date\").set_index(\"date\").plot(grid=True);"},{"location":"get_started/#fusion-get-started","title":"Fusion - get started\u00b6","text":""},{"location":"get_started/#establish-the-connection","title":"Establish the connection\u00b6","text":""},{"location":"get_started/#show-the-available-functionality","title":"Show the available functionality\u00b6","text":""},{"location":"get_started/#access-function-documentation","title":"Access function documentation\u00b6","text":""},{"location":"get_started/#explore-the-datasets","title":"Explore the datasets\u00b6","text":""},{"location":"get_started/#display-the-attributes","title":"Display the attributes\u00b6","text":""},{"location":"get_started/#download-and-load","title":"Download and load\u00b6","text":""},{"location":"get_started/#analyze","title":"Analyze\u00b6","text":""},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#installation","title":"Installation","text":""},{"location":"installation/#stable-release","title":"Stable release","text":"<p>To install PyFusion, run this command in your terminal:</p> <pre><code>$ pip install pyfusion\n</code></pre> <p>This is the preferred method to install PyFusion, as it will always install the most recent stable release.</p> <p>If you don't have pip installed, this Python installation guide can guide you through the process.</p>"},{"location":"installation/#from-source","title":"From source","text":"<p>The source for PyFusion can be downloaded from the Github repo.</p> <p>You can either clone the public repository:</p> <pre><code>$ git clone git://github.com/jpmorganchase/fusion\n</code></pre> <p>Or download the tarball:</p> <pre><code>$ curl -OJL https://github.com/jpmorganchase/fusion/tarball/master\n</code></pre> <p>Once you have a copy of the source, you can install it with:</p> <pre><code>$ pip install .\n</code></pre>"},{"location":"usage/","title":"Usage","text":""},{"location":"usage/#usage","title":"Usage","text":""},{"location":"usage/#import-fusion","title":"Import Fusion","text":"<pre><code>from fusion import Fusion\n</code></pre>"},{"location":"usage/#fusion-object","title":"Fusion Object","text":"<pre><code>fusion = Fusion()\n</code></pre>"}]}